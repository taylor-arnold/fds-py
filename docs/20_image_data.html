<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Taylor Arnold">

<title>20&nbsp; Image Data – Foundations of Data Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./21_notes.html" rel="next">
<link href="./19_textual_data.html" rel="prev">
<link href="./img/owl_explore.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-25064e4154141e7ea34817c6f1220590.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./16_spatial_data.html">Part IV: Applications</a></li><li class="breadcrumb-item"><a href="./20_image_data.html"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Image Data</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Foundations of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/taylor-arnold/fds-py" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Part I: Core</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_modify.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">EDA I: Organizing Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_graphics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">EDA II: Visualizing Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_combine.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">EDA III: Restructuring Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_collect.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">EDA IV: Collecting Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Part II: Programming</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_program.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Programming</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_strings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Strings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_dataformats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Data Formats</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_requests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Requests</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Part III: Models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_supervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Supervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_deeplearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14_cnnwordvec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">CNNs and Word2Vec</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15_transferlearn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Transfer Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Part IV: Applications</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16_spatial_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Spatial Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17_temporal_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Temporal Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18_network_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Network Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19_textual_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Textual Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20_image_data.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Image Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Part V: Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21_notes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Notes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#setup" id="toc-setup" class="nav-link active" data-scroll-target="#setup"><span class="header-section-number">20.1</span> Setup</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">20.2</span> Introduction</a></li>
  <li><a href="#image-arrays" id="toc-image-arrays" class="nav-link" data-scroll-target="#image-arrays"><span class="header-section-number">20.3</span> Image Arrays</a></li>
  <li><a href="#brightness" id="toc-brightness" class="nav-link" data-scroll-target="#brightness"><span class="header-section-number">20.4</span> Brightness</a></li>
  <li><a href="#hue-saturation-and-value" id="toc-hue-saturation-and-value" class="nav-link" data-scroll-target="#hue-saturation-and-value"><span class="header-section-number">20.5</span> Hue, Saturation, and Value</a></li>
  <li><a href="#colors" id="toc-colors" class="nav-link" data-scroll-target="#colors"><span class="header-section-number">20.6</span> Colors</a></li>
  <li><a href="#object-detection" id="toc-object-detection" class="nav-link" data-scroll-target="#object-detection"><span class="header-section-number">20.7</span> Object Detection</a></li>
  <li><a href="#segmentation" id="toc-segmentation" class="nav-link" data-scroll-target="#segmentation"><span class="header-section-number">20.8</span> Segmentation</a></li>
  <li><a href="#pose-detection" id="toc-pose-detection" class="nav-link" data-scroll-target="#pose-detection"><span class="header-section-number">20.9</span> Pose Detection</a></li>
  <li><a href="#training-yolo" id="toc-training-yolo" class="nav-link" data-scroll-target="#training-yolo"><span class="header-section-number">20.10</span> Training YOLO</a></li>
  <li><a href="#vision-language-models" id="toc-vision-language-models" class="nav-link" data-scroll-target="#vision-language-models"><span class="header-section-number">20.11</span> Vision Language Models</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions"><span class="header-section-number">20.12</span> Conclusions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./16_spatial_data.html">Part IV: Applications</a></li><li class="breadcrumb-item"><a href="./20_image_data.html"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Image Data</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-image" class="quarto-section-identifier"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Image Data</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practice Notebooks
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Notebook20a [<a href="https://colab.research.google.com/github/taylor-arnold/fds-py-nb/blob/main/nb/notebook20a.ipynb?hl=en">Colab↗</a>]</li>
<li>Notebook20b [<a href="https://colab.research.google.com/github/taylor-arnold/fds-py-nb/blob/main/nb/notebook20b.ipynb?hl=en">Colab↗</a>]</li>
</ul>
</div>
</div>
</div>
<section id="setup" class="level2" data-number="20.1">
<h2 data-number="20.1" class="anchored" data-anchor-id="setup"><span class="header-section-number">20.1</span> Setup</h2>
<p>Load all of the modules and datasets needed for the chapter. In addition to the standard tools, we will use OpenCV (cv2) for image processing, PIL for image display, and the Ultralytics library for accessing pre-trained YOLO models.</p>
<div id="44c53c2f" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> polars <span class="im">as</span> pl</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> funs <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> plotnine <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> polars <span class="im">import</span> col <span class="im">as</span> c</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>theme_set(theme_minimal())</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>birds <span class="op">=</span> pl.read_parquet(<span class="st">"data/birds10.parquet"</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>birds_bbox <span class="op">=</span> pl.read_csv(<span class="st">"data/birds_1000.csv"</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>fsac <span class="op">=</span> pl.read_csv(<span class="st">"data/fsac.csv"</span>).select(c.filepath, c.photographer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="introduction" class="level2" data-number="20.2">
<h2 data-number="20.2" class="anchored" data-anchor-id="introduction"><span class="header-section-number">20.2</span> Introduction</h2>
<p>Images are among the most ubiquitous forms of data in the modern world. Every day, billions of photographs are captured on smartphones, satellites orbit the Earth generating terabytes of imagery, medical scanners produce detailed views of the human body, and security cameras record countless hours of video. The ability to extract meaningful information from these images has become one of the most important capabilities in data science.</p>
<p>At their core, digital images are simply arrays of numbers. Each pixel in an image stores numerical values representing color or intensity. Yet from these raw numbers emerge faces, objects, scenes, and stories. The challenge of image analysis lies in bridging this gap between low-level pixel values and high-level semantic meaning. How do we go from a grid of numbers to understanding that an image contains a bird, or that a person in a photograph is smiling, or that a tumor is present in a medical scan?</p>
<p>In this chapter, we explore several approaches to extracting information from images. We begin with basic properties that can be computed directly from pixel values, including brightness and color distributions. These simple features, while limited in their semantic power, provide useful descriptors for large collections of images and can reveal interesting patterns in visual datasets. We then turn to more sophisticated computer vision techniques that leverage deep learning models trained on millions of images. These models can detect objects, segment regions, and identify human poses with remarkable accuracy.</p>
<p>The progression in this chapter mirrors the broader evolution of computer vision as a field. Early approaches relied on hand-crafted features computed from pixel statistics. Modern methods use neural networks to learn relevant features directly from data. Both perspectives remain valuable: simple pixel-based features are interpretable, fast to compute, and often sufficient for exploratory analysis, while deep learning models offer unprecedented accuracy for complex recognition tasks. Understanding both approaches equips you to choose the right tool for each analytical situation.</p>
<p>Note that in addition to the task-specific models we explore here, images can also be converted into dense vector embeddings using the transfer learning techniques described in <a href="15_transferlearn.html" class="quarto-xref"><span>Chapter 15</span></a>. These embeddings represent images as points in a high-dimensional space where visually or semantically similar images are close together. Once you have embeddings, all the familiar tools apply: you can train classifiers to predict categories, use dimensionality reduction techniques like PCA or UMAP to visualize collections, or apply clustering algorithms to discover groups of related images. The embedding approach is particularly powerful when you have domain-specific categories that don’t match the labels in pre-trained models.</p>
</section>
<section id="image-arrays" class="level2" data-number="20.3">
<h2 data-number="20.3" class="anchored" data-anchor-id="image-arrays"><span class="header-section-number">20.3</span> Image Arrays</h2>
<p>We will work with the birds image dataset, which contains photographs of ten species of birds. Each image has been preprocessed to a standard size and format, making it suitable for comparative analysis. The dataset includes species with distinctive coloration, from the brilliant yellow of canaries to the iridescent blues and greens of peacocks.</p>
<div id="a86727c5" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>birds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (1_555, 5)</small>
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">label</th>
<th data-quarto-table-cell-role="th">filepath</th>
<th data-quarto-table-cell-role="th">index</th>
<th data-quarto-table-cell-role="th">vit</th>
<th data-quarto-table-cell-role="th">siglip</th>
</tr>
<tr class="even">
<th>str</th>
<th>str</th>
<th>str</th>
<th>list[f64]</th>
<th>list[f64]</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"canary"</td>
<td>"media/birds10/00000.png"</td>
<td>"test"</td>
<td>[0.017947, -0.0305, … 0.003526]</td>
<td>[-0.00611, -0.042975, … -0.031687]</td>
</tr>
<tr class="even">
<td>"canary"</td>
<td>"media/birds10/00001.png"</td>
<td>"train"</td>
<td>[0.024804, -0.045255, … -0.007233]</td>
<td>[-0.033767, -0.011978, … -0.020352]</td>
</tr>
<tr class="odd">
<td>"canary"</td>
<td>"media/birds10/00002.png"</td>
<td>"train"</td>
<td>[0.050587, -0.024486, … 0.029895]</td>
<td>[-0.033664, -0.008117, … -0.01725]</td>
</tr>
<tr class="even">
<td>"canary"</td>
<td>"media/birds10/00003.png"</td>
<td>"train"</td>
<td>[0.047036, -0.038993, … -0.008446]</td>
<td>[-0.010029, -0.018192, … -0.009869]</td>
</tr>
<tr class="odd">
<td>"canary"</td>
<td>"media/birds10/00004.png"</td>
<td>"train"</td>
<td>[0.036349, -0.02734, … -0.018185]</td>
<td>[-0.027327, 0.003568, … -0.033407]</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>"swallow"</td>
<td>"media/birds10/01550.png"</td>
<td>"train"</td>
<td>[-0.022461, -0.025098, … -0.061945]</td>
<td>[-0.022029, -0.008476, … -0.003879]</td>
</tr>
<tr class="even">
<td>"swallow"</td>
<td>"media/birds10/01551.png"</td>
<td>"train"</td>
<td>[-0.000212, -0.003448, … -0.058042]</td>
<td>[-0.02476, -0.016369, … 0.003875]</td>
</tr>
<tr class="odd">
<td>"swallow"</td>
<td>"media/birds10/01552.png"</td>
<td>"train"</td>
<td>[-0.012531, -0.006788, … -0.047077]</td>
<td>[-0.022153, 0.00765, … -0.011152]</td>
</tr>
<tr class="even">
<td>"swallow"</td>
<td>"media/birds10/01553.png"</td>
<td>"test"</td>
<td>[-0.007587, -0.053535, … -0.046395]</td>
<td>[-0.005022, 0.00878, … -0.017564]</td>
</tr>
<tr class="odd">
<td>"swallow"</td>
<td>"media/birds10/01554.png"</td>
<td>"train"</td>
<td>[-0.01325, -0.032453, … -0.050751]</td>
<td>[-0.015117, -0.00685, … -0.029756]</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>The dataset contains a <code>filepath</code> column pointing to each image file and a <code>label</code> column identifying the species. This structure, with metadata in a table and actual image files stored separately, is a common pattern for working with image datasets. It allows us to use familiar tabular data tools for filtering, grouping, and summarizing while keeping the image files in their native format.</p>
<p>To work with an image programmatically, we need to read it into Python as a numerical array. The OpenCV library provides the <code>imread</code> function for this purpose. Let’s load the first image in our dataset.</p>
<div id="83e7c383" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> cv2.imread(birds.select(pl.col(<span class="st">"filepath"</span>).get(<span class="dv">0</span>)).item())</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>array([[[ 18,  20,  21],
        [ 22,  24,  25],
        [ 23,  25,  26],
        ...,
        [ 48,  48,  54],
        [ 48,  47,  56],
        [ 47,  46,  55]],

       [[ 18,  20,  21],
        [ 20,  22,  23],
        [ 20,  22,  23],
        ...,
        [ 50,  50,  56],
        [ 49,  48,  57],
        [ 47,  46,  55]],

       [[ 12,  15,  19],
        [ 11,  14,  18],
        [ 12,  15,  19],
        ...,
        [ 55,  53,  59],
        [ 53,  51,  57],
        [ 52,  50,  56]],

       ...,

       [[133, 148, 180],
        [147, 159, 187],
        [126, 130, 149],
        ...,
        [122,  98,  92],
        [104,  81,  79],
        [ 87,  66,  65]],

       [[116, 133, 166],
        [138, 152, 181],
        [133, 140, 165],
        ...,
        [113,  91,  86],
        [106,  87,  90],
        [ 87,  70,  74]],

       [[ 89, 108, 141],
        [127, 142, 174],
        [140, 149, 176],
        ...,
        [103,  82,  80],
        [ 91,  74,  78],
        [122, 107, 115]]], shape=(224, 224, 3), dtype=uint8)</code></pre>
</div>
</div>
<p>What we see is a NumPy array filled with numbers. Each number represents the intensity of a color channel at a specific pixel location. The values range from 0 (no intensity) to 255 (maximum intensity), using 8 bits per channel, which is the standard format for most digital images.</p>
<p>The shape of this array reveals the structure of the image data.</p>
<div id="9dd9e209" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>img.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>(224, 224, 3)</code></pre>
</div>
</div>
<p>The three dimensions correspond to height (number of rows of pixels), width (number of columns), and color channels. Most color images use three channels: red, green, and blue (RGB). However, OpenCV reads images in BGR order (blue, green, red), a historical convention from early computer vision libraries. This ordering rarely matters for analysis but becomes important when displaying images or converting between color spaces.</p>
<p>We can examine a small region of the array to see the actual pixel values. Here are the values for a 4×4 block of pixels in the upper-left corner.</p>
<div id="43569d60" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>img[:<span class="dv">4</span>, :<span class="dv">4</span>, :]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>array([[[18, 20, 21],
        [22, 24, 25],
        [23, 25, 26],
        [22, 24, 25]],

       [[18, 20, 21],
        [20, 22, 23],
        [20, 22, 23],
        [19, 21, 22]],

       [[12, 15, 19],
        [11, 14, 18],
        [12, 15, 19],
        [15, 18, 22]],

       [[17, 20, 24],
        [13, 16, 20],
        [13, 16, 20],
        [14, 17, 21]]], dtype=uint8)</code></pre>
</div>
</div>
<p>Each pixel is represented by three values, one for each color channel. A pixel with values <code>[255, 255, 255]</code> would be pure white, while <code>[0, 0, 0]</code> would be pure black. Values like <code>[255, 0, 0]</code> represent pure blue (remember, OpenCV uses BGR ordering), <code>[0, 255, 0]</code> is green, and <code>[0, 0, 255]</code> is red.</p>
<p>One of the simplest statistics we can compute from an image is the mean of all pixel values.</p>
<div id="0a9f9c4f" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>img.mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>np.float64(119.86716757015306)</code></pre>
</div>
</div>
<p>This single number summarizes the overall intensity of the image. Higher values indicate brighter images (more pixels closer to 255), while lower values indicate darker images (more pixels closer to 0). While crude, this measure provides a starting point for understanding the visual characteristics of image collections.</p>
</section>
<section id="brightness" class="level2" data-number="20.4">
<h2 data-number="20.4" class="anchored" data-anchor-id="brightness"><span class="header-section-number">20.4</span> Brightness</h2>
<p>Let’s compute the mean brightness for every image in our bird dataset. This requires iterating through all images, loading each one, and computing its mean pixel value. We store the results and add them as a new column to our DataFrame.</p>
<div id="3bd69d04" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> row <span class="kw">in</span> birds.iter_rows(named<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> cv2.imread(row[<span class="st">"filepath"</span>])</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    results.append(img.mean())</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>birds <span class="op">=</span> birds.with_columns(brightness <span class="op">=</span> pl.Series(results))</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>birds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (1_555, 6)</small>
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">label</th>
<th data-quarto-table-cell-role="th">filepath</th>
<th data-quarto-table-cell-role="th">index</th>
<th data-quarto-table-cell-role="th">vit</th>
<th data-quarto-table-cell-role="th">siglip</th>
<th data-quarto-table-cell-role="th">brightness</th>
</tr>
<tr class="even">
<th>str</th>
<th>str</th>
<th>str</th>
<th>list[f64]</th>
<th>list[f64]</th>
<th>f64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"canary"</td>
<td>"media/birds10/00000.png"</td>
<td>"test"</td>
<td>[0.017947, -0.0305, … 0.003526]</td>
<td>[-0.00611, -0.042975, … -0.031687]</td>
<td>119.867168</td>
</tr>
<tr class="even">
<td>"canary"</td>
<td>"media/birds10/00001.png"</td>
<td>"train"</td>
<td>[0.024804, -0.045255, … -0.007233]</td>
<td>[-0.033767, -0.011978, … -0.020352]</td>
<td>135.112989</td>
</tr>
<tr class="odd">
<td>"canary"</td>
<td>"media/birds10/00002.png"</td>
<td>"train"</td>
<td>[0.050587, -0.024486, … 0.029895]</td>
<td>[-0.033664, -0.008117, … -0.01725]</td>
<td>97.398424</td>
</tr>
<tr class="even">
<td>"canary"</td>
<td>"media/birds10/00003.png"</td>
<td>"train"</td>
<td>[0.047036, -0.038993, … -0.008446]</td>
<td>[-0.010029, -0.018192, … -0.009869]</td>
<td>97.469866</td>
</tr>
<tr class="odd">
<td>"canary"</td>
<td>"media/birds10/00004.png"</td>
<td>"train"</td>
<td>[0.036349, -0.02734, … -0.018185]</td>
<td>[-0.027327, 0.003568, … -0.033407]</td>
<td>144.174758</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>"swallow"</td>
<td>"media/birds10/01550.png"</td>
<td>"train"</td>
<td>[-0.022461, -0.025098, … -0.061945]</td>
<td>[-0.022029, -0.008476, … -0.003879]</td>
<td>119.851436</td>
</tr>
<tr class="even">
<td>"swallow"</td>
<td>"media/birds10/01551.png"</td>
<td>"train"</td>
<td>[-0.000212, -0.003448, … -0.058042]</td>
<td>[-0.02476, -0.016369, … 0.003875]</td>
<td>151.026155</td>
</tr>
<tr class="odd">
<td>"swallow"</td>
<td>"media/birds10/01552.png"</td>
<td>"train"</td>
<td>[-0.012531, -0.006788, … -0.047077]</td>
<td>[-0.022153, 0.00765, … -0.011152]</td>
<td>156.078922</td>
</tr>
<tr class="even">
<td>"swallow"</td>
<td>"media/birds10/01553.png"</td>
<td>"test"</td>
<td>[-0.007587, -0.053535, … -0.046395]</td>
<td>[-0.005022, 0.00878, … -0.017564]</td>
<td>118.326265</td>
</tr>
<tr class="odd">
<td>"swallow"</td>
<td>"media/birds10/01554.png"</td>
<td>"train"</td>
<td>[-0.01325, -0.032453, … -0.050751]</td>
<td>[-0.015117, -0.00685, … -0.029756]</td>
<td>156.02357</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Now we have a brightness value for each image. What can this simple statistic tell us about our photographs? Let’s examine the darkest images in the collection.</p>
<div id="5a99ed8c" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    birds</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    .sort(c.brightness)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    .head(<span class="dv">12</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    .pipe(DSImage.plot_image_grid, ncol<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-10-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="20_image_data_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>We see that the darkest images tend to have black backgrounds. This makes sense: a large area of pure black pixels (value 0) dramatically reduces the mean. The birds themselves may be brightly colored, but the dark backgrounds dominate the overall brightness calculation.</p>
<p>The brightest images show the opposite pattern.</p>
<div id="4d4bc39f" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    birds</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    .sort(c.brightness, descending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    .head(<span class="dv">12</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    .pipe(DSImage.plot_image_grid, ncol<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-11-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="20_image_data_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>These images feature white or light-colored backgrounds, which push the mean toward higher values. This observation reveals an important limitation of simple pixel statistics: they capture properties of the entire image, including background, lighting conditions, and photographic style, not just the subject we care about. A dark bird photographed against a white background will have a higher brightness score than the same bird against a dark background.</p>
<p>Despite this limitation, brightness can reveal interesting patterns across categories. Here are the distributions of brightness by species.</p>
<div id="1605342e" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    birds</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    .pipe(ggplot, aes(<span class="st">"reorder(label, brightness)"</span>, <span class="st">"brightness"</span>))</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_boxplot()</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-12-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="20_image_data_files/figure-html/cell-12-output-1.png" width="672" height="480" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Ostriches have brighter images on average, followed by ducks and robins. Peacock images are the darkest. Why might this be? Consider how each species is typically photographed. Ostriches are often captured outdoors in bright savanna settings, while peacocks with their dark iridescent plumage are sometimes photographed against dramatic dark backgrounds that emphasize their colorful displays. The brightness distribution reflects not just the birds themselves but the entire photographic context.</p>
</section>
<section id="hue-saturation-and-value" class="level2" data-number="20.5">
<h2 data-number="20.5" class="anchored" data-anchor-id="hue-saturation-and-value"><span class="header-section-number">20.5</span> Hue, Saturation, and Value</h2>
<p>While brightness provides a single summary statistic, the RGB color model used by digital images is not ideal for describing color as humans perceive it. The HSV (Hue, Saturation, Value) color model offers a more intuitive representation that separates color information into three distinct components.</p>
<p><strong>Hue</strong> represents the pure color itself, independent of how light or vivid it appears. Hue is measured as an angle around a color wheel, typically ranging from 0° to 360°. Red appears at 0° (and wraps around to 360°), yellow at 60°, green at 120°, cyan at 180°, blue at 240°, and magenta at 300°. This circular representation explains why red and magenta appear adjacent: they are neighbors on the color wheel.</p>
<p><strong>Saturation</strong> measures the purity or intensity of a color. A fully saturated color contains no gray: it is vivid and intense. As saturation decreases, the color becomes more washed out, eventually becoming a pure gray at zero saturation. Saturation ranges from 0 (completely desaturated, gray) to 1 (fully saturated, pure color).</p>
<p><strong>Value</strong> (also called brightness or lightness in related color models) indicates how light or dark the color is. A value of 0 produces black regardless of hue or saturation, while a value of 1 produces the brightest possible version of that hue and saturation combination.</p>
<p>The mathematical conversion from RGB to HSV proceeds as follows. Given RGB values normalized to the range [0, 1], we first compute:</p>
<p><span class="math display">\[
\begin{aligned}
M &amp;= \max(R, G, B) \\
m &amp;= \min(R, G, B) \\
C &amp;= M - m
\end{aligned}
\]</span></p>
<p>The value <span class="math inline">\(M\)</span> is the maximum of the three channels, <span class="math inline">\(m\)</span> is the minimum, and <span class="math inline">\(C\)</span> (chroma) measures the range. The HSV components are then calculated as:</p>
<p><span class="math display">\[
V = M
\]</span></p>
<p><span class="math display">\[
S = \begin{cases}
0 &amp; \text{if } V = 0 \\
\frac{C}{V} &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p><span class="math display">\[
H = \begin{cases}
0° &amp; \text{if } C = 0 \\
60° \times \frac{G - B}{C} \mod 360° &amp; \text{if } M = R \\
60° \times \left(\frac{B - R}{C} + 2\right) &amp; \text{if } M = G \\
60° \times \left(\frac{R - G}{C} + 4\right) &amp; \text{if } M = B
\end{cases}
\]</span></p>
<p>The value component is simply the maximum RGB value. Saturation measures how much the color differs from gray by comparing the range of RGB values to the maximum. Hue is computed by determining which RGB component is dominant and calculating the position around the color wheel.</p>
<p>OpenCV provides built-in functions for color space conversion. Here is how to convert an image from BGR to HSV.</p>
<div id="646196f3" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> cv2.imread(birds.select(pl.col(<span class="st">"filepath"</span>).get(<span class="dv">0</span>)).item())</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>img_hsv <span class="op">=</span> cv2.cvtColor(img, cv2.COLOR_BGR2HSV)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>img_hsv.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>(224, 224, 3)</code></pre>
</div>
</div>
<p>The resulting array has the same shape as the original, but now the three channels represent hue, saturation, and value instead of blue, green, and red. Note that OpenCV scales hue to the range [0, 179] (to fit in an 8-bit value while covering the full 360° range) and saturation and value to [0, 255].</p>
</section>
<section id="colors" class="level2" data-number="20.6">
<h2 data-number="20.6" class="anchored" data-anchor-id="colors"><span class="header-section-number">20.6</span> Colors</h2>
<p>With images represented in HSV color space, we can classify pixels by their dominant color. The hue value tells us where each pixel falls on the color wheel, allowing us to count how many pixels are predominantly red, orange, yellow, green, and so on.</p>
<p>Our helper method <code>DSImage.compute_colors</code> takes an HSV image and returns a dictionary with the proportion of pixels falling into each color category. It bins the hue values into segments corresponding to common color names and also accounts for the saturation and value (very dark or desaturated pixels are classified separately as black, white, or gray).</p>
<div id="33fea9c9" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>DSImage.compute_colors(img_hsv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>{'red': np.float64(6.048708545918367),
 'orange': np.float64(8.326690051020408),
 'yellow': np.float64(9.394929846938775),
 'green': np.float64(0.06377551020408163),
 'cyan': np.float64(0.005978954081632653),
 'blue': np.float64(1.0981345663265305),
 'purple': np.float64(0.007971938775510204),
 'magenta': np.float64(0.04185267857142857),
 'neutral': np.float64(75.01195790816327)}</code></pre>
</div>
</div>
<p>The output shows what fraction of pixels in this image fall into each color category. The proportions sum to 1, giving us a complete description of the color distribution.</p>
<p>We will cycle through the entire DataFrame and compute these color proportions for all images.</p>
<div id="98e0a298" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> row <span class="kw">in</span> birds.iter_rows(named<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> cv2.imread(row[<span class="st">"filepath"</span>])</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    img_hsv <span class="op">=</span> cv2.cvtColor(img, cv2.COLOR_BGR2HSV)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    results.append(DSImage.compute_colors(img_hsv))</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> pl.DataFrame(results)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>birds <span class="op">=</span> pl.concat([birds, results], how<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>birds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (1_555, 15)</small>
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">label</th>
<th data-quarto-table-cell-role="th">filepath</th>
<th data-quarto-table-cell-role="th">index</th>
<th data-quarto-table-cell-role="th">vit</th>
<th data-quarto-table-cell-role="th">siglip</th>
<th data-quarto-table-cell-role="th">brightness</th>
<th data-quarto-table-cell-role="th">red</th>
<th data-quarto-table-cell-role="th">orange</th>
<th data-quarto-table-cell-role="th">yellow</th>
<th data-quarto-table-cell-role="th">green</th>
<th data-quarto-table-cell-role="th">cyan</th>
<th data-quarto-table-cell-role="th">blue</th>
<th data-quarto-table-cell-role="th">purple</th>
<th data-quarto-table-cell-role="th">magenta</th>
<th data-quarto-table-cell-role="th">neutral</th>
</tr>
<tr class="even">
<th>str</th>
<th>str</th>
<th>str</th>
<th>list[f64]</th>
<th>list[f64]</th>
<th>f64</th>
<th>f64</th>
<th>f64</th>
<th>f64</th>
<th>f64</th>
<th>f64</th>
<th>f64</th>
<th>f64</th>
<th>f64</th>
<th>f64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"canary"</td>
<td>"media/birds10/00000.png"</td>
<td>"test"</td>
<td>[0.017947, -0.0305, … 0.003526]</td>
<td>[-0.00611, -0.042975, … -0.031687]</td>
<td>119.867168</td>
<td>6.048709</td>
<td>8.32669</td>
<td>9.39493</td>
<td>0.063776</td>
<td>0.005979</td>
<td>1.098135</td>
<td>0.007972</td>
<td>0.041853</td>
<td>75.011958</td>
</tr>
<tr class="even">
<td>"canary"</td>
<td>"media/birds10/00001.png"</td>
<td>"train"</td>
<td>[0.024804, -0.045255, … -0.007233]</td>
<td>[-0.033767, -0.011978, … -0.020352]</td>
<td>135.112989</td>
<td>8.968431</td>
<td>23.794244</td>
<td>6.806043</td>
<td>1.7578125</td>
<td>1.480788</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>57.192682</td>
</tr>
<tr class="odd">
<td>"canary"</td>
<td>"media/birds10/00002.png"</td>
<td>"train"</td>
<td>[0.050587, -0.024486, … 0.029895]</td>
<td>[-0.033664, -0.008117, … -0.01725]</td>
<td>97.398424</td>
<td>0.01993</td>
<td>30.29536</td>
<td>52.800143</td>
<td>0.049825</td>
<td>0.009965</td>
<td>0.932717</td>
<td>0.0</td>
<td>0.0</td>
<td>15.89206</td>
</tr>
<tr class="even">
<td>"canary"</td>
<td>"media/birds10/00003.png"</td>
<td>"train"</td>
<td>[0.047036, -0.038993, … -0.008446]</td>
<td>[-0.010029, -0.018192, … -0.009869]</td>
<td>97.469866</td>
<td>0.0</td>
<td>26.546556</td>
<td>64.339525</td>
<td>0.007972</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>9.105947</td>
</tr>
<tr class="odd">
<td>"canary"</td>
<td>"media/birds10/00004.png"</td>
<td>"train"</td>
<td>[0.036349, -0.02734, … -0.018185]</td>
<td>[-0.027327, 0.003568, … -0.033407]</td>
<td>144.174758</td>
<td>0.288983</td>
<td>96.593989</td>
<td>0.454401</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>2.662628</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>"swallow"</td>
<td>"media/birds10/01550.png"</td>
<td>"train"</td>
<td>[-0.022461, -0.025098, … -0.061945]</td>
<td>[-0.022029, -0.008476, … -0.003879]</td>
<td>119.851436</td>
<td>0.061783</td>
<td>14.807876</td>
<td>0.655692</td>
<td>0.063776</td>
<td>0.121572</td>
<td>57.553412</td>
<td>0.0</td>
<td>0.0</td>
<td>26.73589</td>
</tr>
<tr class="even">
<td>"swallow"</td>
<td>"media/birds10/01551.png"</td>
<td>"train"</td>
<td>[-0.000212, -0.003448, … -0.058042]</td>
<td>[-0.02476, -0.016369, … 0.003875]</td>
<td>151.026155</td>
<td>0.691566</td>
<td>2.451371</td>
<td>24.447943</td>
<td>0.089684</td>
<td>0.187341</td>
<td>3.386081</td>
<td>0.0</td>
<td>0.027902</td>
<td>68.718112</td>
</tr>
<tr class="odd">
<td>"swallow"</td>
<td>"media/birds10/01552.png"</td>
<td>"train"</td>
<td>[-0.012531, -0.006788, … -0.047077]</td>
<td>[-0.022153, 0.00765, … -0.011152]</td>
<td>156.078922</td>
<td>0.185348</td>
<td>11.820392</td>
<td>5.369101</td>
<td>0.053811</td>
<td>0.159439</td>
<td>64.845743</td>
<td>0.0</td>
<td>0.0</td>
<td>17.566167</td>
</tr>
<tr class="even">
<td>"swallow"</td>
<td>"media/birds10/01553.png"</td>
<td>"test"</td>
<td>[-0.007587, -0.053535, … -0.046395]</td>
<td>[-0.005022, 0.00878, … -0.017564]</td>
<td>118.326265</td>
<td>0.007972</td>
<td>0.239158</td>
<td>0.159439</td>
<td>0.819117</td>
<td>0.185348</td>
<td>5.624203</td>
<td>0.0</td>
<td>0.0</td>
<td>92.964764</td>
</tr>
<tr class="odd">
<td>"swallow"</td>
<td>"media/birds10/01554.png"</td>
<td>"train"</td>
<td>[-0.01325, -0.032453, … -0.050751]</td>
<td>[-0.015117, -0.00685, … -0.029756]</td>
<td>156.02357</td>
<td>1.052296</td>
<td>20.938297</td>
<td>0.187341</td>
<td>0.155453</td>
<td>0.075733</td>
<td>0.277025</td>
<td>0.0</td>
<td>0.001993</td>
<td>77.311862</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Now our dataset includes columns for each color category, enabling systematic analysis of color distributions across species. Let’s examine which images contain the most yellow.</p>
<div id="f527b8d4" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    birds</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    .sort(c.yellow, descending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    .head(<span class="dv">12</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    .pipe(DSImage.plot_image_grid, ncol<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-16-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="20_image_data_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The images with the highest yellow proportions include many hummingbirds photographed near yellow-green foliage, as well as the expected yellow birds. This demonstrates how color analysis captures not just the subject but the entire scene.</p>
<p>We can compare the yellow content across species systematically.</p>
<div id="5a0c11cc" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    birds</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    .pipe(ggplot, aes(<span class="st">"reorder(label, yellow)"</span>, <span class="st">"yellow"</span>))</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_boxplot()</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-17-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="20_image_data_files/figure-html/cell-17-output-1.png" width="672" height="480" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Canaries, as expected, have the highest yellow content on average. Their bright yellow plumage dominates the images. Other species show less yellow, with the distribution reflecting both the birds’ coloration and their typical photographic environments.</p>
<p>Peacocks, known for their brilliant blue-green displays, should show high values in the cyan range.</p>
<div id="5ffc769f" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    birds</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    .pipe(ggplot, aes(<span class="st">"reorder(label, cyan)"</span>, <span class="st">"cyan"</span>))</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_boxplot()</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-18-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="20_image_data_files/figure-html/cell-18-output-1.png" width="672" height="480" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Indeed, peacocks lead in cyan content, reflecting their characteristic iridescent feathers that shimmer between blue and green. Parrots also show substantial cyan, while species like robins and canaries have minimal cyan in their typical coloration.</p>
<p>These color features, while simple to compute, provide useful descriptors for organizing and exploring image collections. They can identify outliers (an unusually red canary might indicate a different subspecies or unusual lighting), reveal patterns across categories, and serve as features for downstream machine learning tasks.</p>
</section>
<section id="object-detection" class="level2" data-number="20.7">
<h2 data-number="20.7" class="anchored" data-anchor-id="object-detection"><span class="header-section-number">20.7</span> Object Detection</h2>
<p>In <a href="15_transferlearn.html" class="quarto-xref"><span>Chapter 15</span></a> we saw how to use pre-trained deep learning models for image-level predictions, classifying entire images into categories. Object detection extends this capability by identifying and locating multiple objects within a single image. Rather than asking “what is in this image?”, object detection asks “what objects are in this image, and where are they?”</p>
<p>Modern object detection models output a set of bounding boxes, each consisting of four coordinates that define a rectangle enclosing a detected object, along with a class label and a confidence score. The model might report: “there is a person at coordinates (100, 50) to (200, 300) with 95% confidence, and a dog at (250, 100) to (350, 250) with 87% confidence.”</p>
<p>The YOLO (You Only Look Once) family of models represents the state of the art in real-time object detection. These models process entire images in a single forward pass through the network, making them remarkably fast while maintaining high accuracy. The architecture divides the image into a grid, with each cell responsible for predicting objects centered in that cell. This design enables the model to reason globally about the image while making localized predictions.</p>
<p>Training an object detection model requires labeled data where humans have drawn bounding boxes around objects and assigned class labels. The loss function combines several components:</p>
<p><span class="math display">\[
\mathcal{L} = \lambda_{\text{coord}} \mathcal{L}_{\text{box}} + \lambda_{\text{obj}} \mathcal{L}_{\text{obj}} + \lambda_{\text{cls}} \mathcal{L}_{\text{cls}}
\]</span></p>
<p>The box loss <span class="math inline">\(\mathcal{L}_{\text{box}}\)</span> penalizes errors in the predicted bounding box coordinates. Modern implementations often use Complete Intersection over Union (CIoU) loss, which considers the overlap between predicted and ground-truth boxes along with the distance between their centers and aspect ratio consistency:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{CIoU}} = 1 - \text{IoU} + \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v
\]</span></p>
<p>where IoU is the intersection over union of the boxes, <span class="math inline">\(\rho(b, b^{gt})\)</span> is the Euclidean distance between box centers, <span class="math inline">\(c\)</span> is the diagonal length of the smallest enclosing box, and <span class="math inline">\(v\)</span> measures aspect ratio consistency.</p>
<p>The objectness loss <span class="math inline">\(\mathcal{L}_{\text{obj}}\)</span> trains the model to predict whether each grid cell contains an object. The classification loss <span class="math inline">\(\mathcal{L}_{\text{cls}}\)</span> trains the model to correctly identify the class of detected objects. The <span class="math inline">\(\lambda\)</span> coefficients balance these components during training.</p>
<p>Let’s load a pre-trained YOLO model and apply it to historical photographs.</p>
<div id="e60af6a4" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">"yolo11n.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For this analysis, we will use a collection of color photographs from the FSA-OWI (Farm Security Administration - Office of War Information) project, a remarkable documentary photography initiative from the 1930s and 1940s. These images captured American life during the Great Depression and World War II.</p>
<div id="08f1d03d" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>fsac</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (500, 2)</small>
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">filepath</th>
<th data-quarto-table-cell-role="th">photographer</th>
</tr>
<tr class="even">
<th>str</th>
<th>str</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"media/fsac/1a35266v.jpg"</td>
<td>"Alfred T. Palmer"</td>
</tr>
<tr class="even">
<td>"media/fsac/1a34940v.jpg"</td>
<td>"Howard R. Hollem"</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34143v.jpg"</td>
<td>"Russell Lee"</td>
</tr>
<tr class="even">
<td>"media/fsac/1a35375v.jpg"</td>
<td>"Alfred T. Palmer"</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34758v.jpg"</td>
<td>"Jack Delano"</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34359v.jpg"</td>
<td>"Marion Post Wolcott"</td>
</tr>
<tr class="even">
<td>"media/fsac/1a34893v.jpg"</td>
<td>"Howard R. Hollem"</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34100v.jpg"</td>
<td>"Russell Lee"</td>
</tr>
<tr class="even">
<td>"media/fsac/1a35045v.jpg"</td>
<td>"Howard Liberman"</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34863v.jpg"</td>
<td>"Howard R. Hollem"</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Here is an example of running object detection on one of these historical photographs.</p>
<div id="12fc4c43" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> model.predict(<span class="st">"media/fsac/1a35210v.jpg"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
image 1/1 /Users/admin/gh/fds-py/media/fsac/1a35210v.jpg: 544x640 3 persons, 33.8ms
Speed: 1.0ms preprocess, 33.8ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 640)</code></pre>
</div>
</div>
<p>The model returns predictions that include bounding boxes, class labels, and confidence scores. We can visualize the detections by overlaying them on the original image.</p>
<div id="5f12bd53" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>rgb <span class="op">=</span> cv2.cvtColor(pred[<span class="dv">0</span>].plot(), cv2.COLOR_BGR2RGB)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>pil_img <span class="op">=</span> Image.fromarray(rgb)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>pil_img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-22-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="20_image_data_files/figure-html/cell-22-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The visualization shows bounding boxes around detected objects, each labeled with the predicted class and confidence score. The model has been trained on the COCO dataset, which includes 80 common object categories such as person, car, dog, chair, and bicycle.</p>
<p>Let’s systematically count the number of people detected in each image across our collection.</p>
<div id="9251f094" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> row <span class="kw">in</span> fsac.iter_rows(named<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> model.predict(row[<span class="st">"filepath"</span>], verbose<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>]</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> preds.boxes <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>        results.append(<span class="dv">0</span>)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    cls_ids <span class="op">=</span> preds.boxes.cls.tolist()</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> preds.names</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    results.append(<span class="bu">sum</span>(names[<span class="bu">int</span>(c)] <span class="op">==</span> <span class="st">"person"</span> <span class="cf">for</span> c <span class="kw">in</span> cls_ids))</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>fsac <span class="op">=</span> fsac.with_columns(people <span class="op">=</span> pl.Series(results))</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>fsac</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (500, 3)</small>
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">filepath</th>
<th data-quarto-table-cell-role="th">photographer</th>
<th data-quarto-table-cell-role="th">people</th>
</tr>
<tr class="even">
<th>str</th>
<th>str</th>
<th>i64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"media/fsac/1a35266v.jpg"</td>
<td>"Alfred T. Palmer"</td>
<td>0</td>
</tr>
<tr class="even">
<td>"media/fsac/1a34940v.jpg"</td>
<td>"Howard R. Hollem"</td>
<td>0</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34143v.jpg"</td>
<td>"Russell Lee"</td>
<td>10</td>
</tr>
<tr class="even">
<td>"media/fsac/1a35375v.jpg"</td>
<td>"Alfred T. Palmer"</td>
<td>1</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34758v.jpg"</td>
<td>"Jack Delano"</td>
<td>0</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34359v.jpg"</td>
<td>"Marion Post Wolcott"</td>
<td>8</td>
</tr>
<tr class="even">
<td>"media/fsac/1a34893v.jpg"</td>
<td>"Howard R. Hollem"</td>
<td>1</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34100v.jpg"</td>
<td>"Russell Lee"</td>
<td>1</td>
</tr>
<tr class="even">
<td>"media/fsac/1a35045v.jpg"</td>
<td>"Howard Liberman"</td>
<td>0</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34863v.jpg"</td>
<td>"Howard R. Hollem"</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>We can examine the images with the most detected people.</p>
<div id="866ae11e" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    fsac</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    .sort(c.people, descending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    .head(<span class="dv">8</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    .pipe(DSImage.plot_image_grid, ncol<span class="op">=</span><span class="dv">4</span>, label_name<span class="op">=</span><span class="st">"people"</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-24-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="20_image_data_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The model successfully identifies crowd scenes, though the exact count becomes less reliable as the number of people increases. In dense crowds, overlapping individuals and partial occlusions make precise counting challenging. Nevertheless, the detection provides a useful proxy for the scale of human presence in each photograph.</p>
<p>The number of people varies considerably by photographer, reflecting different documentary styles and subject matter choices.</p>
<div id="d0b03760" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    fsac</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    .pipe(ggplot, aes(<span class="st">"reorder(photographer, people)"</span>, <span class="st">"people"</span>))</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_boxplot()</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> coord_flip()</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-25-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="20_image_data_files/figure-html/cell-25-output-1.png" width="672" height="480" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Some photographers specialized in intimate portraits with one or two subjects, while others captured street scenes and public gatherings. These patterns emerge clearly from the automated analysis.</p>
</section>
<section id="segmentation" class="level2" data-number="20.8">
<h2 data-number="20.8" class="anchored" data-anchor-id="segmentation"><span class="header-section-number">20.8</span> Segmentation</h2>
<p>Object detection tells us where objects are, but treats each detection as a simple rectangle. Instance segmentation goes further by identifying the exact pixels that belong to each object. Instead of a bounding box, segmentation produces a mask—a binary image where pixels belonging to the object are marked as 1 and background pixels as 0.</p>
<p>Segmentation models build on object detection architectures by adding a mask prediction branch. For each detected object, the model predicts not just a bounding box but also a pixel-wise mask within that box. The training loss includes an additional component for mask accuracy:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{mask}} = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
\]</span></p>
<p>This is the binary cross-entropy loss computed over all <span class="math inline">\(N\)</span> pixels in the mask region, where <span class="math inline">\(y_i\)</span> is the ground-truth label (1 if the pixel belongs to the object, 0 otherwise) and <span class="math inline">\(\hat{y}_i\)</span> is the predicted probability. The loss penalizes both false positives (predicting object when the pixel is background) and false negatives (predicting background when the pixel is object).</p>
<p>YOLO includes segmentation variants that maintain real-time performance while producing pixel-accurate masks.</p>
<div id="37a766a7" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">"yolo11n-seg.pt"</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> model.predict(<span class="st">"media/fsac/1a35210v.jpg"</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>rgb <span class="op">=</span> cv2.cvtColor(pred[<span class="dv">0</span>].plot(), cv2.COLOR_BGR2RGB)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>pil_img <span class="op">=</span> Image.fromarray(rgb)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>pil_img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
image 1/1 /Users/admin/gh/fds-py/media/fsac/1a35210v.jpg: 544x640 3 persons, 43.9ms
Speed: 0.7ms preprocess, 43.9ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 640)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="25">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-26-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="20_image_data_files/figure-html/cell-26-output-2.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The visualization now shows colored masks overlaid on detected objects, precisely delineating their boundaries rather than just enclosing them in rectangles. This pixel-level precision enables more nuanced analysis of image content.</p>
<p>We can use segmentation to compute what percentage of each image is occupied by people, providing a measure of how prominently human figures feature in the composition.</p>
<div id="aa6cdabd" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> row <span class="kw">in</span> fsac.iter_rows(named<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> model.predict(row[<span class="st">"filepath"</span>], verbose<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>]</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    cls_ids <span class="op">=</span> preds.boxes.cls.to(<span class="st">"cpu"</span>).numpy().astype(<span class="bu">int</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> preds.names</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    person_idx <span class="op">=</span> np.where(np.array([names[c] <span class="op">==</span> <span class="st">"person"</span> <span class="cf">for</span> c <span class="kw">in</span> cls_ids]))[<span class="dv">0</span>]</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> person_idx.size <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>        results.append(<span class="fl">0.0</span>)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    masks <span class="op">=</span> preds.masks.data[person_idx]</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    union <span class="op">=</span> masks.<span class="bu">any</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    coverage <span class="op">=</span> union.<span class="bu">float</span>().mean().item()</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    results.append(coverage <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>fsac <span class="op">=</span> fsac.with_columns(people_prop<span class="op">=</span>pl.Series(results))</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>fsac</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (500, 4)</small>
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">filepath</th>
<th data-quarto-table-cell-role="th">photographer</th>
<th data-quarto-table-cell-role="th">people</th>
<th data-quarto-table-cell-role="th">people_prop</th>
</tr>
<tr class="even">
<th>str</th>
<th>str</th>
<th>i64</th>
<th>f64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"media/fsac/1a35266v.jpg"</td>
<td>"Alfred T. Palmer"</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>"media/fsac/1a34940v.jpg"</td>
<td>"Howard R. Hollem"</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34143v.jpg"</td>
<td>"Russell Lee"</td>
<td>10</td>
<td>3.520182</td>
</tr>
<tr class="even">
<td>"media/fsac/1a35375v.jpg"</td>
<td>"Alfred T. Palmer"</td>
<td>1</td>
<td>19.758731</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34758v.jpg"</td>
<td>"Jack Delano"</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34359v.jpg"</td>
<td>"Marion Post Wolcott"</td>
<td>8</td>
<td>28.831056</td>
</tr>
<tr class="even">
<td>"media/fsac/1a34893v.jpg"</td>
<td>"Howard R. Hollem"</td>
<td>1</td>
<td>8.354187</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34100v.jpg"</td>
<td>"Russell Lee"</td>
<td>1</td>
<td>3.764648</td>
</tr>
<tr class="even">
<td>"media/fsac/1a35045v.jpg"</td>
<td>"Howard Liberman"</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34863v.jpg"</td>
<td>"Howard R. Hollem"</td>
<td>0</td>
<td>0.0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>The <code>people_prop</code> column now contains the percentage of each image covered by detected people. Let’s see which images have the highest human coverage.</p>
<div id="906c59a7" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    fsac</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    .sort(c.people_prop, descending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    .head(<span class="dv">8</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    .pipe(DSImage.plot_image_grid, ncol<span class="op">=</span><span class="dv">4</span>, label_name<span class="op">=</span><span class="st">"people_prop"</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-28-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="20_image_data_files/figure-html/cell-28-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>These tend to be closely framed portraits where the subject fills most of the frame. The segmentation-based measure captures a different aspect of photographic style than simple person counts: a single person in a tight portrait can have higher coverage than a dozen people in a wide street scene.</p>
<p>The relationship between person coverage and photographer reveals stylistic differences.</p>
<div id="8b8e30d3" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    fsac</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    .group_by(c.photographer)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    .agg(avg_people <span class="op">=</span> c.people_prop.mean())</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    .pipe(ggplot, aes(<span class="st">"avg_people"</span>, <span class="st">"reorder(photographer, avg_people)"</span>))</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_point()</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-29-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="20_image_data_files/figure-html/cell-29-output-1.png" width="672" height="480" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Photographers with higher average coverage tended toward portrait work, while those with lower coverage may have focused on environmental scenes, architecture, or wide-angle documentary shots. The measure is not always inversely related to person count; some photographers captured groups in intimate settings with high coverage, while others documented single individuals in expansive landscapes.</p>
</section>
<section id="pose-detection" class="level2" data-number="20.9">
<h2 data-number="20.9" class="anchored" data-anchor-id="pose-detection"><span class="header-section-number">20.9</span> Pose Detection</h2>
<p>Human pose estimation identifies the locations of body parts in an image, typically represented as a set of keypoints corresponding to joints like shoulders, elbows, wrists, hips, knees, and ankles. These keypoints, connected according to human anatomy, form a “skeleton” that describes body position and posture.</p>
<p>Pose estimation models predict coordinates for each keypoint along with confidence scores indicating detection reliability. The COCO pose format defines 17 keypoints: nose, left/right eyes, left/right ears, left/right shoulders, left/right elbows, left/right wrists, left/right hips, left/right knees, and left/right ankles.</p>
<p>The training loss for pose estimation typically combines localization accuracy with visibility prediction:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{pose}} = \sum_{k=1}^{K} v_k \cdot \left\| \mathbf{p}_k - \hat{\mathbf{p}}_k \right\|^2
\]</span></p>
<p>where <span class="math inline">\(K\)</span> is the number of keypoints, <span class="math inline">\(v_k\)</span> is a visibility flag (1 if the keypoint is visible, 0 otherwise), <span class="math inline">\(\mathbf{p}_k\)</span> is the ground-truth position, and <span class="math inline">\(\hat{\mathbf{p}}_k\)</span> is the predicted position. This formulation only penalizes errors on visible keypoints, acknowledging that occluded body parts cannot be reliably localized.</p>
<p>Advanced pose estimation models use heatmap-based representations, predicting a probability distribution over possible keypoint locations:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{heatmap}} = \sum_{k=1}^{K} \sum_{(x,y)} \left( H_k(x, y) - \hat{H}_k(x, y) \right)^2
\]</span></p>
<p>where <span class="math inline">\(H_k(x, y)\)</span> is the ground-truth heatmap (typically a 2D Gaussian centered on the keypoint) and <span class="math inline">\(\hat{H}_k(x, y)\)</span> is the predicted heatmap.</p>
<p>Let’s apply pose detection to our historical photographs.</p>
<div id="39363de2" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">"yolo11n-pose.pt"</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> model.predict(<span class="st">"media/fsac/1a35210v.jpg"</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>rgb <span class="op">=</span> cv2.cvtColor(pred[<span class="dv">0</span>].plot(), cv2.COLOR_BGR2RGB)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>pil_img <span class="op">=</span> Image.fromarray(rgb)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>pil_img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
image 1/1 /Users/admin/gh/fds-py/media/fsac/1a35210v.jpg: 544x640 3 persons, 35.0ms
Speed: 0.7ms preprocess, 35.0ms inference, 0.4ms postprocess per image at shape (1, 3, 544, 640)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="29">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-30-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="20_image_data_files/figure-html/cell-30-output-2.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The visualization shows detected poses as skeleton overlays, with lines connecting keypoints according to anatomical structure. The colored points indicate individual keypoints, while the connecting lines show the body structure.</p>
<p>We can use pose information to analyze photographic composition in sophisticated ways. Let’s compute the proportion of image height occupied by the torso of the largest detected person. This metric indicates how prominently human figures are framed in each photograph.</p>
<div id="389089a2" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> row <span class="kw">in</span> fsac.iter_rows(named<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> model.predict(row[<span class="st">"filepath"</span>], verbose<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>]</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> preds.keypoints <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>        results.append(<span class="fl">0.0</span>)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>    img_h <span class="op">=</span> preds.orig_img.shape[<span class="dv">0</span>]</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    xy <span class="op">=</span> preds.keypoints.xy</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    conf <span class="op">=</span> preds.keypoints.conf</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> conf <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>        results.append(<span class="fl">0.0</span>)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>    xy <span class="op">=</span> xy.cpu().numpy()</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>    conf <span class="op">=</span> conf.cpu().numpy()</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>    best_torso <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(xy.shape[<span class="dv">0</span>]):</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>        kpt_confs <span class="op">=</span> conf[i, [<span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">11</span>, <span class="dv">12</span>]]</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.<span class="bu">any</span>(kpt_confs <span class="op">&lt;</span> <span class="fl">0.5</span>):</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>        mid_sh <span class="op">=</span> xy[i, [<span class="dv">5</span>, <span class="dv">6</span>], :].mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>        mid_hip <span class="op">=</span> xy[i, [<span class="dv">11</span>, <span class="dv">12</span>], :].mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>        torso_len <span class="op">=</span> <span class="bu">float</span>(np.linalg.norm(mid_sh <span class="op">-</span> mid_hip))</span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> torso_len <span class="op">&gt;</span> best_torso:</span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>            best_torso <span class="op">=</span> torso_len</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>    torso_pct <span class="op">=</span> (best_torso <span class="op">/</span> img_h) <span class="op">*</span> <span class="fl">100.0</span> <span class="cf">if</span> best_torso <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>    results.append(torso_pct)</span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>fsac <span class="op">=</span> fsac.with_columns(torso<span class="op">=</span>pl.Series(results))</span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a>fsac</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (500, 5)</small>
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">filepath</th>
<th data-quarto-table-cell-role="th">photographer</th>
<th data-quarto-table-cell-role="th">people</th>
<th data-quarto-table-cell-role="th">people_prop</th>
<th data-quarto-table-cell-role="th">torso</th>
</tr>
<tr class="even">
<th>str</th>
<th>str</th>
<th>i64</th>
<th>f64</th>
<th>f64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"media/fsac/1a35266v.jpg"</td>
<td>"Alfred T. Palmer"</td>
<td>0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>"media/fsac/1a34940v.jpg"</td>
<td>"Howard R. Hollem"</td>
<td>0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34143v.jpg"</td>
<td>"Russell Lee"</td>
<td>10</td>
<td>3.520182</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>"media/fsac/1a35375v.jpg"</td>
<td>"Alfred T. Palmer"</td>
<td>1</td>
<td>19.758731</td>
<td>47.362576</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34758v.jpg"</td>
<td>"Jack Delano"</td>
<td>0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34359v.jpg"</td>
<td>"Marion Post Wolcott"</td>
<td>8</td>
<td>28.831056</td>
<td>21.056602</td>
</tr>
<tr class="even">
<td>"media/fsac/1a34893v.jpg"</td>
<td>"Howard R. Hollem"</td>
<td>1</td>
<td>8.354187</td>
<td>24.024989</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34100v.jpg"</td>
<td>"Russell Lee"</td>
<td>1</td>
<td>3.764648</td>
<td>11.063779</td>
</tr>
<tr class="even">
<td>"media/fsac/1a35045v.jpg"</td>
<td>"Howard Liberman"</td>
<td>0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>"media/fsac/1a34863v.jpg"</td>
<td>"Howard R. Hollem"</td>
<td>0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>The computation identifies the shoulder and hip keypoints (indices 5, 6 for shoulders and 11, 12 for hips), calculates the midpoint of each pair, and measures the distance between them. This torso length, expressed as a percentage of image height, indicates how large human subjects appear in the frame.</p>
<div id="7a2e5ec1" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    fsac</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    .sort(c.torso, descending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    .head(<span class="dv">8</span>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    .pipe(DSImage.plot_image_grid, ncol<span class="op">=</span><span class="dv">4</span>, label_name<span class="op">=</span><span class="st">"torso"</span>)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-32-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="20_image_data_files/figure-html/cell-32-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The images with the highest torso proportions are medium shots of workers and subjects, framed tightly enough that the torso occupies a substantial portion of the vertical space. These are neither extreme close-ups (which might exclude the torso entirely) nor distant shots (where the full figure would be small).</p>
<p>This metric shows strong variation by photographer.</p>
<div id="6d7d703a" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    fsac</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">filter</span>(c.torso <span class="op">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    .group_by(c.photographer)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    .agg(torso_mean <span class="op">=</span> c.torso.mean(), n <span class="op">=</span> pl.<span class="bu">len</span>())</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">filter</span>(c.n <span class="op">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    .pipe(ggplot, aes(<span class="st">"torso_mean"</span>, <span class="st">"reorder(photographer, torso_mean)"</span>))</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_point()</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-33-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img src="20_image_data_files/figure-html/cell-33-output-1.png" width="672" height="480" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The differences reflect distinct documentary styles: some photographers favored intimate working shots that captured subjects at medium distance, while others preferred wider environmental compositions or tighter facial portraits. Pose estimation enables these nuanced analyses of compositional choices that would be difficult to quantify manually.</p>
<p>This approach could be extended in many directions: analyzing face orientation to study where subjects are looking, comparing the poses of multiple people to identify group dynamics, or tracking limb positions to characterize types of physical activity depicted in photographs.</p>
</section>
<section id="training-yolo" class="level2" data-number="20.10">
<h2 data-number="20.10" class="anchored" data-anchor-id="training-yolo"><span class="header-section-number">20.10</span> Training YOLO</h2>
<p>The pre-trained YOLO models we have used so far recognize objects from the COCO dataset: people, cars, dogs, and other common categories. But what if we want to detect objects specific to our domain? Training a custom YOLO model requires labeled data where humans have drawn bounding boxes around objects of interest and assigned class labels to each box.</p>
<p>Our bird bounding box dataset contains exactly this information. Each row specifies an image filepath, a species label, and four coordinates defining the corners of a bounding box around the bird.</p>
<div id="27d2747e" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>birds_bbox</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (1_000, 7)</small>
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">label</th>
<th data-quarto-table-cell-role="th">filepath</th>
<th data-quarto-table-cell-role="th">bbox_x0</th>
<th data-quarto-table-cell-role="th">bbox_y0</th>
<th data-quarto-table-cell-role="th">bbox_x1</th>
<th data-quarto-table-cell-role="th">bbox_y1</th>
<th data-quarto-table-cell-role="th">index</th>
</tr>
<tr class="even">
<th>str</th>
<th>str</th>
<th>f64</th>
<th>f64</th>
<th>f64</th>
<th>f64</th>
<th>str</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"Gray_Catbird"</td>
<td>"media/birds_1000/00000.png"</td>
<td>15.0</td>
<td>44.0</td>
<td>480.0</td>
<td>331.0</td>
<td>"train"</td>
</tr>
<tr class="even">
<td>"Sayornis"</td>
<td>"media/birds_1000/00001.png"</td>
<td>131.0</td>
<td>85.0</td>
<td>488.0</td>
<td>326.0</td>
<td>"train"</td>
</tr>
<tr class="odd">
<td>"Tennessee_Warbler"</td>
<td>"media/birds_1000/00002.png"</td>
<td>40.0</td>
<td>5.0</td>
<td>345.0</td>
<td>239.0</td>
<td>"test"</td>
</tr>
<tr class="even">
<td>"White_throated_Sparrow"</td>
<td>"media/birds_1000/00003.png"</td>
<td>99.0</td>
<td>42.0</td>
<td>448.0</td>
<td>344.0</td>
<td>"train"</td>
</tr>
<tr class="odd">
<td>"Ring_billed_Gull"</td>
<td>"media/birds_1000/00004.png"</td>
<td>104.0</td>
<td>32.0</td>
<td>451.0</td>
<td>284.0</td>
<td>"train"</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>"White_breasted_Kingfisher"</td>
<td>"media/birds_1000/00995.png"</td>
<td>17.0</td>
<td>105.0</td>
<td>419.0</td>
<td>336.0</td>
<td>"test"</td>
</tr>
<tr class="even">
<td>"Blue_Grosbeak"</td>
<td>"media/birds_1000/00996.png"</td>
<td>96.0</td>
<td>102.0</td>
<td>361.0</td>
<td>338.0</td>
<td>"test"</td>
</tr>
<tr class="odd">
<td>"Yellow_headed_Blackbird"</td>
<td>"media/birds_1000/00997.png"</td>
<td>53.0</td>
<td>42.0</td>
<td>424.0</td>
<td>208.0</td>
<td>"train"</td>
</tr>
<tr class="even">
<td>"Tree_Sparrow"</td>
<td>"media/birds_1000/00998.png"</td>
<td>47.0</td>
<td>11.0</td>
<td>450.0</td>
<td>427.0</td>
<td>"train"</td>
</tr>
<tr class="odd">
<td>"Cliff_Swallow"</td>
<td>"media/birds_1000/00999.png"</td>
<td>158.0</td>
<td>50.0</td>
<td>352.0</td>
<td>316.0</td>
<td>"train"</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>The <code>bbox_x0</code> and <code>bbox_y0</code> columns give the coordinates of the upper-left corner, while <code>bbox_x1</code> and <code>bbox_y1</code> give the lower-right corner. The <code>index</code> column indicates whether each image belongs to the training set or the test set, a split we made beforehand to enable honest evaluation of model performance.</p>
<p>YOLO expects training data in a specific directory structure with images and label files organized into train and validation folders. Our helper function <code>DSImage.build_yolo_data</code> converts our tabular format into the required layout, creating a YAML configuration file that tells YOLO where to find the data and what classes to recognize.</p>
<div id="b36c0c31" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>DSImage.prepare_yolo_dataset(</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    birds_bbox, root<span class="op">=</span><span class="st">"media/yolo_birds"</span>, yaml_name<span class="op">=</span><span class="st">"birds.yaml"</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With the data prepared, training proceeds by loading a pre-trained model and calling its <code>train</code> method. We start from <code>yolo11n.pt</code>, a model already trained on COCO, and fine-tune it on our bird data. This transfer learning approach leverages features the model has already learned from millions of general images, adapting them to our specific task.</p>
<div id="b0d72dc8" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">"yolo11n.pt"</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>model.train(data<span class="op">=</span><span class="st">"media/yolo_birds/birds.yaml"</span>, epochs<span class="op">=</span><span class="dv">50</span>, imgsz<span class="op">=</span><span class="dv">640</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>model.save(<span class="st">"yolo_birds_final.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>epochs</code> parameter controls how many complete passes through the training data the model makes, with each epoch updating the model weights based on the loss function described earlier. The <code>imgsz</code> parameter specifies the image size used during training; images are resized to 640×640 pixels regardless of their original dimensions.</p>
<p>After training completes, we evaluate performance on the held-out test set using the <code>val</code> method. This computes standard object detection metrics that quantify how well the model’s predictions match ground-truth annotations.</p>
<div id="0e0b3e3e" class="cell" data-message="false" data-execution_count="37">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>metrics <span class="op">=</span> model.val(data<span class="op">=</span><span class="st">"media/yolo_birds/birds.yaml"</span>, imgsz<span class="op">=</span><span class="dv">640</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The primary metrics for object detection are variants of mean Average Precision (mAP), which measures how well the model balances finding all relevant objects (recall) with avoiding false detections (precision). To understand mAP, we first need to define when a predicted bounding box counts as a correct detection.</p>
<p>A prediction is considered a true positive if its Intersection over Union (IoU) with a ground-truth box exceeds some threshold. Recall that IoU measures the overlap between two boxes:</p>
<p><span class="math display">\[
\text{IoU} = \frac{\text{Area of Intersection}}{\text{Area of Union}}
\]</span></p>
<p>An IoU of 1.0 means perfect overlap, while 0 means no overlap at all. The choice of IoU threshold determines how strict we are about localization accuracy. For each class, we can compute precision and recall at various confidence thresholds, tracing out a precision-recall curve. Average Precision (AP) summarizes this curve as the area underneath it:</p>
<p><span class="math display">\[
\text{AP} = \int_0^1 p(r) \, dr
\]</span></p>
<p>where <span class="math inline">\(p(r)\)</span> is precision at recall level <span class="math inline">\(r\)</span>. In practice, this integral is approximated by interpolating the precision-recall curve at discrete points. The mAP50 metric uses an IoU threshold of 0.50, meaning a prediction counts as correct if it overlaps with the ground truth by at least 50%. This is a relatively lenient standard that rewards finding objects even if the bounding box is not perfectly aligned.</p>
<div id="eec8011c" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>metrics.box.map50</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The mAP50-95 averages performance across multiple IoU thresholds from 0.50 to 0.95 in steps of 0.05:</p>
<p><span class="math display">\[
\text{mAP50-95} = \frac{1}{10} \sum_{t \in \{0.50, 0.55, \ldots, 0.95\}} \text{mAP}_t
\]</span></p>
<p>This stricter metric rewards precise localization. A model might achieve high mAP50 by finding objects with roughly correct boxes, but achieving high mAP50-95 requires tight alignment between predictions and ground truth.</p>
<div id="db1b3bc3" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>metrics.box.<span class="bu">map</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The gap between these two metrics reveals how well the model localizes objects. A large gap suggests the model finds objects but draws imprecise boxes; a small gap indicates accurate localization.</p>
<p>Finally, let’s see the trained model in action on a test image it has never seen during training.</p>
<div id="98bd85f0" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> birds_bbox.<span class="bu">filter</span>(c.index <span class="op">==</span> <span class="st">"test"</span>).select(c.filepath.last()).item()</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> model.predict(path)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>rgb <span class="op">=</span> cv2.cvtColor(pred[<span class="dv">0</span>].plot(), cv2.COLOR_BGR2RGB)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>pil_img <span class="op">=</span> Image.fromarray(rgb)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>pil_img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
image 1/1 /Users/admin/gh/fds-py/media/birds_1000/00996.png: 512x640 1 bird, 29.6ms
Speed: 1.1ms preprocess, 29.6ms inference, 0.4ms postprocess per image at shape (1, 3, 512, 640)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="35">
<div>
<figure class="figure">
<p><a href="20_image_data_files/figure-html/cell-41-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img src="20_image_data_files/figure-html/cell-41-output-2.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>And, at least in this one example, it does a very good job of boxing in the bird within the image.</p>
</section>
<section id="vision-language-models" class="level2" data-number="20.11">
<h2 data-number="20.11" class="anchored" data-anchor-id="vision-language-models"><span class="header-section-number">20.11</span> Vision Language Models</h2>
<p>The models we have explored so far each produce structured outputs: bounding boxes, segmentation masks, or keypoint coordinates. These representations are useful for quantitative analysis but do not capture the full richness of what an image depicts. A photograph of workers in a factory contains not just “3 persons detected” but a story about labor, industry, and daily life. Extracting such narrative content has traditionally required human interpretation.</p>
<p>Vision Language Models (VLMs) bridge this gap by combining visual understanding with natural language generation. These models can look at an image and produce free-form text descriptions, answer questions about visual content, or engage in dialogue about what they see. They represent a convergence of computer vision and large language models, trained on massive datasets of images paired with textual descriptions.</p>
<p>The architecture of a VLM typically consists of three components: a vision encoder that processes the image into a sequence of visual tokens, a projection layer that maps these tokens into the same embedding space used by the language model, and a language model that generates text conditioned on both the visual tokens and any text prompt. During training, the model learns to associate visual patterns with their linguistic descriptions, enabling it to describe novel images it has never seen before.</p>
<p>VLMs open new possibilities for image analysis. Rather than counting objects or measuring pixel proportions, we can ask open-ended questions: “What activity is taking place in this photograph?” or “Describe the mood conveyed by this image.” The responses, while subjective and sometimes imperfect, capture aspects of visual content that structured outputs cannot represent.</p>
<p>Let’s use the OpenAI API to get a description of one of our FSA photographs. We will send the image to a vision-capable model and ask it to describe what it sees.</p>
<div id="b42c4bf7" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> base64</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> OpenAI</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> OpenAI()</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">"media/fsac/1a35210v.jpg"</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(image_path, <span class="st">"rb"</span>) <span class="im">as</span> f:</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>    image_data <span class="op">=</span> base64.standard_b64encode(f.read()).decode(<span class="st">"utf-8"</span>)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> (<span class="st">"Provide a detailed plain-text description of the "</span></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>       <span class="st">"objects, activities, people, background and/or composition "</span></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>       <span class="st">"of this photograph"</span>)</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> client.chat.completions.create(</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"gpt-5-mini-2025-08-07"</span>,</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>    messages<span class="op">=</span>[</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>            <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>            <span class="st">"content"</span>: [</span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>                {</span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"type"</span>: <span class="st">"text"</span>,</span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"text"</span>: text</span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a>                },</span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a>                {</span>
<span id="cb49-27"><a href="#cb49-27" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"type"</span>: <span class="st">"image_url"</span>,</span>
<span id="cb49-28"><a href="#cb49-28" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"image_url"</span>: {</span>
<span id="cb49-29"><a href="#cb49-29" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"url"</span>: <span class="ss">f"data:image/jpeg;base64,</span><span class="sc">{</span>image_data<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb49-30"><a href="#cb49-30" aria-hidden="true" tabindex="-1"></a>                    }</span>
<span id="cb49-31"><a href="#cb49-31" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb49-32"><a href="#cb49-32" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb49-33"><a href="#cb49-33" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb49-34"><a href="#cb49-34" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb49-35"><a href="#cb49-35" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-36"><a href="#cb49-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-37"><a href="#cb49-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response.choices[<span class="dv">0</span>].message.content)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fb8f3c71" class="cell" data-execution_count="42">
<div class="cell-output cell-output-stdout">
<pre><code>
The photograph shows a close, waist-up group portrait of three young men in
military work clothes standing in front of an armored vehicle. Each man wears
a one-piece coverall or boiler suit, belted at the waist, and a padded tank
helmet with ear flaps or built-in headphones. Their coveralls are dusty and
stained with grease and dirt; their faces and hands also show grime,
suggesting recent hard work or field activity.  All three men are smiling
broadly and appear relaxed and friendly with one another. The central figure
stands slightly forward with his arms linked to the two men beside him; the
man on the left has one hand resting on his hip, and the man on the right has
a hand on his belt. Their posture and expressions convey camaraderie and a
moment of shared good spirits.  Behind them, a large tank or armored vehicle
dominates the background. The vehicle’s turret and a long gun barrel run
diagonally across the upper right of the image. Parts of the tank’s hull and
tracks are visible, showing a dusty, worn surface. In the upper left
background, a pair of legs and a boot belonging to another crew member can
be seen standing on the vehicle, only partially included in the frame.
The setting is outdoors under a clear blue sky; a low horizon of open
countryside or field is visible in the far background. The photograph is
in color with warm, natural daylight highlighting the men’s faces and the
textured surfaces of their clothing and the vehicle. The composition centers
the trio, creating a tight, informal group portrait against the larger
mechanical backdrop, emphasizing both the human element and the military
equipment. The image has the look of a candid, in-the-field moment rather
than a formal posed studio shot.
</code></pre>
</div>
</div>
<p>The model returns a natural language description that captures elements no structured detector could identify: the apparent era suggested by clothing and photographic style, the social context implied by the scene, and interpretive observations about mood or activity. This kind of output is inherently more subjective than counting bounding boxes, but it provides a different and complementary form of understanding.</p>
<p>VLMs can also be used programmatically to generate structured data from images. By carefully crafting prompts, you can ask the model to output JSON with specific fields, effectively creating a flexible object detector that can identify whatever categories you specify without retraining. This approach trades some accuracy for remarkable flexibility: the same model can describe fashion items, identify architectural styles, or catalog the contents of historical photographs.</p>
<p>The combination of structured computer vision models and flexible VLMs provides a powerful toolkit. Use object detection when you need precise counts and locations. Use segmentation when pixel-level boundaries matter. Use pose estimation for body position analysis. And use VLMs when you need rich, contextual descriptions or want to extract information that no pre-trained detector was designed to find.</p>
</section>
<section id="conclusions" class="level2" data-number="20.12">
<h2 data-number="20.12" class="anchored" data-anchor-id="conclusions"><span class="header-section-number">20.12</span> Conclusions</h2>
<p>This chapter has explored a range of techniques for extracting information from images, progressing from simple pixel statistics to sophisticated deep learning models and vision language models. Each approach offers different tradeoffs between interpretability, computational cost, and semantic richness.</p>
<p>Basic pixel-level features like brightness and color distributions are fast to compute and easy to understand. They capture global properties of images and can reveal interesting patterns across collections. However, these features cannot distinguish between a yellow bird and a yellow background, or between a dark subject and a dark photograph.</p>
<p>Deep learning models for object detection, segmentation, and pose estimation offer dramatically more sophisticated understanding. These models identify meaningful objects, delineate their boundaries at pixel precision, and localize body parts in complex poses. They enable analyses that would be impossible with simple features: counting people in crowds, measuring how much of an image depicts human figures, or characterizing body positions.</p>
<p>Vision language models add yet another dimension by producing natural language descriptions of visual content. They capture contextual and interpretive aspects of images that structured outputs cannot represent, though their outputs require different analytical approaches than numerical features.</p>
<p>Beyond the specific models explored here, remember that images can be converted to embeddings using transfer learning approaches (<a href="15_transferlearn.html" class="quarto-xref"><span>Chapter 15</span></a>), enabling classification, clustering, and visualization with standard machine learning tools. This embedding-based approach is particularly valuable when your analytical categories don’t match the labels in pre-trained models.</p>
<p>The models we explored here—YOLO variants trained on the COCO dataset and VLMs trained on web-scale image-text pairs—represent just a sample of available approaches. The field of computer vision continues to advance rapidly, with new architectures and training techniques regularly improving accuracy and enabling new capabilities. The fundamental pattern, however, remains consistent: we extract information from images by combining low-level pixel data with learned representations that capture semantic meaning.</p>
<p>For data science applications, image analysis opens vast possibilities. Archives of historical photographs can be systematically analyzed to understand social patterns. Medical images can be screened for abnormalities. Satellite imagery can track environmental changes. Social media photographs can reveal trends in consumer behavior. Wherever images contain information, the techniques in this chapter provide tools to extract it.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/taylor-arnold\.github\.io\/fds-py");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./19_textual_data.html" class="pagination-link" aria-label="Textual Data">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Textual Data</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./21_notes.html" class="pagination-link" aria-label="Notes">
        <span class="nav-page-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Notes</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Taylor Arnold</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>A <a href="https://distantviewing.org/">Distant Viewing Lab</a> project</p>
</div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>