[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Data Science",
    "section": "",
    "text": "Welcome\nThis book is currently under development. It will be extended and updated throughout the 2025-2026 academic year. It is being used for our year-long undergraduate sequence of data science courses (DSST289 and DSST389).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Introduction\nIn this book, we focus on tools and techniques for exploratory data analysis, also known as EDA. Initially described in John Tukey’s classic text of the same name, EDA is a general approach to examining data through visualizations and broad summary statistics [1] [2]. It prioritizes studying data directly to generate hypotheses and ascertain general trends prior to and often in lieu of formal statistical modeling. The growth in both data volume and complexity has further increased the need for careful application of these exploratory techniques. In the intervening years, EDA techniques have become widely used within statistics, computer science, and many other data-driven fields and professions.\nThe histories of data programming and EDA are deeply entwined. Concurrent with Tukey’s development of exploratory data analysis (EDA), Rick Becker, John Chambers, and Allan Wilks of Bell Labs began developing software designed specifically for statistical computing. By 1980, the ‘S’ language was released for general distribution outside Bell Labs. It was followed by a popular series of books and updates, including ‘New S’ and ‘S-Plus’ [3] [4] [5] [6]. In the early 1990s, Ross Ihaka and Robert Gentleman produced a fully open-source implementation of S called ‘R’. The name ‘R’ was chosen both as a play on the previous letter in the alphabet and as a reference to the authors’ shared initial. Their implementation has become the de facto standard tool in statistics. More recently, many of R’s best ideas have been extended to other general-purpose programming languages such as Python and JavaScript.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#introduction",
    "href": "01_intro.html#introduction",
    "title": "1  Introduction",
    "section": "",
    "text": "Figure 1.1: Diagram of the process of exploratory data analysis",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#data-science-ecosystem",
    "href": "01_intro.html#data-science-ecosystem",
    "title": "1  Introduction",
    "section": "1.2 Data Science Ecosystem",
    "text": "1.2 Data Science Ecosystem\nThe ideas and methods of exploratory data analysis (EDA) have been successfully adopted and extended in Python through a rich ecosystem of data science libraries. Python, originally created by Guido van Rossum in 1991, has evolved into one of the most popular programming languages for data analysis, machine learning, and scientific computing. Pandas, developed by Wes McKinney beginning in 2008, brought R-like data structures and manipulation capabilities to Python [7]. Matplotlib and, later, Seaborn provided comprehensive plotting capabilities, while plotnine implemented the grammar of graphics approach pioneered by ggplot2 in R. More recently, Polars has emerged as a high-performance alternative to pandas for large-scale data manipulation.\nThis Python ecosystem preserves the philosophy behind EDA: prioritizing interactive exploration, readable code, and the ability to move seamlessly between data manipulation, visualization, and analysis. We see this book as contributing to efforts to bring new communities into data analysis and to help shape that analysis by offering the humanities and humanistic social sciences powerful tools for data-driven inquiry. A visual summary of the steps of EDA is shown above in Fig. 1.1. We will see that the core chapters in this text map onto the steps outlined in the diagram.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#setup",
    "href": "01_intro.html#setup",
    "title": "1  Introduction",
    "section": "1.3 Setup",
    "text": "1.3 Setup\nWhile it is possible to read this book as a conceptual text, we expect that the majority of readers will eventually want to follow along with the code and examples given throughout the text. The first step is to obtain a working installation of Python with the necessary data science modules. For readers new to programming, we recommend getting started with Google Colab, which provides free access to a working Python session with no setup required. For users comfortable with the command line, the uv library is extremely powerful and reduces many of the pain points associated with other methods of setting up Python.\nIn addition to the Python software, following the examples in this text requires access to the datasets we use. Care has been taken to ensure that these are all in the public domain, making them easy to redistribute to readers. The materials and download instructions are available through the associated notebooks at the start of each chapter.\nLearning to program is hard, and questions and issues will invariably arise in the process (even the most experienced users require help surprisingly frequently). As a first source of help, searching a question or error message in a search engine or a chat-based generative AI system will often pull up a solution directly related to your question. If you cannot find an immediate answer, the next best step is to find local, in-person help. While we’ve done our best in this static text to explain the concepts for working with Python, nothing beats talking to a real person.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#working-with-notebooks",
    "href": "01_intro.html#working-with-notebooks",
    "title": "1  Introduction",
    "section": "1.4 Working with Notebooks",
    "text": "1.4 Working with Notebooks\nThe supplemental materials include all the data and code needed to replicate the analyses and visualizations in the book. We include the exact code that is printed in the book. We have used Quarto notebooks (with an .qmd extension) to store this code, with a file corresponding to each chapter. Quarto notebooks are an excellent choice for data analysis because they allow us to mix code, visualizations, and explanations within the same file. In fact, the entire data science workflow—from initial exploration through final presentation—can be contained within a single notebook. Furthermore, Quarto notebooks can be executed from Google Colab, allowing you to run the notebooks via a one-click link.\nThe Quarto environment provides a convenient way to view and edit notebooks. A notebook interface is organized into cells that contain either code or formatted text (markdown). Running a code cell executes the Python code and displays the output directly below the cell, making the environment ideal for exploratory data analysis because we can experiment and immediately see the results. Code cells typically have a gray background and can be executed by clicking the run button or pressing Shift+Enter. When we read or create a dataset, we can inspect it by typing the variable name in a cell.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#running-python-code",
    "href": "01_intro.html#running-python-code",
    "title": "1  Introduction",
    "section": "1.5 Running Python Code",
    "text": "1.5 Running Python Code\nNow let’s look at some examples of how to run Python code. In this book, we will show snippets of Python code and their output. Note that each snippet should be thought of as occurring in a code cell in a Quarto notebook. In one of its simplest forms, Python acts as a calculator. We can add 1 and 1 by typing 1 + 1 into a code cell; running the cell will display the output (2) below. In this book, we will present code and its output in a black box, with the Python code shown inside it and any output shown beneath. An example is given below.\n\n1 + 1\n\n2\n\n\nIn addition to returning a value, running Python code can also store values by creating new variables. Variables in Python are used to store anything—such as numbers, datasets, functions, or models—that we want to use again later. Each variable has a name we can use to access it in later code. To create a variable, we use the = (equals) symbol, with the name on the left and the expression that produces the value on the right. For example, we can create a new variable called mynum with the value 8 by running the following code.\n\nmynum = 3 + 5\n\nNotice that this code did not print any results because the result was saved to a new variable. We can now use our variable mynum in the same way we would use the number 8. For example, adding it to 1 yields the number nine:\n\nmynum + 1\n\n9\n\n\nVariable names must start with a letter or an underscore, but can contain numbers after the first character. We recommend using only lowercase letters and underscores. This makes it easier to read the code later without having to remember whether and where you used capital letters.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#functions-in-python",
    "href": "01_intro.html#functions-in-python",
    "title": "1  Introduction",
    "section": "1.6 Functions in Python",
    "text": "1.6 Functions in Python\nA function in Python takes a set of input values and returns an output value. Typically, a function has a format similar to the code below:\n\nfunction_name(input1, input2)\n\nWhere input1 and input2 are the values we pass to the first and second arguments of the function. The number of arguments is not always two, of course. There may be any number of arguments, including zero. There may also be additional optional arguments with default values that can be modified. Let us look at an example function: round. This function returns a rounded version of a number. If you give the function a single number, it returns the nearest integer. For example, here is the rounded value of π:\n\nround(3.14159)\n\n3\n\n\nThe function has an additional optional parameter that specifies the number of significant digits. For example, this will round π to two significant digits:\n\nround(3.14159, 2)\n\n3.14\n\n\nAn alternative way to call the same function is to use named arguments, where the two input values are specified by name rather than by position:\n\nround(number=3.14159, ndigits=2)\n\n3.14\n\n\nHow do we know the inputs to each function and what they do? In this text, we will explain the names and usage of the required inputs for new functions as they are introduced. To learn more about all of the possible inputs to a function, we can consult the function’s documentation. Python has excellent built-in documentation that can be accessed using the help() function. Most Python modules also have extensive online documentation. For example, the Polars library has comprehensive documentation at https://pola.rs/. We will learn how to use numerous functions in the coming chapters, each of which will help us explore and understand data.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#data-science-modules",
    "href": "01_intro.html#data-science-modules",
    "title": "1  Introduction",
    "section": "1.7 Data Science Modules",
    "text": "1.7 Data Science Modules\nA major selling point of Python is its extensive collection of user-contributed modules, available through the Python Package Index (PyPI). Most of the core modules we will need are already installed in Google Colab, and all we need to do is import them. When extra modules are needed, we will include lines of code such as the following to install them from within the Colab environment.\n\n! pip install requests --quiet\n\nOnce all modules are available, we’ll run code to load the modules we need into Python. Below is a common example of what we’ll have at the start of our scripts.\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\n\nThe lines that start with import make a module available for use in Python. Any function available within the module can be accessed by writing the module name followed by a dot and the function name. The as clauses create shortcuts that make the code easier to type. For example, after running the code above, we can call the read_csv function from Polars by typing pl.read_csv. Code that starts with from imports specific functions from the given libraries, making them available without needing to prefix them with the library name. The asterisk statements imports all the functions from the corresponding packages so you don’t have to import each one individually. The second-to-last line imports the col function from Polars and aliases it as c. We will use this function extensively throughout the book; the shorthand will greatly reduce clutter in our code. The final line loads all of the custom functions in a local file called funs.py. These are wrapper functions we have created to help simplify the code in this text. Each wrapper will be explained as they arise and can be directly inspected by opening the Python script directly.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#loading-data-in-python",
    "href": "01_intro.html#loading-data-in-python",
    "title": "1  Introduction",
    "section": "1.8 Loading Data in Python",
    "text": "1.8 Loading Data in Python\nIn this book, we will be primarily working with data stored in a tabular format. Fig. 1.2 shows an example of a tabular dataset consisting of information about metropolitan regions in the United States. The figure shows structures organized by rows and columns. Each row of the dataset represents a particular metropolitan region; we call each row an observation. The columns represent the measurements we record for each observation; these measurements are called variables.\n\n\n\n\n\n\nFigure 1.2: An example of a tabular dataset.\n\n\n\nIn our example dataset, we have five variables that record the region name, the quadrant of the country where the region is located, the region’s population (in millions), the population density (in tens of thousands of people per square kilometer), and the median age of the region’s residents. More details are given in the following section.\nA common format for storing tabular datasets are in plaintext comma-separated values (CSV) files. Almost all of the datasets in this book will use this format for storing the data that we will work with. To read a dataset into Python, we use the function we use the function pl.read_csv() from the Polars library. We call pl.read_csv() with the path to the file relative to the script’s location. Below is an example of how to read the dataset contained in the file “data/countries.csv” and save it as an object called country. The resulting dataset is stored as a Python object called a DataFrame.\n\ncountry = pl.read_csv(\"data/countries.csv\")\ncountry\n\n\nshape: (135, 15)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"SEN\"\n\"Senegal\"\n\"Africa\"\n\"Western Africa\"\n18.932\n70.43\n14.366667\n-14.283333\n0.53\n4871\n38.1\n50.93\n66.0\n54.93987\n\"pbp|fra|wol\"\n\n\n\"VEN\"\n\"Venezuela, Bolivarian Republic…\n\"Americas\"\n\"South America\"\n28.517\n76.18\n8.0\n-67.0\n0.709\n8899\n44.8\n57.65\n96.8\n95.66913\n\"spa|vsl\"\n\n\n\"FIN\"\n\"Finland\"\n\"Europe\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n\"fin|swe\"\n\n\n\"USA\"\n\"United States of America\"\n\"Americas\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n\"eng\"\n\n\n\"LKA\"\n\"Sri Lanka\"\n\"Asia\"\n\"Southern Asia\"\n23.229\n78.51\n7.0\n81.0\n0.776\n14380\n39.3\n36.02\n83.1\n90.77437\n\"sin|sin|tam|tam\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"ALB\"\n\"Albania\"\n\"Europe\"\n\"Southern Europe\"\n2.772\n79.67\n41.0\n20.0\n0.81\n20362\n30.8\n54.45\n91.9\n98.5473\n\"sqi\"\n\n\n\"MYS\"\n\"Malaysia\"\n\"Asia\"\n\"South-eastern Asia\"\n35.978\n76.03\n3.7805111\n102.314362\n0.819\n35990\n46.2\n58.68\n118.2\n95.69194\n\"msa\"\n\n\n\"SLV\"\n\"El Salvador\"\n\"Americas\"\n\"Central America\"\n6.366\n76.98\n13.668889\n-88.866111\n0.678\n12221\n38.3\n64.82\n126.9\n86.19786\n\"spa\"\n\n\n\"CYP\"\n\"Cyprus\"\n\"Asia\"\n\"Western Asia\"\n1.371\n81.77\n35.0\n33.0\n0.913\n55720\n31.2\n60.71\n123.1\n99.41781\n\"ell|tur\"\n\n\n\"PAK\"\n\"Pakistan\"\n\"Asia\"\n\"Southern Asia\"\n255.22\n66.71\n30.0\n71.0\n0.544\n5717\n29.6\n45.49\n49.8\n61.92651\n\"eng|urd\"\n\n\n\n\n\n\nNotice that the display shows a total of 135 rows and 15 columns. Or, with our terms defined above, there are 135 observations and 15 variables. Only the first five and last five observations are shown in the output, along with information about the shape of the DataFrame.\nThe data types shown by pandas tell us the types of data stored in each column. Here we see the three most common types: str (strings), i64 (integers; whole numbers stored in 64 bits), and f64 (floats; numbers with decimal points, also stored in 64 bits). We will refer to any variable of either integer or float type as numeric data. Knowing the data types of each column is important because, as we will see throughout the book, they affect the kinds of visualizations and analyses that can be applied. The data types in the DataFrame are automatically determined by the pl.read_csv() function. Optional arguments, such as dtype, can be used to specify alternatives, or we can modify data types after the DataFrame is created using techniques shown in the following chapters.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#datasets",
    "href": "01_intro.html#datasets",
    "title": "1  Introduction",
    "section": "1.9 Datasets",
    "text": "1.9 Datasets\nThroughout this book, we will use various datasets to illustrate concepts and show how each approach can be applied across different application domains. A complete description of all of the datasets used in the text and exercises can be found in Chapter 22. These include the variable names, types, sources, and some motivation behind each within the text.\nIn the following chapters, we will introduce the core concepts of exploratory data analysis relying heavily on the country dataset defined in which we have various pieces of metadata related to countries across the world. There are several associated datasets that we will also work with to illustrate additional concepts that are not directly applicable to the country-level data.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#formatting-code",
    "href": "01_intro.html#formatting-code",
    "title": "1  Introduction",
    "section": "1.10 Formatting Code",
    "text": "1.10 Formatting Code\nIt is important to format Python code consistently. Even if code runs without errors and produces the desired results, consistent formatting makes it easier to read, debug, and maintain. Throughout this book, we will follow the conventions below, based on PEP 8 (Python’s official style guide). These are the same conventions used by professional data scientists and software developers. Adopt these rules whenever you write code for this course.\n\nUse one space before and after the equals sign = and around binary operators (such as +, -, *, /, %, &lt;, &gt;, ==).\nException: do not use spaces around the equals sign when specifying function arguments or keyword arguments (for example, def f(x=5): or geom_text(size=6)).\nUse one space after a comma, but no space before it.\nUse lowercase letters with underscores for variable and function names (snake_case).\nUse double quotes for strings (for example, \"hello\" rather than 'hello').\nKeep lines reasonably short; if a line becomes too long (typically over 79–88 characters), split it across multiple lines.\nWhen breaking code across multiple lines:\n\nWrap the expression in parentheses\nIndent continuation lines by 4 spaces\nPut each logical step (method call, operator) on its own line\nFor operator chaining, place the operator at the start of the continuation line\nAlign the closing parenthesis with the start of the expression\n\nAvoid trailing whitespace and keep indentation consistent throughout the file.\n\nThe following example applies these formatting rules in practice. Do not worry if you have not yet seen these specific functions or libraries; we will cover them later in the course. For now, focus on the structure and layout of the code.\n\n(\n    country\n    .filter(c.region == \"Europe\")\n    .pipe(ggplot, aes(\"hdi\", \"happy\"))\n    + geom_point()\n    + geom_text(aes(label=\"iso\"), size=6, nudge_y=0.5)\n)\n\nNotice how spacing, indentation, and line breaks work together to make the logic clear and readable. Following these conventions from the start will make your work easier to read, debug, and maintain.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#extensions",
    "href": "01_intro.html#extensions",
    "title": "1  Introduction",
    "section": "1.11 Extensions",
    "text": "1.11 Extensions\nEach chapter in this book contains a short concluding section with extensions to the main material. These include references for further study, additional Python libraries, and other suggested methods that may be of interest for the study of each specific type of humanities data.\nIn this chapter, we mention a few standard Python references that may be useful to consult alongside our text. The classic introduction to the Python language is Learning Python by Mark Lutz [8]. For those specifically interested in data science applications, Python for Data Analysis by Wes McKinney (the creator of pandas) provides comprehensive coverage of the core data science libraries [7].\nFor the specific approach to data analysis we follow in this book, Python Data Science Handbook by Jake VanderPlas is an excellent reference [9]. It covers the full stack of data science tools in Python, from basic data manipulation through machine learning. The book is also freely available online.\nFor those interested in the grammar of graphics approach to visualization that we use throughout this book, The Grammar of Graphics by Leland Wilkinson provides the theoretical foundation [10]. The plotnine module implements these concepts in Python, closely following the ggplot2 implementation in R.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#references",
    "href": "01_intro.html#references",
    "title": "1  Introduction",
    "section": "References",
    "text": "References\n\n\n\n\n[1] Tukey, J W (1977 ). Exploratory Data Analysis. Addison-Wesley\n\n\n[2] Brillinger, D R (2002 ). John w. Tukey: His life and professional contributions. Annals of Statistics. JSTOR. 1535–75\n\n\n[3] Becker, R A and Chambers, J M (1984 ). S: An Interactive Environment for Data Analysis and Graphics. CRC Press\n\n\n[4] Becker, R A and Chambers, J M (1985 ). Extending the S System. Wadsworth Advanced Books; Software\n\n\n[5] Becker, R A, Chambers, J M and Wilks, A R (1988 ). The New S Language: A Programming Environment for Data Analysis and Graphics. Wadsworth Advanced Books; Software\n\n\n[6] Chambers, J M and Hastie, T J (1991 ). Statistical Models in S. CRC Press\n\n\n[7] McKinney, W (2012 ). Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. \" O’Reilly Media, Inc.\"\n\n\n[8] Lutz, M (2013 ). Learning Python: Powerful Object-Oriented Programming. \" O’Reilly Media, Inc.\"\n\n\n[9] VanderPlas, J, Granger, B, Heer, J, Moritz, D, Wongsuphasawat, K, Satyanarayan, A, Lees, E, Timofeev, I, Welsh, B and Sievert, S (2018 ). Altair: Interactive statistical visualizations for python. Journal of open source software. The Open Journal. 3 1057\n\n\n[10] Wilkinson, L (2012 ). The Grammar of Graphics. Springer",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02_modify.html",
    "href": "02_modify.html",
    "title": "2  EDA I: Organizing Data",
    "section": "",
    "text": "2.1 Setup\nWe will begin this and every subsequent chapter by listing the core modules and datasets that must be loaded before getting started. Datasets and sources that require new functions will be loaded within the body of the text.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\ncountry = pl.read_csv(\"data/countries.csv\")",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Organizing Data</span>"
    ]
  },
  {
    "objectID": "02_modify.html#introduction",
    "href": "02_modify.html#introduction",
    "title": "2  EDA I: Organizing Data",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nWe will use a variety of methods to modify our datasets. All data operations shown in this chapter come from the Polars library—the same one we used to read our data. These operations take a DataFrame as input and produce a modified DataFrame as output. This consistency makes it easy to chain operations and build complex data transformations step by step. The operations we learn in this chapter are fundamental building blocks for data analysis; while they may seem simple individually, they become powerful when combined.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Organizing Data</span>"
    ]
  },
  {
    "objectID": "02_modify.html#sorting-rows",
    "href": "02_modify.html#sorting-rows",
    "title": "2  EDA I: Organizing Data",
    "section": "2.3 Sorting Rows",
    "text": "2.3 Sorting Rows\nOrganizing data in a meaningful order is often crucial for understanding patterns and trends. We’ll use the .sort() method to reorder the dataset by arranging rows according to one or more columns. This is particularly useful for identifying extreme values, understanding distributions, or preparing data for specific types of analysis. Let’s look at an example of sorting the countries dataset by population, then break down how it works.\n\n(\n    country\n    .sort(c.pop)\n)\n\n\nshape: (135, 15)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"ISL\"\n\"Iceland\"\n\"Europe\"\n\"Northern Europe\"\n0.398\n84.92\n65.0\n-19.0\n0.972\n67444\n26.1\n75.62\n107.2\n98.78217\n\"isl\"\n\n\n\"MLT\"\n\"Malta\"\n\"Europe\"\n\"Southern Europe\"\n0.545\n83.48\n35.883333\n14.5\n0.924\n63314\n31.0\n62.95\n107.8\n99.96455\n\"eng|mlt\"\n\n\n\"MNE\"\n\"Montenegro\"\n\"Europe\"\n\"Southern Europe\"\n0.633\n76.92\n42.766667\n19.216667\n0.862\n29492\n29.4\n58.13\n185.1\n96.657\n\"cnr|cnr\"\n\n\n\"LUX\"\n\"Luxembourg\"\n\"Europe\"\n\"Western Europe\"\n0.68\n83.82\n49.77\n6.13\n0.922\n131038\n34.9\n70.16\n143.3\n97.60054\n\"fra|deu|ltz\"\n\n\n\"COM\"\n\"Comoros\"\n\"Africa\"\n\"Eastern Africa\"\n0.883\n70.26\n-12.3\n43.7\n0.603\n3718\n45.3\n35.88\n25.2\n35.85662\n\"ara|fra\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"PAK\"\n\"Pakistan\"\n\"Asia\"\n\"Southern Asia\"\n255.22\n66.71\n30.0\n71.0\n0.544\n5717\n29.6\n45.49\n49.8\n61.92651\n\"eng|urd\"\n\n\n\"IDN\"\n\"Indonesia\"\n\"Asia\"\n\"South-eastern Asia\"\n285.721\n72.25\n-2.0\n118.0\n0.728\n15391\n37.9\n56.95\n85.8\n77.51606\n\"ind\"\n\n\n\"USA\"\n\"United States of America\"\n\"Americas\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n\"eng\"\n\n\n\"CHN\"\n\"China\"\n\"Asia\"\n\"Eastern Asia\"\n1416.096\n78.81\n35.844722\n103.451944\n0.797\n25505\n38.2\n61.45\n63.6\n88.06831\n\"cmn\"\n\n\n\"IND\"\n\"India\"\n\"Asia\"\n\"Southern Asia\"\n1463.866\n72.4\n22.8\n83.0\n0.685\n10608\n35.7\n46.76\n60.5\n63.83039\n\"eng|hin\"\n\n\n\n\n\n\nThere are several important things to note in this code. First, the code is wrapped in a pair of parentheses, each on its own line, and all other lines are indented by four spaces. Second, we place the dataset on its own line and then call the sort method on the following line. Third, we use the c. prefix followed by the column name to refer to a column. Finally, the results are printed by default, but the original dataset is not modified. After running this code, the country column still has its original order. All of these features are shared across the entire set of DataFrame methods.\n\n\n\n\n\n\nSelecting columns\n\n\n\n\n\nThe standard way to specify columns in the functions in this chapter is the c. format. You can also specify a column using c(\"&lt;str&gt;\"), where &lt;str&gt; is the column name. This is useful when the column name contains spaces or special characters and cannot be represented using dot notation. Some functions, such as sort, also allow specifying the column name directly in quotes without referencing c. We will avoid this in our code, but you may encounter it in other sources.\n\n\n\nYou can reverse the sorting order of sort by setting its descending parameter to True.\n\n(\n    country\n    .sort(c.pop, descending=True)\n)\n\n\nshape: (135, 15)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"IND\"\n\"India\"\n\"Asia\"\n\"Southern Asia\"\n1463.866\n72.4\n22.8\n83.0\n0.685\n10608\n35.7\n46.76\n60.5\n63.83039\n\"eng|hin\"\n\n\n\"CHN\"\n\"China\"\n\"Asia\"\n\"Eastern Asia\"\n1416.096\n78.81\n35.844722\n103.451944\n0.797\n25505\n38.2\n61.45\n63.6\n88.06831\n\"cmn\"\n\n\n\"USA\"\n\"United States of America\"\n\"Americas\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n\"eng\"\n\n\n\"IDN\"\n\"Indonesia\"\n\"Asia\"\n\"South-eastern Asia\"\n285.721\n72.25\n-2.0\n118.0\n0.728\n15391\n37.9\n56.95\n85.8\n77.51606\n\"ind\"\n\n\n\"PAK\"\n\"Pakistan\"\n\"Asia\"\n\"Southern Asia\"\n255.22\n66.71\n30.0\n71.0\n0.544\n5717\n29.6\n45.49\n49.8\n61.92651\n\"eng|urd\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"COM\"\n\"Comoros\"\n\"Africa\"\n\"Eastern Africa\"\n0.883\n70.26\n-12.3\n43.7\n0.603\n3718\n45.3\n35.88\n25.2\n35.85662\n\"ara|fra\"\n\n\n\"LUX\"\n\"Luxembourg\"\n\"Europe\"\n\"Western Europe\"\n0.68\n83.82\n49.77\n6.13\n0.922\n131038\n34.9\n70.16\n143.3\n97.60054\n\"fra|deu|ltz\"\n\n\n\"MNE\"\n\"Montenegro\"\n\"Europe\"\n\"Southern Europe\"\n0.633\n76.92\n42.766667\n19.216667\n0.862\n29492\n29.4\n58.13\n185.1\n96.657\n\"cnr|cnr\"\n\n\n\"MLT\"\n\"Malta\"\n\"Europe\"\n\"Southern Europe\"\n0.545\n83.48\n35.883333\n14.5\n0.924\n63314\n31.0\n62.95\n107.8\n99.96455\n\"eng|mlt\"\n\n\n\"ISL\"\n\"Iceland\"\n\"Europe\"\n\"Northern Europe\"\n0.398\n84.92\n65.0\n-19.0\n0.972\n67444\n26.1\n75.62\n107.2\n98.78217\n\"isl\"\n\n\n\n\n\n\nNow the countries with the highest population appear first. This descending order is often useful when we want to focus on the most extreme cases or identify outliers in our data. For example, the code below sorts by region (alphabetical order) and then, within each region, by population.\n\n(\n    country\n    .sort(c.region, c.pop)\n)\n\n\nshape: (135, 15)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"COM\"\n\"Comoros\"\n\"Africa\"\n\"Eastern Africa\"\n0.883\n70.26\n-12.3\n43.7\n0.603\n3718\n45.3\n35.88\n25.2\n35.85662\n\"ara|fra\"\n\n\n\"MUS\"\n\"Mauritius\"\n\"Africa\"\n\"Eastern Africa\"\n1.268\n76.35\n-20.2\n57.5\n0.806\n28927\n36.8\n57.59\n92.8\n95.45876\n\"eng|fra\"\n\n\n\"BWA\"\n\"Botswana\"\n\"Africa\"\n\"Southern Africa\"\n2.562\n63.08\n-22.2\n23.7\n0.731\n18189\n53.3\n33.32\n116.2\n77.26922\n\"eng\"\n\n\n\"GAB\"\n\"Gabon\"\n\"Africa\"\n\"Middle Africa\"\n2.593\n68.68\n-0.683331\n11.5\n0.733\n19543\n38.0\n51.04\n93.6\n49.20331\n\"fra\"\n\n\n\"GMB\"\n\"Gambia\"\n\"Africa\"\n\"Western Africa\"\n2.822\n68.16\n13.5\n-15.5\n0.524\n3199\nnull\n46.91\n76.7\n46.90722\n\"eng\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"GBR\"\n\"United Kingdom of Great Britai…\n\"Europe\"\n\"Northern Europe\"\n69.551\n81.93\n54.6\n-2.0\n0.946\n53993\n32.8\n66.58\n121.8\n99.1103\n\"eng\"\n\n\n\"DEU\"\n\"Germany\"\n\"Europe\"\n\"Western Europe\"\n84.075\n82.13\n51.0\n10.0\n0.959\n64355\n31.7\n67.92\n109.4\n99.22504\n\"deu\"\n\n\n\"RUS\"\n\"Russian Federation\"\n\"Europe\"\n\"Eastern Europe\"\n143.997\n73.89\n66.416667\n94.25\n0.832\n40871\n36.0\n58.65\n165.1\n88.24222\n\"rus\"\n\n\n\"NZL\"\n\"New Zealand\"\n\"Oceania\"\n\"Australia and New Zealand\"\n5.252\n82.65\n-41.2\n174.0\n0.938\n49598\nnull\n69.76\n108.3\n100.0\n\"eng|mri|nzs\"\n\n\n\"AUS\"\n\"Australia\"\n\"Oceania\"\n\"Australia and New Zealand\"\n26.974\n83.79\n-25.0\n133.0\n0.958\n61943\n32.4\n70.25\n101.6\n99.99923\n\"asf\"\n\n\n\n\n\n\nFinally, store a copy of the dataset in a named variable using the following code:\n\ncountry_saved = (\n    country\n    .sort(c.region, c.pop)\n)\n\nAfter running the code, country_saved contains the reordered dataset, while country still contains the original data. The same format can be used to save the results from the other methods described in the sections below.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Organizing Data</span>"
    ]
  },
  {
    "objectID": "02_modify.html#selecting-columns-1",
    "href": "02_modify.html#selecting-columns-1",
    "title": "2  EDA I: Organizing Data",
    "section": "2.4 Selecting Columns",
    "text": "2.4 Selecting Columns\nIn some cases—particularly when a dataset grows large—you may want to select or reorder its columns. Use the .select method with the same c. notation to specify the columns to include.\n\n(\n    country\n    .select(c.iso, c.lat, c.lon)\n)\n\n\nshape: (135, 3)\n\n\n\niso\nlat\nlon\n\n\nstr\nf64\nf64\n\n\n\n\n\"SEN\"\n14.366667\n-14.283333\n\n\n\"VEN\"\n8.0\n-67.0\n\n\n\"FIN\"\n65.0\n27.0\n\n\n\"USA\"\n39.828175\n-98.5795\n\n\n\"LKA\"\n7.0\n81.0\n\n\n…\n…\n…\n\n\n\"ALB\"\n41.0\n20.0\n\n\n\"MYS\"\n3.7805111\n102.314362\n\n\n\"SLV\"\n13.668889\n-88.866111\n\n\n\"CYP\"\n35.0\n33.0\n\n\n\"PAK\"\n30.0\n71.0\n\n\n\n\n\n\nSelecting columns is something we will do frequently in this text to clarify which parts of the dataset we want you to examine. It is less important for most data science pipelines, but it is particularly useful when performing more complex transformations, such as those in Chapter 4.\nThere is a similar function, .drop, which lets us select columns to remove from the data. Dropping columns is useful when a dataset has many columns and you want to remove only a few.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Organizing Data</span>"
    ]
  },
  {
    "objectID": "02_modify.html#selecting-rows",
    "href": "02_modify.html#selecting-rows",
    "title": "2  EDA I: Organizing Data",
    "section": "2.5 Selecting Rows",
    "text": "2.5 Selecting Rows\nOften, we want to take a subset of rows from a dataset for a particular analysis or visualization. There are several functions for doing this, depending on the selection criteria. The .head() method, for example, returns a specified number of rows from the top of the dataset. The example below selects the first four rows.\n\n(\n    country\n    .head(n=4)\n)\n\n\nshape: (4, 15)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"SEN\"\n\"Senegal\"\n\"Africa\"\n\"Western Africa\"\n18.932\n70.43\n14.366667\n-14.283333\n0.53\n4871\n38.1\n50.93\n66.0\n54.93987\n\"pbp|fra|wol\"\n\n\n\"VEN\"\n\"Venezuela, Bolivarian Republic…\n\"Americas\"\n\"South America\"\n28.517\n76.18\n8.0\n-67.0\n0.709\n8899\n44.8\n57.65\n96.8\n95.66913\n\"spa|vsl\"\n\n\n\"FIN\"\n\"Finland\"\n\"Europe\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n\"fin|swe\"\n\n\n\"USA\"\n\"United States of America\"\n\"Americas\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n\"eng\"\n\n\n\n\n\n\nThe method .tail(n=&lt;int&gt;) works the same way, except that it selects rows starting from the bottom of the table. The methods .drop_nulls() and .drop_nans() take no arguments and return only rows that contain no missing or invalid data values, respectively.\nThe .sample method randomly selects rows. You can specify the number of rows with n=&lt;int&gt; or a fraction with fraction=&lt;float&gt;. Below is an example that takes a random 20% sample of the dataset.\n\n(\n    country\n    .sample(fraction=0.2)\n)\n\n\nshape: (27, 15)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"IRN\"\n\"Iran, Islamic Republic of\"\n\"Asia\"\n\"Southern Asia\"\n92.418\n79.07\n32.0\n53.0\n0.799\n16540\n40.9\n50.04\n69.8\n89.31666\n\"fas\"\n\n\n\"COG\"\n\"Congo\"\n\"Africa\"\n\"Middle Africa\"\n6.484\n66.38\n-0.75\n15.383331\n0.649\n6377\n48.9\n49.54\n83.3\n20.17288\n\"fra\"\n\n\n\"SGP\"\n\"Singapore\"\n\"Asia\"\n\"South-eastern Asia\"\n5.871\n85.63\n1.3\n103.8\n0.946\n137906\nnull\n66.54\n145.5\n100.0\n\"eng|msa|cmn|tam\"\n\n\n\"POL\"\n\"Poland\"\n\"Europe\"\n\"Eastern Europe\"\n38.141\n79.1\n52.0\n19.0\n0.906\n47892\n30.2\n66.85\n123.2\n98.79661\n\"pol\"\n\n\n\"AUT\"\n\"Austria\"\n\"Europe\"\n\"Western Europe\"\n9.114\n83.17\n48.0\n14.0\n0.93\n64665\n30.2\n66.36\n146.3\n99.97291\n\"asq|deu\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"MUS\"\n\"Mauritius\"\n\"Africa\"\n\"Eastern Africa\"\n1.268\n76.35\n-20.2\n57.5\n0.806\n28927\n36.8\n57.59\n92.8\n95.45876\n\"eng|fra\"\n\n\n\"NIC\"\n\"Nicaragua\"\n\"Americas\"\n\"Central America\"\n7.008\n76.51\n13.0\n-85.0\n0.706\n8023\n46.2\n63.62\n69.1\n72.87482\n\"spa\"\n\n\n\"CHL\"\n\"Chile\"\n\"Americas\"\n\"South America\"\n19.86\n81.39\n-33.0\n-71.0\n0.878\n31425\n44.9\n62.3\n115.5\n100.0\n\"spa\"\n\n\n\"MEX\"\n\"Mexico\"\n\"Americas\"\n\"Central America\"\n131.947\n76.88\n23.0\n-102.0\n0.789\n22375\n45.4\n70.06\n80.4\n89.53789\n\"spa|yua\"\n\n\n\"JPN\"\n\"Japan\"\n\"Asia\"\n\"Eastern Asia\"\n123.103\n85.58\n35.0\n136.0\n0.925\n47815\n32.9\n59.1\n96.2\n99.95242\n\"jpn\"\n\n\n\n\n\n\nWe can select rows that meet a condition defined by the dataset’s variables using the .filter() method. Similar to the sort() method, we use the syntax c. followed by variable names to define relationships between variables. Below is code that selects only the rows where the Human Development Index (HDI) is greater than 0.9.\n\n(\n    country\n    .filter(c.hdi &gt; 0.9)\n)\n\n\nshape: (32, 15)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"FIN\"\n\"Finland\"\n\"Europe\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n\"fin|swe\"\n\n\n\"USA\"\n\"United States of America\"\n\"Americas\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n\"eng\"\n\n\n\"SGP\"\n\"Singapore\"\n\"Asia\"\n\"South-eastern Asia\"\n5.871\n85.63\n1.3\n103.8\n0.946\n137906\nnull\n66.54\n145.5\n100.0\n\"eng|msa|cmn|tam\"\n\n\n\"AUT\"\n\"Austria\"\n\"Europe\"\n\"Western Europe\"\n9.114\n83.17\n48.0\n14.0\n0.93\n64665\n30.2\n66.36\n146.3\n99.97291\n\"asq|deu\"\n\n\n\"DEU\"\n\"Germany\"\n\"Europe\"\n\"Western Europe\"\n84.075\n82.13\n51.0\n10.0\n0.959\n64355\n31.7\n67.92\n109.4\n99.22504\n\"deu\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"NLD\"\n\"Netherlands, Kingdom of the\"\n\"Europe\"\n\"Western Europe\"\n18.347\n82.55\n52.366667\n4.883333\n0.955\n73384\nnull\n72.55\n114.4\n97.71353\n\"nld\"\n\n\n\"IRL\"\n\"Ireland\"\n\"Europe\"\n\"Northern Europe\"\n5.308\n82.94\n53.0\n-8.0\n0.949\n119406\nnull\n68.17\n103.1\n89.3966\n\"eng|gle\"\n\n\n\"LUX\"\n\"Luxembourg\"\n\"Europe\"\n\"Western Europe\"\n0.68\n83.82\n49.77\n6.13\n0.922\n131038\n34.9\n70.16\n143.3\n97.60054\n\"fra|deu|ltz\"\n\n\n\"CAN\"\n\"Canada\"\n\"Americas\"\n\"Northern America\"\n40.127\n83.15\n56.0\n-109.0\n0.939\n58422\n33.3\n68.41\n75.5\n98.92568\n\"eng|fra\"\n\n\n\"CYP\"\n\"Cyprus\"\n\"Asia\"\n\"Western Asia\"\n1.371\n81.77\n35.0\n33.0\n0.913\n55720\n31.2\n60.71\n123.1\n99.41781\n\"ell|tur\"\n\n\n\n\n\n\nWe can also filter categorical variables using equality comparisons. To prevent the page from becoming too cluttered, we will avoid printing this and some other results in future examples.\n\n(\n    country\n    .filter(c.region == \"Europe\")\n)\n\nThis selects all countries in Europe. Note that we use the double-equals operator (==) to test equality; a single equals sign (=) is used only for assignment. The resulting dataset contains only rows where region == \"Europe\".\nOften, we want to apply multiple conditions simultaneously. We can combine conditions using the & (and) and | (or) operators. When using these operators, wrap each condition in parentheses:\n\n(\n    country\n    .filter((c.region == \"Europe\") & (c.hdi &gt; 0.9))\n)\n\nSometimes we want to know whether a value belongs to a larger set. We can do this with the .is_in method, as shown in the following code. Note that the collection of elements passed to .is_in must be enclosed in square brackets. A detailed explanation of the object being created is provided in Chapter 6.\n\n(\n    country\n    .filter(c.region.is_in([\"Europe\", \"Africa\"]))\n)\n\n\nshape: (75, 15)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"SEN\"\n\"Senegal\"\n\"Africa\"\n\"Western Africa\"\n18.932\n70.43\n14.366667\n-14.283333\n0.53\n4871\n38.1\n50.93\n66.0\n54.93987\n\"pbp|fra|wol\"\n\n\n\"FIN\"\n\"Finland\"\n\"Europe\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n\"fin|swe\"\n\n\n\"GAB\"\n\"Gabon\"\n\"Africa\"\n\"Middle Africa\"\n2.593\n68.68\n-0.683331\n11.5\n0.733\n19543\n38.0\n51.04\n93.6\n49.20331\n\"fra\"\n\n\n\"BGR\"\n\"Bulgaria\"\n\"Europe\"\n\"Eastern Europe\"\n6.715\n74.33\n42.75\n25.5\n0.845\n36211\n40.3\n55.9\n137.1\n86.00395\n\"bul\"\n\n\n\"TZA\"\n\"Tanzania, United Republic of\"\n\"Africa\"\n\"Eastern Africa\"\n70.546\n68.59\n-6.306944\n34.853889\n0.555\n3924\n40.5\n40.42\n46.9\n26.78297\n\"eng|swa\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"LUX\"\n\"Luxembourg\"\n\"Europe\"\n\"Western Europe\"\n0.68\n83.82\n49.77\n6.13\n0.922\n131038\n34.9\n70.16\n143.3\n97.60054\n\"fra|deu|ltz\"\n\n\n\"LTU\"\n\"Lithuania\"\n\"Europe\"\n\"Northern Europe\"\n2.83\n77.19\n55.2\n24.0\n0.895\n49761\n35.3\n65.53\n157.9\n92.73568\n\"lit\"\n\n\n\"MNE\"\n\"Montenegro\"\n\"Europe\"\n\"Southern Europe\"\n0.633\n76.92\n42.766667\n19.216667\n0.862\n29492\n29.4\n58.13\n185.1\n96.657\n\"cnr|cnr\"\n\n\n\"GMB\"\n\"Gambia\"\n\"Africa\"\n\"Western Africa\"\n2.822\n68.16\n13.5\n-15.5\n0.524\n3199\nnull\n46.91\n76.7\n46.90722\n\"eng\"\n\n\n\"ALB\"\n\"Albania\"\n\"Europe\"\n\"Southern Europe\"\n2.772\n79.67\n41.0\n20.0\n0.81\n20362\n30.8\n54.45\n91.9\n98.5473\n\"sqi\"\n\n\n\n\n\n\nThese operators, including != (not equal) and &gt;= (greater than or equal to), allow us to select rows based on nearly any relationship in the dataset required for our analysis.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Organizing Data</span>"
    ]
  },
  {
    "objectID": "02_modify.html#grouping-data",
    "href": "02_modify.html#grouping-data",
    "title": "2  EDA I: Organizing Data",
    "section": "2.6 Grouping Data",
    "text": "2.6 Grouping Data\nThere is a special method called .group_by(...) that allows you to group a dataset by one or more variables using a syntax similar to .sort(...). When used in a method chain, the next method is applied separately to each group of rows that share the same value(s) for the grouping variable(s). For example, the following code selects the first country from each region in the dataset.\n\n(\n    country\n    .group_by(c.region)\n    .head(n=1)\n)\n\n\nshape: (5, 15)\n\n\n\nregion\niso\nfull_name\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"Oceania\"\n\"AUS\"\n\"Australia\"\n\"Australia and New Zealand\"\n26.974\n83.79\n-25.0\n133.0\n0.958\n61943\n32.4\n70.25\n101.6\n99.99923\n\"asf\"\n\n\n\"Asia\"\n\"LKA\"\n\"Sri Lanka\"\n\"Southern Asia\"\n23.229\n78.51\n7.0\n81.0\n0.776\n14380\n39.3\n36.02\n83.1\n90.77437\n\"sin|sin|tam|tam\"\n\n\n\"Europe\"\n\"FIN\"\n\"Finland\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n\"fin|swe\"\n\n\n\"Americas\"\n\"VEN\"\n\"Venezuela, Bolivarian Republic…\n\"South America\"\n28.517\n76.18\n8.0\n-67.0\n0.709\n8899\n44.8\n57.65\n96.8\n95.66913\n\"spa|vsl\"\n\n\n\"Africa\"\n\"SEN\"\n\"Senegal\"\n\"Western Africa\"\n18.932\n70.43\n14.366667\n-14.283333\n0.53\n4871\n38.1\n50.93\n66.0\n54.93987\n\"pbp|fra|wol\"\n\n\n\n\n\n\nThis can be useful for various types of analysis. For example, we can determine the largest countries by population in each region using the following code:\n\n(\n    country\n    .sort(c.pop, descending=True)\n    .group_by(c.region)\n    .head(n=1)\n)\n\n\nshape: (5, 15)\n\n\n\nregion\niso\nfull_name\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"Oceania\"\n\"AUS\"\n\"Australia\"\n\"Australia and New Zealand\"\n26.974\n83.79\n-25.0\n133.0\n0.958\n61943\n32.4\n70.25\n101.6\n99.99923\n\"asf\"\n\n\n\"Europe\"\n\"RUS\"\n\"Russian Federation\"\n\"Eastern Europe\"\n143.997\n73.89\n66.416667\n94.25\n0.832\n40871\n36.0\n58.65\n165.1\n88.24222\n\"rus\"\n\n\n\"Americas\"\n\"USA\"\n\"United States of America\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n\"eng\"\n\n\n\"Asia\"\n\"IND\"\n\"India\"\n\"Southern Asia\"\n1463.866\n72.4\n22.8\n83.0\n0.685\n10608\n35.7\n46.76\n60.5\n63.83039\n\"eng|hin\"\n\n\n\"Africa\"\n\"NGA\"\n\"Nigeria\"\n\"Western Africa\"\n237.528\n66.07\n9.0\n8.0\n0.56\n5848\n35.1\n48.69\n52.4\n41.19972\n\"eng\"\n\n\n\n\n\n\nThe following sections describe additional methods that can operate on grouped datasets.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Organizing Data</span>"
    ]
  },
  {
    "objectID": "02_modify.html#adding-columns",
    "href": "02_modify.html#adding-columns",
    "title": "2  EDA I: Organizing Data",
    "section": "2.7 Adding Columns",
    "text": "2.7 Adding Columns\nCreating or modifying columns in a dataset is a fundamental operation in data analysis. Analysts often need to compute new variables from existing data, create indicator (dummy) variables, or transform variables for analysis.\nThe basic pattern for adding a new column is to use the .with_columns() method. In the method call, specify the values you want to create or modify and define them in terms of existing columns. For example, the following multiplies the population column (values in millions) by 1,000 to obtain the population in thousands.\n\n(\n    country\n    .with_columns(population_1k = c.pop * 1000)\n)\n\n\nshape: (135, 16)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\npopulation_1k\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n\"SEN\"\n\"Senegal\"\n\"Africa\"\n\"Western Africa\"\n18.932\n70.43\n14.366667\n-14.283333\n0.53\n4871\n38.1\n50.93\n66.0\n54.93987\n\"pbp|fra|wol\"\n18932.0\n\n\n\"VEN\"\n\"Venezuela, Bolivarian Republic…\n\"Americas\"\n\"South America\"\n28.517\n76.18\n8.0\n-67.0\n0.709\n8899\n44.8\n57.65\n96.8\n95.66913\n\"spa|vsl\"\n28517.0\n\n\n\"FIN\"\n\"Finland\"\n\"Europe\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n\"fin|swe\"\n5623.0\n\n\n\"USA\"\n\"United States of America\"\n\"Americas\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n\"eng\"\n347276.0\n\n\n\"LKA\"\n\"Sri Lanka\"\n\"Asia\"\n\"Southern Asia\"\n23.229\n78.51\n7.0\n81.0\n0.776\n14380\n39.3\n36.02\n83.1\n90.77437\n\"sin|sin|tam|tam\"\n23229.0\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"ALB\"\n\"Albania\"\n\"Europe\"\n\"Southern Europe\"\n2.772\n79.67\n41.0\n20.0\n0.81\n20362\n30.8\n54.45\n91.9\n98.5473\n\"sqi\"\n2772.0\n\n\n\"MYS\"\n\"Malaysia\"\n\"Asia\"\n\"South-eastern Asia\"\n35.978\n76.03\n3.7805111\n102.314362\n0.819\n35990\n46.2\n58.68\n118.2\n95.69194\n\"msa\"\n35978.0\n\n\n\"SLV\"\n\"El Salvador\"\n\"Americas\"\n\"Central America\"\n6.366\n76.98\n13.668889\n-88.866111\n0.678\n12221\n38.3\n64.82\n126.9\n86.19786\n\"spa\"\n6366.0\n\n\n\"CYP\"\n\"Cyprus\"\n\"Asia\"\n\"Western Asia\"\n1.371\n81.77\n35.0\n33.0\n0.913\n55720\n31.2\n60.71\n123.1\n99.41781\n\"ell|tur\"\n1371.0\n\n\n\"PAK\"\n\"Pakistan\"\n\"Asia\"\n\"Southern Asia\"\n255.22\n66.71\n30.0\n71.0\n0.544\n5717\n29.6\n45.49\n49.8\n61.92651\n\"eng|urd\"\n255220.0\n\n\n\n\n\n\nCommon mathematical transformations of variables can be accessed by using a . after the variable name, followed by a transformation function such as sqrt() or log10(). The following example demonstrates the method: take the square root and the sine of population, and save each as a new column.\n\n(\n    country\n    .with_columns(\n      pop_sqrt = c.pop.sqrt(),\n      pop_sin = c.pop.sin()\n    )\n)\n\nNote the format of the code above. When a column definition is long (or when defining multiple columns at once), put each definition on its own line and indent it by four spaces.\n\n\n\n\n\n\nAlias method\n\n\n\n\n\nPolars includes a method called .alias that can be used within .with_columns to create or overwrite a column in a dataset. It was originally the standard technique, but it has largely been replaced by the equals-sign method, which many people, including me, find more intuitive. It’s useful to understand only because you’ll often see it in the Polars documentation.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Organizing Data</span>"
    ]
  },
  {
    "objectID": "02_modify.html#aggregation",
    "href": "02_modify.html#aggregation",
    "title": "2  EDA I: Organizing Data",
    "section": "2.8 Aggregation",
    "text": "2.8 Aggregation\nAlthough examining individual rows can be informative, we often need summary statistics to understand the overall patterns in our data. Polars provides powerful aggregation tools that let us compute statistics across rows within specific groups. The .agg() method (short for “aggregate”) is the primary tool for computing summary statistics. It allows us to specify exactly which statistics to compute and for which columns.\n\n(\n    country\n    .group_by(c.region)\n    .agg(\n      hdi_avg = c.hdi.mean()\n    )\n)\n\n\nshape: (5, 2)\n\n\n\nregion\nhdi_avg\n\n\nstr\nf64\n\n\n\n\n\"Americas\"\n0.783579\n\n\n\"Europe\"\n0.898895\n\n\n\"Oceania\"\n0.948\n\n\n\"Africa\"\n0.583351\n\n\n\"Asia\"\n0.760897\n\n\n\n\n\n\nIn this example, we compute the average HDI across countries within each region. The syntax uses c. followed by the column name, then the aggregation function with parentheses. Note that only the grouping variable(s) and the columns created by the .agg() function appear in the result.\nWe can compute multiple statistics at once by including additional items in our aggregation. It is often useful to include the special function pl.len(), which counts the number of rows in each group.\n\n(\n    country\n    .group_by(c.region)\n    .agg(\n        hdi_avg = c.hdi.mean(),\n        hdi_median = c.hdi.median(),\n        count = pl.len()\n    )\n)\n\n\nshape: (5, 4)\n\n\n\nregion\nhdi_avg\nhdi_median\ncount\n\n\nstr\nf64\nf64\nu32\n\n\n\n\n\"Asia\"\n0.760897\n0.766\n39\n\n\n\"Europe\"\n0.898895\n0.9115\n38\n\n\n\"Americas\"\n0.783579\n0.786\n19\n\n\n\"Africa\"\n0.583351\n0.571\n37\n\n\n\"Oceania\"\n0.948\n0.948\n2\n\n\n\n\n\n\nThis operation computes the average density and the number of regions in each division of the United States. The result is a new DataFrame where each row represents a division and each column shows one of the computed statistics. This grouped analysis is essential for understanding how variables vary across categories in our data.\n\n\n\n\n\n\nAggregating the entire DataFrame\n\n\n\n\n\nIn some cases, we may want to perform one or more aggregation functions on an entire DataFrame. In Polars, it is not possible to apply the .agg method without first grouping the data (this is different from R or Pandas). Instead, use .select and the same syntax as .agg inside the function call. While using the column-selector function to perform aggregation may seem unusual, it makes sense given how Polars optimizes chains of data processing. The specific reason, however, is well beyond the scope of this text.\n\n\n\nPolars provides a wide range of aggregation functions that can be used with the .agg() method. Here are some of the most commonly used functions:\n\n.mean(), .median(), .min(), .max(), .sum(), .quantile(&lt;f64&gt;)\n.first(), .last() - .any(), .all()\n.n_unique(), .count(), .len()\npl.corr(..., ...) (correlation)\npl.concat_str(...)\n\nThese cover most use cases in our analyses.\n\n\n\n\n\n\nCounting observations\n\n\n\n\n\nThere are three ways to count the number of observations in a group. We can use the .len() method applied to any column (it doesn’t matter which) to get the total number of rows, or use .count() on a column to get the number of non-missing values in that column. The special function pl.len() returns the same result as applying .len() to any column, but makes it clear that the result does not depend on which column we choose. For this reason, I prefer pl.len() unless I specifically want the number of non-missing values. There is also .n_unique() to find the number of unique non-missing values in a column.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Organizing Data</span>"
    ]
  },
  {
    "objectID": "02_modify.html#pipes",
    "href": "02_modify.html#pipes",
    "title": "2  EDA I: Organizing Data",
    "section": "2.9 Pipes",
    "text": "2.9 Pipes\nThe syntax of chaining methods is very powerful. It makes our code more readable, avoids errors from naming intermediate values, and allows Polars to create internally optimized functions that chain the sequence of methods in the most efficient way. One possible roadblock to method chaining is that we only have access to methods built into Polars. To overcome this, Polars allows us to call an arbitrary function with the DataFrame as its first argument using the .pipe method. .pipe takes the function to apply as its first argument; additional arguments are passed on to the function.\nAs a simple example, we can apply the built-in Python function type to a DataFrame using the .pipe method to verify the object’s type. The output shows the full Python description of the DataFrame.\n\n(\n    country\n    .pipe(type)\n)\n\npolars.dataframe.frame.DataFrame\n\n\nThis will be particularly helpful in Chapter 3, where we will use the .pipe method to pass a DataFrame to the ggplot function to create a data visualization directly within a chain of methods.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Organizing Data</span>"
    ]
  },
  {
    "objectID": "02_modify.html#further-functions",
    "href": "02_modify.html#further-functions",
    "title": "2  EDA I: Organizing Data",
    "section": "2.10 Further Functions",
    "text": "2.10 Further Functions\nLet’s finish with a few additional functions that are useful inside the with_columns method. We can create new columns that depend on conditions in our existing data using the special function pl.when. The code below shows its use in creating a variable equal to 1 for countries in the Northern Hemisphere and 0 for those in the Southern Hemisphere.\n\n(\n    country\n    .with_columns(\n        is_north = pl.when(c.lat &gt;= 0).then(1).otherwise(0)\n    )\n)\n\nWe can also access the next or previous values of a column using shift(&lt;i64&gt;), where &lt;i64&gt; specifies the number of rows to shift: positive values access rows above, and negative values access rows below. For example, the code below shows the percentage by which each country is larger than the next-smallest country in the dataset.\n\n(\n    country\n    .sort(c.pop, descending=True)\n    .with_columns(\n        perc_larger = c.pop / c.pop.shift(-1) * 100\n    )\n)\n\nThe .with_columns method can compute values within a group, returning aggregations repeated for each row. However, it does not work with the group_by() function. Instead, use the .over() method on a specific column to indicate the grouping. For example, here we compute the percentage that each country occupies within its region.\n\n(\n    country\n    .with_columns(\n        pop_region = c.pop / c.pop.over(\"region\").sum() * 100\n    )\n    .sort(c.pop_region, descending=True)\n)\n\n\nshape: (135, 16)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\npop_region\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n\"IND\"\n\"India\"\n\"Asia\"\n\"Southern Asia\"\n1463.866\n72.4\n22.8\n83.0\n0.685\n10608\n35.7\n46.76\n60.5\n63.83039\n\"eng|hin\"\n18.762505\n\n\n\"CHN\"\n\"China\"\n\"Asia\"\n\"Eastern Asia\"\n1416.096\n78.81\n35.844722\n103.451944\n0.797\n25505\n38.2\n61.45\n63.6\n88.06831\n\"cmn\"\n18.150232\n\n\n\"USA\"\n\"United States of America\"\n\"Americas\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n\"eng\"\n4.451068\n\n\n\"IDN\"\n\"Indonesia\"\n\"Asia\"\n\"South-eastern Asia\"\n285.721\n72.25\n-2.0\n118.0\n0.728\n15391\n37.9\n56.95\n85.8\n77.51606\n\"ind\"\n3.662112\n\n\n\"PAK\"\n\"Pakistan\"\n\"Asia\"\n\"Southern Asia\"\n255.22\n66.71\n30.0\n71.0\n0.544\n5717\n29.6\n45.49\n49.8\n61.92651\n\"eng|urd\"\n3.271178\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"COM\"\n\"Comoros\"\n\"Africa\"\n\"Eastern Africa\"\n0.883\n70.26\n-12.3\n43.7\n0.603\n3718\n45.3\n35.88\n25.2\n35.85662\n\"ara|fra\"\n0.011317\n\n\n\"LUX\"\n\"Luxembourg\"\n\"Europe\"\n\"Western Europe\"\n0.68\n83.82\n49.77\n6.13\n0.922\n131038\n34.9\n70.16\n143.3\n97.60054\n\"fra|deu|ltz\"\n0.008716\n\n\n\"MNE\"\n\"Montenegro\"\n\"Europe\"\n\"Southern Europe\"\n0.633\n76.92\n42.766667\n19.216667\n0.862\n29492\n29.4\n58.13\n185.1\n96.657\n\"cnr|cnr\"\n0.008113\n\n\n\"MLT\"\n\"Malta\"\n\"Europe\"\n\"Southern Europe\"\n0.545\n83.48\n35.883333\n14.5\n0.924\n63314\n31.0\n62.95\n107.8\n99.96455\n\"eng|mlt\"\n0.006985\n\n\n\"ISL\"\n\"Iceland\"\n\"Europe\"\n\"Northern Europe\"\n0.398\n84.92\n65.0\n-19.0\n0.972\n67444\n26.1\n75.62\n107.2\n98.78217\n\"isl\"\n0.005101\n\n\n\n\n\n\nThe .over() and .shift() methods can also be used within .filter().",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Organizing Data</span>"
    ]
  },
  {
    "objectID": "02_modify.html#rs-dplyr-and-python-pandas",
    "href": "02_modify.html#rs-dplyr-and-python-pandas",
    "title": "2  EDA I: Organizing Data",
    "section": "2.11 R’s dplyr and Python Pandas",
    "text": "2.11 R’s dplyr and Python Pandas\nIf you’re familiar with dplyr in R or pandas in Python, the code in this chapter should feel familiar. Specific function names differ, and each library has its own quirks. Ultimately, the important thing is to understand how to compose the methods above to transform data flexibly into the format you need for a particular application.\nIf you’re coming from dplyr, the main differences are prefixing column names with c., using methods instead of pipes (the same idea, different syntax), and using methods in place of aggregate functions. Some methods are renamed—for example, agg replaces summarize and sort replaces arrange—but they otherwise behave similarly.\nTransitioning from Pandas requires a few more changes, but most are designed to make the code simpler and more consistent. We no longer have an index column that needs to be reset after every aggregation. Nor do we have aggregation functions wrapped in complex tuples or the eval and query methods buried inside an extra layer of quotes.\nAs with Pandas and dplyr, Polars also provides functions for pivots and table joins. We will see those in Chapter 4. Although some arguments have different names across the three libraries, the differences in these transformations are relatively minor.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Organizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html",
    "href": "03_graphics.html",
    "title": "3  EDA II: Visualizing Data",
    "section": "",
    "text": "3.1 Setup\nLoad all of the modules and datasets needed for the chapter.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\ncountry = pl.read_csv(\"data/countries.csv\")\ncellphone = pl.read_csv(\"data/countries_cellphone.csv\")",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html#introduction",
    "href": "03_graphics.html#introduction",
    "title": "3  EDA II: Visualizing Data",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nIn this chapter, we will learn and use the plotnine package for building informative graphics [1] [2]. The package makes it easy to build fairly complex graphics guided by a general theory of data visualization. The only downside is that, because it is built around a theoretical model rather than many one-off solutions for different tasks, it has a somewhat steeper initial learning curve. The chapter is designed to get us started using the package to make a variety of data visualizations.\nThe core idea of the grammar of graphics is that visualizations are composed of independent layers. The term “grammar” is used because the theory builds connections between elements of the dataset and elements of a visualization. It assembles complex elements from smaller ones, much like a grammar relates words to generate larger phrases and sentences. To describe a specific layer, we need to specify several components. First, we must specify the dataset from which data will be taken to construct the plot. Next, we specify a set of mappings called aesthetics that describe how elements of the plot relate to columns in the data. For example, we often indicate which column corresponds to the horizontal axis and which corresponds to the vertical axis. We can also describe attributes such as color, shape, and size by associating these quantities with columns in the data. Finally, we need to provide the geometry that will be used in the plot. The geometry describes the kinds of objects associated with each row of the data. A common example is the points geometry, which associates a single point with each observation.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html#points-geometry",
    "href": "03_graphics.html#points-geometry",
    "title": "3  EDA II: Visualizing Data",
    "section": "3.3 Points Geometry",
    "text": "3.3 Points Geometry\nWe can demonstrate the grammar of graphics by using the countries data, where each row corresponds to a particular country. The first plot we will make is a scatter plot that investigates the relationship between the Human Development Index (HDI)—a “composite index of life expectancy, education (mean years of schooling completed and expected years of schooling upon entering the education system), and per capita income indicators, which is used to rank countries into four tiers of human development”—and perceived happiness. In the language of the grammar of graphics, we begin describing this visualization by specifying the Python variable containing the data (country). Next, we associate the horizontal axis (the x aesthetic) with the column in the data named hdi. The vertical axis (the y aesthetic) can similarly be associated with the column named happy. We will make a scatter plot, with each point describing one of our metropolitan regions, which leads us to use a point geometry. Our plot will allow us to understand the relationship between city density and rental prices.\nIn Python, we need to use some special functions to indicate all of this information and to instruct the program to produce a plot. We start by specifying the name of the underlying dataset and applying the special method .pipe with its first argument set to the function ggplot. This indicates that we want to create a data visualization. In the second argument of that function we use the aes (short for aesthetics) function to specify the names of the x and y aesthetics corresponding to columns in the dataset. The plot itself is created by adding—literally, with the plus sign—the geom_point function. This function adds a point geometry (a layer of points) to the plot. Code to do this using the values described above is given below.\n\n(\n    country\n    .filter(c.region == \"Europe\")\n    .pipe(ggplot, aes(\"hdi\", \"happy\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing x and y aesthetics\n\n\n\n\n\nWhen we have a scatter plot showing the relationship between two columns in a dataset, the usual convention is to put the quantity that is more likely to be a cause on the x-axis and the variable more likely to be the effect or outcome on the y-axis. Here, we put the HDI on the x-axis because it seems more likely that investment in human development leads to happier people rather than the other way around. This general rule can be broken when needed to make the plot more readable or to make it easier to compare with other related visualizations.\n\n\n\nRunning the code above will show the desired visualization directly below the code block. In this plot, each row of our dataset — a country — is represented as a point. The location of each point is determined by the country’s HDI and happiness score. Notice that Python has automatically made several choices for the plot that we did not explicitly indicate in the code. For example, the range of values on the two axes, the axis labels, the grid lines, and the tick marks. Python has also automatically picked the color, size, and shape of the points. While the defaults are a good starting point, it’s often useful to modify them; we will see how to change these aspects of the plot in later sections of this chapter.\nAs shown in the example, we can include data-manipulation code before the call to .pipe. This lets us seamlessly combine data manipulation and data visualization.\nScatter plots are typically used to examine the relationship between two numeric variables. What does our first plot tell us about the relationship between HDI and happiness? It appears that, at least in Europe, there is a generally positive relationship between investment in human development and the reported happiness of a population.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html#optional-aesthetics",
    "href": "03_graphics.html#optional-aesthetics",
    "title": "3  EDA II: Visualizing Data",
    "section": "3.4 Optional Aesthetics",
    "text": "3.4 Optional Aesthetics\nThe point geometry requires x and y aesthetics. In addition to these required aesthetics, each geometry type also has several optional aesthetics that add information to the plot. For example, most geometries have a color aesthetic. The syntax for specifying this is similar to the required aesthetics, with the only difference being that we place the aesthetics inside the geom_point() function using another call to aes(). This is true for all aesthetics other than x and y. Let’s see what happens when we add a color aesthetic to our scatter plot by mapping the column subregion to the color aesthetic.\n\n(\n    country\n    .filter(c.region == \"Europe\")\n    .pipe(ggplot, aes(\"hdi\", \"happy\"))\n    + geom_point(aes(color=\"subregion\"))\n)\n\n\n\n\n\n\n\n\nThe result of associating a column in the dataset with a color produces a new variation of the original scatter plot. We have the same set of points and locations on the plot, as well as the same axes. However, now each subregion has been automatically associated with a color, and every point has been colored according to the subregion value in each row of the data. The mapping between colors and region names is shown in an automatically created legend on the right-hand side of the plot. The ability to add additional information to the plot by specifying a single aesthetic mapping illustrates how powerful the grammar of graphics is for quickly producing informative visualizations.\nIn the previous example, we changed the color aesthetic from the fixed default color of black to a color that varies with another variable. It is also possible to specify an alternative fixed value for any aesthetic. We can use the color names available in Python; for example, we might want all of the points to be a shade of green. This can be done with a small change to the function call: set the color aesthetic to the name of a color, such as “red”. However, unlike with variable aesthetics, the mapping needs to be done outside of the aes() function but still within the geom_* function. Below is an example of the code to recreate our plot with a different color. We use a color called “#F5276C”, which is a bright reddish-pink.\n\n(\n    country\n    .filter(c.region == \"Europe\")\n    .pipe(ggplot, aes(\"hdi\", \"happy\"))\n    + geom_point(color=\"#F5276C\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHexadecimal color codes\n\n\n\n\n\nThe color code below uses a format called an HTML hexadecimal color code. These codes let us unambiguously describe a large number of colors with a single string. The first two characters specify the amount of red, the next two the amount of green, and the last two the amount of blue. The values are written in base-16 (hexadecimal) notation, where the digits a–f correspond to the numbers 10 through 15. Usually, people use an interactive website to determine the hex code for a desired color. One good choice is htmlcolorcodes.\n\n\n\nAlthough minor, the change in notation for specifying fixed aesthetics is a common source of confusion for users new to the grammar of graphics; follow the correct argument syntax shown in the code above. You can interchange the fixed and variable aesthetic commands; the relative order should not affect the output. Just be sure to put fixed terms after finishing the aes() call.\nWhile each geometry may have different required and optional aesthetics, the plotnine package tries, as much as possible, to use a common set of terms across geometries. Here are some typical aesthetics we’ll see for most geometries:\n\ncolor: the color of points, lines, or the insides of boxes\nfill: the interior color of boxes and polygons\nalpha: controls the opacity of objects; set to a number between 0 and 1\nsize: controls the size of the points, boxes, font etc. of the layer\nshape: controls the shape of the points\ngroup: groups together data without changing the color; useful for lines\n\nSome of these, such as alpha, are most frequently used with fixed values. If needed, however, almost all can be provided with a variable mapping.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html#text-geometry",
    "href": "03_graphics.html#text-geometry",
    "title": "3  EDA II: Visualizing Data",
    "section": "3.5 Text Geometry",
    "text": "3.5 Text Geometry\nA common critique of computational methods is that they obscure close understanding of each individual object of study by focusing on numeric patterns. This is an important caution: computational analysis should be paired with close analysis. However, visualizations do not always have to reduce complex collections to a few numerical summaries, especially when the dataset contains a relatively small number of observations. Looking back at our first scatter plot, how could we recover a closer analysis of individual countries while also looking for general patterns between the two variables? One option is to add labels indicating the names of the countries. These labels allow viewers to bring their own knowledge of the countries as an additional layer of information when interpreting the plot.\nAdding the names of the countries can be done by using another type of geometry, called text geometry. This geometry is created with the function geom_text. For each row of a dataset, this geometry adds a small textual label. As with the point geometry, we must specify which columns of our data correspond to the x and y aesthetics; these values tell the plot where to place the label. Additionally, the text geometry requires an aesthetic called label that provides the text for each label. In our case, we will use the column called iso to make textual labels on the plot based on the three-letter ISO abbreviation of the country. The code block below produces a plot with text labels by changing the geometry type and adding the label aesthetic.\n\n(\n    country\n    .filter(c.region == \"Europe\")\n    .pipe(ggplot, aes(\"hdi\", \"happy\"))\n    + geom_text(aes(label=\"iso\"))\n) \n\n\n\n\n\n\n\n\nThe plot generated by the code now allows us to see which countries have the highest levels of happiness (Finland, Iceland, and Denmark) and which have the highest levels of HDI (Iceland, Norway, and Switzerland (CHE)). While we added only a single additional piece of information to the plot, each label uniquely identifies a row of the data. This allows anyone familiar with them to bring many more characteristics of each data point to the plot through their own knowledge. For example, although the plot does not include any information about geography, anyone familiar with Europe will note that the Scandinavian countries are clustered together in the upper-right corner of the plot.\nAlthough the text plot adds contextual information compared with the scatter plot, it has some shortcomings. Some labels overlap and become difficult to read, and these issues will only worsen if we increase the number of regions in our dataset. It is also unclear which part of a label corresponds to a country’s HDI or happiness score: the center, the start, or the end? We could add a note that the value refers to the center of the label, but that becomes cumbersome to remember and to keep reminding others about.\nTo start addressing these issues, we can add the points back into the plot, with labels. We do this in Python by adding the two geometry layers (geom_point and geom_text) one after the other. This makes it clearer where each region is located on the x-axis, but it also makes the country names harder to read. To fix that, we’ll add options to the text geometry to nudge the labels upward and reduce their size. Below is the code that makes both of these modifications.\n\n(\n    country\n    .filter(c.region == \"Europe\")\n    .pipe(ggplot, aes(\"hdi\", \"happy\"))\n    + geom_point()\n    + geom_text(aes(label=\"iso\"), size=6, nudge_y=0.5)\n) \n\n\n\n\n\n\n\n\nThe plot with points and text labels shows that we attempted to avoid overlapping labels by nudging them slightly. The points indicate the specific HDI and happiness scores, while the labels identify which country each point represents.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html#lines-and-bars",
    "href": "03_graphics.html#lines-and-bars",
    "title": "3  EDA II: Visualizing Data",
    "section": "3.6 Lines and Bars",
    "text": "3.6 Lines and Bars\nThe plotnine package supplies a large number of different geometries. In this section, we will look at two additional geometry types that allow us to investigate common relationships among the columns of a dataset.\nFor a moment, we look at the cellphone dataset that shows the number of cellphones per one-hundred people over a twenty-year period for each country. Consider a visualization showing the change in the number of cellphones over time. We could create a scatter plot where each point represents a row of the data, the x aesthetic captures the year of each record, and the y aesthetic measures the number of phones. This visualization would be fine and would roughly help us understand changes in cellphone ownership. A more common visualization for data of this format, however, is a line plot, where the values in each year are connected by a line to the values in the subsequent year. To create such a plot, we can use the geom_line geometry. It is most commonly used when the horizontal axis measures some unit of time but can represent other quantities that we expect to change continuously and smoothly between measurements on the x-axis. The line geometry uses the same aesthetics as the point geometry and can be created with the same syntax, as shown in the following block of code.\n\n(\n    cellphone\n    .filter(c.iso.is_in([\"USA\", \"GBR\", \"IRL\", \"CAN\"]))\n    .pipe(ggplot, aes(\"year\", \"cell\"))\n    + geom_line(aes(color=\"iso\"))\n)\n\n\n\n\n\n\n\n\nThe output of this visualization shows how the number of cell phones changes over time. The numbers generally increase slowly in North American countries. In Great Britain, they plateau near 2010, while in Ireland they decrease slightly from a peak around 2007.\nAnother common use of visualization is to compare the values of a numeric column with the categories of a categorical column. You can represent this relationship with a geom_point layer, but it is often more meaningful visually to use a bar for each category, with the height or length of the bar representing the numeric value. This type of plot is common for showing counts of categories (something we will see in the next chapter), but it can be used whenever a numeric value is associated with different categories. To create a plot with bars, we use the geom_col function, providing both x and y aesthetics. By default the categorical variable maps to x and the numeric variable maps to y, but it is often easier to read the other way around. To address this, we add the coord_flip() layer, which flips the axes. The code block below shows the complete commands to create a bar plot of the population for each country in Africa from our dataset.\n\n(\n    country\n    .filter(c.region == \"Africa\")\n    .pipe(ggplot, aes(\"full_name\", \"pop\"))\n    + geom_col()\n    + coord_flip()\n)\n\n\n\n\n\n\n\n\nOne of the first things that stands out in the output is that the regions are ordered alphabetically from bottom to top. The visualization would be much more useful and readable if we could reorder the categories on the y-axis. To do this, we apply the helper function reorder directly inside the graphics code. The function needs to know the name of the variable being reordered and the name of the variable by which it is being reordered.\n\n(\n    country\n    .filter(c.region == \"Africa\")\n    .pipe(ggplot, aes(\"reorder(full_name, pop)\", \"pop\"))\n    + geom_col()\n    + coord_flip()\n)\n\n\n\n\n\n\n\n\nThis function can be applied to other aesthetics, including color, to change the order of categories in the corresponding legend.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html#statistics",
    "href": "03_graphics.html#statistics",
    "title": "3  EDA II: Visualizing Data",
    "section": "3.7 Statistics",
    "text": "3.7 Statistics\nAll the geometries we’ve seen so far directly visualize the rows in the dataset, with one element in the output corresponding to each row in the data. Several geometries first compute summary statistics before plotting. Their aesthetics correspond to the data-summarization process rather than to individual elements in the output. Let’s look at a few common examples.\nWe can use the geom_smooth() geometry, with the option method = \"lm\", to add a line of best fit to the dataset. By default, it adds a confidence band, which we turn off in the code below by setting se = FALSE.\n\n(\n    country\n    .filter(c.region == \"Europe\")\n    .pipe(ggplot, aes(\"hdi\", \"happy\"))\n    + geom_point()\n    + geom_smooth(method=\"lm\", se=False)\n)\n\n\n\n\n\n\n\n\nWe can produce more complex, non-linear smoothing curves by calling geom_smooth() without additional parameters.\nA boxplot lets us see the relationship between a categorical variable and a continuous one. For each category, we plot the 25th, 50th (median), and 75th percentiles as a box with a solid line for the median. A pair of lines, called “whiskers,” extend outward to indicate where most of the remaining data lie. Outliers are plotted as individual points. Often, this involves using reorder() and coord_flip(). Below is an example showing the distribution of HDI values within each region.\n\n(\n    country\n    .pipe(ggplot, aes(\"reorder(region, hdi)\", \"hdi\"))\n    + geom_boxplot()\n    + coord_flip()\n)\n\n\n\n\n\n\n\n\nCertain statistics-based geometries create the y aesthetic directly from the data. These are used to show the univariate distribution of a variable in our dataset. For example, a bar plot using geom_bar takes a categorical variable as a single aesthetic and counts the number of values in each category. The default colors are hard to read, so we change them and flip the coordinates to prevent labels from overlapping. Finally, we can use a special call to reorder to display the results in order of frequency.\n\n(\n    country\n    .pipe(ggplot, aes(\"reorder(subregion, subregion, len)\"))\n    + geom_bar(fill=\"white\", color=\"black\")\n    + coord_flip() \n)\n\n\n\n\n\n\n\n\nTo summarize the distribution of a numeric variable, the equivalent of a bar plot is a histogram, which we create with geom_histogram. The output is easiest to interpret if we set binwidth and boundary to control the width of each bin and how bins are offset from zero. For example, the code below creates histograms that align neatly in groups with a bin width of 0.1. The output shows how many countries fall into each bin.\n\n(\n    country\n    .pipe(ggplot, aes(\"hdi\"))\n    + geom_histogram(fill=\"white\", color=\"black\", binwidth=0.1, boundary=0)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting a histogram\n\n\n\n\n\nIf you find interpreting a histogram difficult, it can help to walk through a concrete example. Take the example above and look at the first bar, which spans from 0.4 to 0.5 on the x-axis. The height on the y-axis appears to be 11, indicating that there are 11 countries in the dataset with an HDI value between 0.4 and 0.5. Similarly, it appears there are around 26 countries with an HDI between 0.8 and 0.9, and around 33 with a value between 0.9 and 1.0.\n\n\n\nWe can also simply set bins=&lt;int&gt; to define the total number of bins. This is an easier way to quickly create results without knowing the scale of the data. However, the resulting counts can be harder to interpret directly.\nAn alternative to a histogram is a density plot. This attempts to model, rather than just counting, the distribution of a continuous variable. The adjust option, which defaults to 1, controls how smooth the distribution is. Higher values produce smoother curves, while lower values more closely fit the data. Trying different values can help you find the one that best reveals the relationship you are interested in.\n\n(\n    country\n    .pipe(ggplot, aes(\"hdi\"))\n    + geom_density(fill=\"white\", color=\"black\", adjust=0.8)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting a density plot\n\n\n\n\n\nThe total area under a density plot equals 1. The integral of the density function between two points (i.e., the area under the curve between those bounds) gives the proportion of observations that fall between those values. With a larger dataset, the density should resemble the histogram’s shape, differing only by scale and being slightly smoother.\n\n\n\nAll of the statistics-based geometries in the section can take an aesthetic called group or color that causes the summaries to be done separately for each unique value of group or color. This quickly becomes very messy and is advisable only when there are a small number (at most 4–5) of groups.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html#scales",
    "href": "03_graphics.html#scales",
    "title": "3  EDA II: Visualizing Data",
    "section": "3.8 Scales",
    "text": "3.8 Scales\nPython makes many choices for us automatically when creating any plot. In the example above, where we set the color of the points to follow another variable in the dataset, Python handles the details of how to pick the specific colors and sizes. It has figured out how large to make the axes, where to add tick marks, and where to draw grid lines. Letting Python deal with these details is convenient because it frees us to focus on the data itself. Sometimes, such as when preparing plots for external distribution or when the defaults are particularly hard to interpret, it is useful to manually adjust these details. This is exactly what scales are designed for.\nEach aesthetic within the grammar of graphics is associated with a scale. Scales detail how a plot maps aesthetics to concrete, perceivable features. For example, a scale for the x aesthetic describes the smallest and largest values on the x-axis and encodes how to label that axis. Similarly, a color scale describes what colors correspond to each category in a dataset and how to format a legend for the plot. To modify the default scales, we add an additional function to the code. The order of the scales relative to the geometries does not affect the output; by convention, scales are usually grouped after the geometries.\nFor example, a popular alternative to the default color palette shown in our previous plot is the function scale_color_cmap_d(). It constructs a set of colors that are color-blind friendly, print well in black and white, and display clearly on poor projectors. After mapping a geometry’s color to a column in the dataset, add this scale by including the function as an extra line in the plot. An example is shown in the following code.\n\n(\n    country\n    .filter(c.region == \"Europe\")\n    .pipe(ggplot, aes(\"hdi\", \"happy\"))\n    + geom_point(aes(color=\"subregion\"))\n    + scale_color_cmap_d()\n)\n\n\n\n\n\n\n\n\nThe output shows that the colors now range from dark purple to bright yellow instead of the rainbow of colors in the default plot. As with the categories in the bar plot, the ordering of the unique colors is determined by putting the categories in alphabetical order. Changing this requires modifying the dataset before passing it to the plot, which we will discuss in the next chapter. Note that the _d at the end of the scale function indicates the colors are used to create mappings for a character variable (it stands for “discrete”). There is also a complementary function scale_color_cmap() that produces a similar set of colors when making the color of the points vary according to a numeric variable.\nMany other scales exist to control various aesthetics. For example, scale_size_area can make point sizes proportional to another column in the dataset. Several scales control the x and y axes; for example, we can add scale_x_log10() and scale_y_log10() to a plot to display values on a logarithmic scale, which is useful with heavily skewed datasets. We’ll use these in later chapters as needed.\nThe default scale for the x-axis is called scale_x_continuous. A corresponding function, scale_y_continuous, is the default for the y-axis. Adding these to a plot by themselves has no visible effect. However, there are many optional arguments we can provide to these functions that change how a plot is displayed. Setting breaks within one of these scales tells Python how many labels to put on the axis. Usually we use a helper function to set them, such as breaks_width. We can set limits to a pair of numbers to specify the start and end of the plotted range. Below is the code to produce a plot that shows the same data as our original scatter plot but with modified grid lines, axis labels, and vertical range.\n\n(\n    country\n    .filter(c.region == \"Europe\")\n    .pipe(ggplot, aes(\"hdi\", \"happy\"))\n    + geom_point()\n    + scale_x_continuous(breaks=True, limits=(0, 1))\n    + scale_y_continuous(breaks=breaks_width(2))\n)\n\n\n\n\n\n\n\n\nFinally, there are two special scale types that can be useful for working with colors. In some cases, a column in the dataset already specifies the color of each observation; in that case, it’s sensible to use those colors directly. To do that, add the scale scale_color_identity to the plot. Another useful scale for colors is scale_color_manual; with it, you can specify exactly which color should be used for each category. Below is the syntax for specifying manual colors for each region in the countries dataset.\n\n(\n    country\n    .filter(c.region == \"Europe\")\n    .pipe(ggplot, aes(\"hdi\", \"happy\"))\n    + geom_point(aes(color=\"subregion\"))\n    + scale_color_manual(values={\n        \"Eastern Europe\": \"lightblue\",\n        \"Northern Europe\": \"navy\", \n        \"Southern Europe\": \"peachpuff\",\n        \"Western Europe\": \"wheat\"\n    })\n)\n\nUsing manual colors is generally advisable when there are well-known colors associated with the groups in the dataset. For example, when plotting data about political parties, it may be helpful to use the colors traditionally associated with each party. The plotnine documentation includes additional ways to customize visualizations using a variety of alternative scales.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html#multiple-datasets",
    "href": "03_graphics.html#multiple-datasets",
    "title": "3  EDA II: Visualizing Data",
    "section": "3.9 Multiple Datasets",
    "text": "3.9 Multiple Datasets\nSo far, we have seen how to create visualizations using multiple layers with a single dataset. It is also possible to use different datasets in each layer by defining the data= argument for a specific geometry. When setting the data= argument for a geometry, it affects only that geometry; all other geometries will continue to use the default dataset.\nA common reason to use two different datasets in the same plot is to highlight a subset of points within a larger dataset.\n\ncountry_ne = country.filter(c.subregion == \"Northern Europe\")\n(\n    country\n    .filter(c.region == \"Europe\")\n    .pipe(ggplot, aes(\"hdi\", \"happy\"))\n    + geom_point(alpha=0.2)\n    + geom_point(color=\"#F5276C\", data=country_ne)\n    + geom_text(\n        aes(label=\"iso\"),\n        color=\"#F5276C\",\n        nudge_y=0.5,\n        size=8,\n        data=country_ne\n    ) \n)\n\n\n\n\n\n\n\n\nThis approach makes it easy to highlight the relationship between a subset of the data and the full dataset.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html#facets",
    "href": "03_graphics.html#facets",
    "title": "3  EDA II: Visualizing Data",
    "section": "3.10 Facets",
    "text": "3.10 Facets\nAnother way to show differences across groups of a categorical variable is through facets. These create a grid of small plots in which the data is separated by one or two variables. This is done by adding the facet_wrap() function to the plot and supplying the grouping variable. Often, as in the example below, we add color mapped to the grouping variable and turn off the legend to visually indicate which data appears in each facet.\n\n(\n    country\n    .pipe(ggplot, aes(\"hdi\", \"happy\"))\n    + geom_point(aes(color=\"region\"), show_legend=False)\n    + facet_wrap(\"region\")\n)\n\n\n\n\n\n\n\n\nThe space option can be set to fixed (the default), free_x, free_y, or free to control whether subplots share the same scales. The facet_grid function lets you supply two variables to arrange across rows and columns.\nFacets can be very powerful and useful for making certain kinds of relationships more readable and make your outputs look very professional. However, take care in using them, as most people getting started in data science have a tendency to significantly overuse facets. Typically, they are best when we have only a small set of categories and have already used all of the other tricks in the sections above to visualize the patterns in the data.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html#labels-and-themes",
    "href": "03_graphics.html#labels-and-themes",
    "title": "3  EDA II: Visualizing Data",
    "section": "3.11 Labels and Themes",
    "text": "3.11 Labels and Themes\nThroughout this chapter, we have seen a number of ways to create and modify data visualizations. One thing we did not cover was how to label our axes. While many data visualization guides stress the importance of labeling axes, it is often best to use the default labels provided by Python during the exploratory phase of analysis. These defaults are useful for several reasons. First, they require minimal effort and make it easy to change axes, variables, and other settings without spending time adjusting labels. Second, the default labels use the variable names in our dataset. When writing code, those names are exactly what we need to refer to a variable in additional plots, models, and data-manipulation tasks. Of course, when we want to present our results to others, it is essential to provide more detailed descriptions of the axes and legends in our plots. Fortunately, these can be added directly using the grammar of graphics.\nIn order to change the labels in a plot, we can use the labs function as an additional layer of our plot. Inside the function, we assign labels to the names of aesthetic values that we want to describe. Leaving a value unspecified will keep the default value in place. Labels for the x-axis and y-axis will be placed on the sides of the plot; labels for other aesthetics such as size and color will be placed in the legend. We can also add title, subtitle, and a caption to labs to provide additional information describing the plot as a whole. Here is an example of our color-coded scatter plot that includes a variety of labels.\n\n(\n    country\n    .filter(c.region == \"Europe\")\n    .pipe(ggplot, aes(\"hdi\", \"happy\"))\n    + geom_point(aes(color=\"subregion\"))\n    + labs(\n        x=\"Human Development Index (HDI)\",\n        y=\"Happiness Score (0-100)\",\n        color=\"Subregion\",\n        title=\"Relationship between HDI and Happiness\",\n        subtitle=\"Europe\",\n        caption=\"Data from the Gapminder\"\n    )\n)\n\n\n\n\n\n\n\n\nAnother way to prepare our graphics for distribution is to modify the theme of a plot. Themes affect the appearance of plot elements such as axis labels, ticks, and the background. Throughout this book, we’ve set the default plot theme to theme_minimal(). We think it’s a great choice for exploring a dataset. As the name implies, it removes much of the clutter of other themes while keeping grid lines and other visual cues that help with interpretation. When presenting information for external publication, we often prefer a theme based on the work of Edward Tufte, a well-known scholar in the field of data visualization. To set the theme, we can add theme_minimal() or another theme function to our plot, as in the example below.\n\ntheme_set(theme_minimal())\n\nNow, whenever we construct a plot, it will use the newly assigned theme. Minimal themes are designed to use as little “ink” as possible, focusing the reader on the data [3]. They can be a bit too minimal when first exploring a dataset, but are a great tool for presenting results. Again, there are many resources online to customize them according to our needs.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html#best-practices",
    "href": "03_graphics.html#best-practices",
    "title": "3  EDA II: Visualizing Data",
    "section": "3.12 Best Practices",
    "text": "3.12 Best Practices\nWhile the focus of this chapter has been how to create certain graphics, it will be helpful to have a few guiding principles about which graphics to create for certain applications. These are general best practices that you may need to break for specific tasks.\n\nUse barplots, histograms, and density plots to understand the distribution of a single variable.\nUse scatter plots (points) and boxplots to understand the relationship between two variables.\nUse scatter plots with text labels to understand individal data points, either as a whole or in relationship to a larger set.\nOnly use variable color scales when you have already made use of the x- and y-aesthetics as much as possible.\nSize and color for continuous variables should be used as an extra element, not the primary one. Do not use size for categorical variables.\nColor and shape should only be used when we have a small set of categories. The exact cut-off depends on the overall datasize and application, but 8 is a good rough number.\nLine plots should be reserved for the case where a single quantity is measured over time, space, or another continuous variable.\n\nThe best way to understand which plots to make for applications — and when it’s appropriate to break those rules — is to see many examples. We’ll do this throughout the remainder of this text.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html#coming-from-r",
    "href": "03_graphics.html#coming-from-r",
    "title": "3  EDA II: Visualizing Data",
    "section": "3.13 Coming from R",
    "text": "3.13 Coming from R\nThere is a relatively small gap between R’s ggplot2 package and plotnine. plotnine was intentionally created to mirror R’s ggplot2 as closely as possible in Python. The only differences are that variable names must be quoted, the entire code block is enclosed in parentheses, and the plus sign is placed at the start of the line rather than the end. Placing the plus sign at the start helps avoid many common errors and makes it easier to comment out particular lines of code when testing.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_graphics.html#references",
    "href": "03_graphics.html#references",
    "title": "3  EDA II: Visualizing Data",
    "section": "References",
    "text": "References\n\n\n\n\n[1] Lavanya, A, Gaurav, L, Sindhuja, S, Seam, H, Joydeep, M, Uppalapati, V, Ali, W and SD, V S (2023 ). Assessing the performance of python data visualization libraries: A review. Int. J. Comput. Eng. Res. Trends. 10 28–39\n\n\n[2] Wilkinson, L (2012 ). The Grammar of Graphics. Springer\n\n\n[3] Tufte, E (1987 ). The Visual Display of Quantitative Information. Graphics Press",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html",
    "href": "04_combine.html",
    "title": "4  EDA III: Restructuring Data",
    "section": "",
    "text": "4.1 Setup\nLoad all of the modules and datasets needed for the chapter.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\ncountry = pl.read_csv(\"data/countries.csv\")",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#introduction",
    "href": "04_combine.html#introduction",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nIn Chapter 2, we learned to use data operations to modify a dataset in Python. These modifications included taking a subset of a DataFrame, such as filtering rows or selecting a reduced set of columns. We saw how to add new columns and how to rearrange rows by sorting on one or more columns. We also explored grouping and summarizing our data to create a new set of summary statistics that aggregate the original dataset at different levels of granularity. Along the way, we saw how these modifications can help create informative data visualizations, particularly when there is too much data to label every row as a point on a single plot.\nIn this chapter, we’ll continue exploring how to modify data using increasingly advanced techniques. First, we’ll examine how to combine information from two different data tables. Then we’ll move on to pivots, a relatively complex but powerful method for reshaping data.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#primary-and-foreign-keys",
    "href": "04_combine.html#primary-and-foreign-keys",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.3 Primary and Foreign Keys",
    "text": "4.3 Primary and Foreign Keys\nPrimary keys are a core concept we need to understand to combine two datasets in Python. A primary key consists of one or more columns that can uniquely and minimally identify a row of data. For example, in our country dataset the column iso is a primary key. The column full_name is also a primary key. In database theory a distinction is made between candidate keys (any set of one or more columns that could serve as a primary key) and the specific primary key chosen for a table. This distinction is less applicable to data science work, where we will use the term primary key to refer to any possible way of identifying a row in our dataset.\nA foreign key is a primary key from one table that appears in another table. For example, we might have a dataset of customers, with one column indicating the iso of the country in which they live. We join multiple tables by associating the primary key in one table with the matching foreign key in another. Notice that the difference between primary and foreign is the table in which the key appears, not the information itself. The value iso is both a primary key in the countries dataset and a foreign key in the customers dataset. The connection between these two tables is called a relation. This is where the term relational database comes from.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#joining-by-relation",
    "href": "04_combine.html#joining-by-relation",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.4 Joining by Relation",
    "text": "4.4 Joining by Relation\nWe will start with an abstract example before turning to a specific example using actual DataFrames in Python. Assume we have two tables, table_one and table_two. The column key_col appears in both tables: it is a primary key in table_two and a foreign key in table_one. This means it has a single unique row associated with it in table_two but possibly many instances in table_one.\nTo join the information in these two tables, we use the .join() method of the table containing the foreign key. The arguments include the second dataset and the name of the key used to combine the tables.\n\n(\n    table_one\n    .join(table_two, on=c.key_col)\n)\n\nAssuming our data are correctly organized, the output will be a dataset with the same number of rows as table_one, where the first set of columns will also match table_one. After the original columns, there will be a version of all the columns from table_two that correspond to the matching values of our key.\nOften some of the values in table_one will not have a matching value in table_two. In that case, the default behavior is to remove rows without matches; this is called an inner join. Alternatively, you can fill missing columns with null values, which results in a left join. We can indicate this to Python as follows:\n\n(\n    table_one\n    .join(table_two, on=c.key_col, how=\"left\")\n)\n\n\n\n\n\n\n\nLeft and right tables\n\n\n\n\n\nThe terminology of “left” and “right” tables comes from thinking about how the join would look when written on a single line. Consider the code table_one.join(table_two); the first table is to the left of the join function and the second table is to the right of the join function. As a rule of thumb, we try to place the table with the foreign key on the left and the table with the primary key on the right. The reason for this is that the output should match the number of observations in the table with the foreign key, so it typically makes it easier to reason about treating it as a method of the foreign-key table. Note that usually the table with the foreign key will be much larger than the table with the primary key.\n\n\n\n\n\n\n\n\n\nEqui-joins: inner, left, right, full, semi, anti, cross\n\n\n\n\n\nThere are six different options that we can assign to the how parameter in the table join method. The default, inner, returns only rows that match in both tables. Left joins return all rows from the left DataFrame; right joins return all rows from the right DataFrame. Full joins (rarely used) return all rows from both tables. A semi join returns only rows from the left table that have a match in the right table; no additional columns are added. An anti join returns only rows in the left table that do not match any rows in the right table. These six options are called equi-joins because they work by finding where the key(s) in one table are equal to the key(s) in another table.\nA cross join returns all combinations of rows from the two tables, with no key required. It has several interesting applications, which we will see in later chapters.\n\n\n\nThere are several other modifications that we can apply to the basic syntax. For example, if our key has multiple columns, we can pass a list to on instead of a single value. Here is the code to join on two matching columns from the two DataFrames.\n\n(\n    table_one\n    .join(table_two, on=[c.key_col1, c.key_col2])\n)\n\nOr, if the key has different names in the two tables, we can describe them with on_left (the first table) and on_right (the second table). These can be individual values or lists, where each element in one list corresponds to the element in the other.\n\n(\n    table_one\n    .join(table_two, on_left=c.key_col1, on_left=c.key_col1)\n)\n\nNote that Python will let you join on any set of columns that match between the two tables. The results can be surprising, and possibly very large, if we are not sure that the join column(s) are a primary key in one of the tables. Avoid joins that do not follow the pattern here (foreign key in the left table and a primary key in the right table).",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#join-examples",
    "href": "04_combine.html#join-examples",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.5 Join Examples",
    "text": "4.5 Join Examples\nThe code outline in the previous section should make more sense once we see examples using actual datasets. To do this, we need multiple datasets that we can join together. We will create a smaller version of the country dataset that contains only two columns, which makes it easier to view the results without scrolling. We also load a dataset of country borders, where each row shows the relationship between a country and one of its neighbors.\n\nc_sml = country.select(c.iso, c.gini)\nborder = pl.read_csv(\"data/countries_borders.csv\")\nborder\n\n\nshape: (829, 2)\n\n\n\niso\niso_neighbor\n\n\nstr\nstr\n\n\n\n\n\"AFG\"\n\"IRN\"\n\n\n\"AFG\"\n\"PAK\"\n\n\n\"AFG\"\n\"CHN\"\n\n\n\"AFG\"\n\"TJK\"\n\n\n\"AFG\"\n\"TKM\"\n\n\n…\n…\n\n\n\"ZMB\"\n\"ZWE\"\n\n\n\"ZWE\"\n\"BWA\"\n\n\n\"ZWE\"\n\"MOZ\"\n\n\n\"ZWE\"\n\"ZAF\"\n\n\n\"ZWE\"\n\"ZMB\"\n\n\n\n\n\n\nWe can use a relational join to add the Gini coefficient for the country referenced in the border data. Note that iso is present in both tables; it’s a primary key in c_sml and a foreign key in border. We can join them using the code below.\n\n(\n    border\n    .join(c_sml, on=c.iso)\n)\n\n\nshape: (641, 3)\n\n\n\niso\niso_neighbor\ngini\n\n\nstr\nstr\nf64\n\n\n\n\n\"AFG\"\n\"IRN\"\n27.8\n\n\n\"AFG\"\n\"PAK\"\n27.8\n\n\n\"AFG\"\n\"CHN\"\n27.8\n\n\n\"AFG\"\n\"TJK\"\n27.8\n\n\n\"AFG\"\n\"TKM\"\n27.8\n\n\n…\n…\n…\n\n\n\"ZMB\"\n\"ZWE\"\n57.1\n\n\n\"ZWE\"\n\"BWA\"\n50.3\n\n\n\"ZWE\"\n\"MOZ\"\n50.3\n\n\n\"ZWE\"\n\"ZAF\"\n50.3\n\n\n\"ZWE\"\n\"ZMB\"\n50.3\n\n\n\n\n\n\nNotice that the Gini coefficient of the base country has been added to the data. The first five rows all correspond to borders with Afghanistan and therefore the same value of the Gini coefficient (27.8) is duplicated for each of the rows. Similarly, the last four rows correspond to Zimbabwe and repeat the Gini coefficient value of 50.3 in each. As with other data methods we use, the .join() method returns a new dataset without modifying the original data. We would need to save the output if we wanted to work further with the results.\nThe DataFrame after the join has only 641 observations compared to the 829 observations in the original. The reason is that our country data only contains those countries for which we have full economic data. This means that some of the countries in border do not have an entry in c_sml. The default relational join is an inner join, so these missing corresponding rows are removed from the result. Changing to a left join restores the DataFrame to the same number of observations as the original border data.\n\n(\n    border\n    .join(c_sml, on=c.iso, how=\"left\")\n)\n\n\nshape: (829, 3)\n\n\n\niso\niso_neighbor\ngini\n\n\nstr\nstr\nf64\n\n\n\n\n\"AFG\"\n\"IRN\"\n27.8\n\n\n\"AFG\"\n\"PAK\"\n27.8\n\n\n\"AFG\"\n\"CHN\"\n27.8\n\n\n\"AFG\"\n\"TJK\"\n27.8\n\n\n\"AFG\"\n\"TKM\"\n27.8\n\n\n…\n…\n…\n\n\n\"ZMB\"\n\"ZWE\"\n57.1\n\n\n\"ZWE\"\n\"BWA\"\n50.3\n\n\n\"ZWE\"\n\"MOZ\"\n50.3\n\n\n\"ZWE\"\n\"ZAF\"\n50.3\n\n\n\"ZWE\"\n\"ZMB\"\n50.3\n\n\n\n\n\n\nSo far, we have added the Gini coefficient of the base country. We could also add the coefficient of the bordering country by associating the column iso_neighbor with the column iso in our countries DataFrame. Here is the code to add the neighboring Gini coefficient.\n\n(\n    border\n    .join(c_sml, left_on=c.iso_neighbor, right_on=c.iso)\n)\n\n\nshape: (667, 3)\n\n\n\niso\niso_neighbor\ngini\n\n\nstr\nstr\nf64\n\n\n\n\n\"AFG\"\n\"IRN\"\n40.9\n\n\n\"AFG\"\n\"PAK\"\n29.6\n\n\n\"AFG\"\n\"CHN\"\n38.2\n\n\n\"AFG\"\n\"TJK\"\n34.0\n\n\n\"AFG\"\n\"UZB\"\n35.3\n\n\n…\n…\n…\n\n\n\"ZMB\"\n\"ZWE\"\n50.3\n\n\n\"ZWE\"\n\"BWA\"\n53.3\n\n\n\"ZWE\"\n\"MOZ\"\n54.0\n\n\n\"ZWE\"\n\"ZAF\"\n63.1\n\n\n\"ZWE\"\n\"ZMB\"\n57.1\n\n\n\n\n\n\nWhat if we wanted to add the Gini coefficient to both countries in each row? This requires doing two separate joins, one after the other. In the second join, we add the suffix _neighbor to the Gini coefficient to distinguish the two scores in the output.\n\n(\n    border\n    .join(c_sml, on=c.iso)\n    .join(c_sml, left_on=c.iso_neighbor, right_on=c.iso, suffix=\"_neighbor\")\n)\n\n\nshape: (523, 4)\n\n\n\niso\niso_neighbor\ngini\ngini_neighbor\n\n\nstr\nstr\nf64\nf64\n\n\n\n\n\"AFG\"\n\"IRN\"\n27.8\n40.9\n\n\n\"AFG\"\n\"PAK\"\n27.8\n29.6\n\n\n\"AFG\"\n\"CHN\"\n27.8\n38.2\n\n\n\"AFG\"\n\"TJK\"\n27.8\n34.0\n\n\n\"AFG\"\n\"UZB\"\n27.8\n35.3\n\n\n…\n…\n…\n…\n\n\n\"ZMB\"\n\"ZWE\"\n57.1\n50.3\n\n\n\"ZWE\"\n\"BWA\"\n50.3\n53.3\n\n\n\"ZWE\"\n\"MOZ\"\n50.3\n54.0\n\n\n\"ZWE\"\n\"ZAF\"\n50.3\n63.1\n\n\n\"ZWE\"\n\"ZMB\"\n50.3\n57.1\n\n\n\n\n\n\nAs noted above, we can add the option how= with the values inner (default), left, right, or outer to indicate what should happen when keys appear only in one table. Left joins retain all keys from the first (left) table, right joins retain all keys from the second (right) table, and outer joins retain all keys. When there is no matching value in the other table, null values are filled in. The default, inner, is often a good choice to avoid missing values.\nAnother use of a key-based join is to determine which elements match between one table and another without adding any extra columns. These are called filtering joins because they act like the .filter method in that they select a subset of rows. There are two variants: a semi-join retains only those rows that have a match in the second table, and an anti-join retains only those rows that do not. When there is only a single key, semi- and anti-joins can be replaced by a call to isin. The real power comes when there are multiple keys and we want to know whether a specific combination of keys matches (or does not match) those found in another table.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#conditional-joins",
    "href": "04_combine.html#conditional-joins",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.6 Conditional Joins",
    "text": "4.6 Conditional Joins\nIn this section, we introduce two additional kinds of joins that are helpful when working with certain types of complex data. Both of these allow us to join by relationships between keys that do not require values to be exactly equal. To start, let’s create two subsets of our countries data, separating out countries in Africa and in Asia, and sorting by the Gini coefficient. We remove any missing values from the output.\n\nafrica = (\n    country\n    .filter(c.region == \"Africa\")\n    .select(c.full_name, c.gini)\n    .sort(c.gini)\n    .drop_nulls()\n)\nasia = (\n    country\n    .filter(c.region == \"Asia\")\n    .select(c.full_name, c.gini)\n    .sort(c.gini)\n    .drop_nulls()\n)\n\nThe function .join_asof combines two datasets by matching each row in the first (left) dataset to the nearest value in the second (right) dataset based on numeric values in a specified pair of columns. It always returns a dataset with the same number of rows as the first (left) dataset. The function is optimized to do this quickly even for large datasets; to achieve this, it requires that both datasets be sorted by the join key before being called. Below is an example where we associate each African country with the nearest Asian country in terms of their Gini coefficient. We set coalesce=False to ensure the matching country’s Gini coefficient is retained in the output.\n\n(\n    africa\n    .join_asof(asia, on=c.gini, strategy=\"nearest\", coalesce=False)\n)\n\n\nshape: (35, 4)\n\n\n\nfull_name\ngini\nfull_name_right\ngini_right\n\n\nstr\nf64\nstr\nf64\n\n\n\n\n\"Guinea\"\n29.6\n\"Pakistan\"\n29.6\n\n\n\"Egypt\"\n31.5\n\"Korea, Republic of\"\n31.4\n\n\n\"Mauritania\"\n32.6\n\"Mongolia\"\n32.7\n\n\n\"Ethiopia\"\n35.0\n\"Thailand\"\n35.0\n\n\n\"Nigeria\"\n35.1\n\"Thailand\"\n35.0\n\n\n…\n…\n…\n…\n\n\n\"Botswana\"\n53.3\n\"Malaysia\"\n46.2\n\n\n\"Mozambique\"\n54.0\n\"Malaysia\"\n46.2\n\n\n\"Zambia\"\n57.1\n\"Malaysia\"\n46.2\n\n\n\"Namibia\"\n59.1\n\"Malaysia\"\n46.2\n\n\n\"South Africa\"\n63.1\n\"Malaysia\"\n46.2\n\n\n\n\n\n\nThe original motivation for the .join_asof method was to join two time-series datasets. The idea is that we might want the current (most recent) value from one table relative to another. Hence the function’s name and the fact that it uses the backward strategy by default to find the nearest value in the second dataset that is less than (i.e., earlier than) the value in the first. You can set the strategy to forward to look in the other direction, or to nearest to find the closest value. You can also add additional keys with the by, by_left, or by_right options; these keys must match exactly before performing the nearest search. The tables must be pre-sorted by the join key(s) for the function to work.\nIf we need to do more exotic joins between two tables, we can use the .join_where method. It takes one or more expressions in the same format used by the .filter method to specify exactly which pairs of rows to keep in the joined dataset. To refer to columns that share the same name in both datasets, use the _right suffix to indicate the second dataset. Below is an example that finds all pairs of countries in Africa and Asia where the difference in the Gini coefficient is less than 5.\n\n(\n    africa\n    .join_where(\n        asia,\n        (c.gini - c.gini_right).abs() &lt; 5\n    )\n)\n\n\nshape: (414, 4)\n\n\n\nfull_name\ngini\nfull_name_right\ngini_right\n\n\nstr\nf64\nstr\nf64\n\n\n\n\n\"Guinea\"\n29.6\n\"Armenia\"\n25.2\n\n\n\"Guinea\"\n29.6\n\"United Arab Emirates\"\n26.0\n\n\n\"Guinea\"\n29.6\n\"Azerbaijan\"\n26.6\n\n\n\"Guinea\"\n29.6\n\"Afghanistan\"\n27.8\n\n\n\"Guinea\"\n29.6\n\"Kazakhstan\"\n27.8\n\n\n…\n…\n…\n…\n\n\n\"Cameroon\"\n46.6\n\"Philippines\"\n42.3\n\n\n\"Cameroon\"\n46.6\n\"Malaysia\"\n46.2\n\n\n\"Burkina Faso\"\n47.3\n\"Malaysia\"\n46.2\n\n\n\"Congo\"\n48.9\n\"Malaysia\"\n46.2\n\n\n\"Zimbabwe\"\n50.3\n\"Malaysia\"\n46.2\n\n\n\n\n\n\nThis method can be slow because it needs to manually check every combination of rows in the two datasets. However, it is very powerful and useful when combining two reasonably sized datasets in a complex way. Notice that we could obtain the same result by starting with a cross join (which returns every combination of rows) and then applying a filter. The benefit of the .join_where is that it avoids having to store in memory all combinations of non-matching rows by effectively combining the cross join and filtering into a single step.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#units-of-observation",
    "href": "04_combine.html#units-of-observation",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.7 Units of Observation",
    "text": "4.7 Units of Observation\nWhen designing a way to store tabular data, we often must decide whether to favor many rows and few columns or many columns and few rows. A related decision, particularly when choosing a format with more columns, is what information to store in columns versus rows. In some cases the answers are clear and obvious. In others, there are trade-offs about which types of analyses will be easiest or hardest to perform with the data. Let’s work through an example.\nAssume that you are a biologist tracking the heights of 100 plants over the next 14 days. There are several different ways that this data could be collected. We could have each day’s measurements in a column and each plant as a row. We could have each plant as a column and each day as a row. Or we could have a row for each combination of plant and day, with 100*14 = 1,400 rows and three columns (plant ID, day ID, and height).\nA core data-science concept will help us distinguish the three cases above. A unit of observation is the entity about which data are collected and recorded in a dataset. In other words, it is the “who” or “what” that each row of the table represents. In the example with one row for each plant and one column for each day, the unit of observation is a plant. In the example with one row for each day and one column for each plant, the unit of observation is a day. In the final example, the one with 1,400 rows, we call the unit of observation plant × day (read “plant crossed with day”), which means each row corresponds to a unique combination of one plant and one day. The key takeaway is that once we have picked the unit of observation, there is only a single way to construct our data table. Choosing the right unit of observation for our data will be a central concern in the next chapter.\nNote that the concept of a unit of observation is closely linked to that of a primary key. By definition, a primary key must uniquely identify the “what” of the unit of observation. For example, a primary key for a table whose unit of observation is a plant must uniquely identify each plant. Likewise, a table with a plant × day unit of observation needs a key that uniquely identifies both the plant and the day.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#pivot-and-unpivot",
    "href": "04_combine.html#pivot-and-unpivot",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.8 Pivot and Unpivot",
    "text": "4.8 Pivot and Unpivot\nSometimes the unit of observation in which a data table is stored does not match the unit needed for our analysis. If the stored format uses a compound unit of observation, we can simplify it by using an aggregation to remove one or more components. For example, we could group a plant × day table by plant ID and calculate summary statistics (such as the maximum height over all days) to generate a dataset with plant as the unit of observation. While aggregations are useful, by design they remove information present in the original dataset. Other times, we want to restructure the unit of observation without losing or adding any information—we simply need the data in a different shape for a particular analysis or visualization. To do this, we can use one of two additional data manipulation functions.\nConsider the cellphone dataset; we show its first few rows below. The unit of observation is year × country: each row gives the number of cellphones per 100 people for that year and country.\n\ncellphone = pl.read_csv(\"data/countries_cellphone.csv\")\ncellphone\n\n\nshape: (3_480, 3)\n\n\n\niso\nyear\ncell\n\n\nstr\ni64\nf64\n\n\n\n\n\"AFG\"\n2003\n0.87978\n\n\n\"AFG\"\n2004\n2.54662\n\n\n\"AFG\"\n2005\n4.91711\n\n\n\"AFG\"\n2006\n9.9133\n\n\n\"AFG\"\n2007\n18.0167\n\n\n…\n…\n…\n\n\n\"ZWE\"\n2018\n85.8627\n\n\n\"ZWE\"\n2019\n86.4094\n\n\n\"ZWE\"\n2020\n84.9604\n\n\n\"ZWE\"\n2021\n90.2538\n\n\n\"ZWE\"\n2022\n88.9958\n\n\n\n\n\n\nWe could have stored this dataset in a format where the unit of observation was year and each country had its own column. What if we want to work with the data in that format? For example, if we wanted to compute the difference in cellphone ownership between the United States and Canada, we would need the data in this format. We can convert our dataset using the .pivot() method, which requires indicating which column will be used to index the output (this becomes the unit of observation), which column will be used to create the new columns, and which column contains the values that will fill the table.\n\ncell_wide = (\n    cellphone\n    .pivot(index=\"year\", on=\"iso\", values=\"cell\")\n)\ncell_wide\n\n\nshape: (20, 175)\n\n\n\nyear\nAFG\nAGO\nALB\nAND\nARE\nARG\nARM\nAUS\nAUT\nAZE\nBDI\nBEL\nBEN\nBFA\nBGD\nBGR\nBHR\nBHS\nBIH\nBLR\nBLZ\nBOL\nBRA\nBRB\nBRN\nBTN\nBWA\nCAF\nCAN\nCHE\nCHL\nCHN\nCIV\nCMR\nCOD\nCOG\n…\nSEN\nSGP\nSLB\nSLV\nSMR\nSOM\nSTP\nSUR\nSVK\nSVN\nSWE\nSWZ\nSYC\nSYR\nTCD\nTGO\nTHA\nTJK\nTLS\nTON\nTTO\nTUN\nTUR\nTZA\nUGA\nUKR\nURY\nUSA\nUZB\nVCT\nVEN\nVNM\nVUT\nYEM\nZAF\nZMB\nZWE\n\n\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n…\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2003\n0.87978\n1.95054\n35.278\n74.6812\n70.8971\n20.4096\n3.76374\n72.3885\n89.5526\n12.5837\n0.90868\n82.9377\n2.98462\n1.81994\n0.96829\n44.8576\n54.6188\n36.4188\n25.7218\n11.4045\n22.899\n14.1298\n25.6741\n52.6759\n51.1674\n0.35363\n25.0588\n0.97488\n41.7477\n84.3941\n45.3566\n20.8536\n6.69626\n6.66446\n2.25179\n9.57701\n…\n7.29792\n86.4376\n0.22739\n19.1676\n60.6975\n2.01834\n3.133\n33.2775\n68.4271\n87.2851\n98.2426\n7.9685\n56.563\n6.62407\n0.68509\n4.38771\n33.3244\n0.71512\n2.18764\n10.7214\n25.1251\n19.0389\n41.1128\n3.49964\n2.94538\n13.6577\n15.1631\n55.4096\n1.24776\n55.6041\n27.093\n3.44629\n3.88551\n3.14667\n34.7626\n2.19418\n2.97287\n\n\n2004\n2.54662\n3.9784\n40.6495\n78.5281\n83.1597\n34.8114\n6.72643\n82.2081\n97.7795\n17.156\n1.37553\n87.6255\n5.62957\n2.93277\n1.94647\n61.0333\n74.5619\n54.743\n33.9881\n23.0022\n27.6052\n19.5647\n35.9135\n74.8745\n57.3669\n2.94185\n28.9318\n1.43102\n46.7437\n84.9756\n57.2138\n25.7141\n8.54469\n9.21627\n3.49263\n10.76\n…\n10.2163\n95.6368\n0.6321\n30.5192\n60.5787\n4.86334\n4.91107\n41.4107\n79.5226\n92.7311\n97.6783\n13.5115\n61.6736\n12.7939\n1.23991\n5.83805\n41.1989\n1.98806\n2.76856\n15.6085\n48.4019\n36.7464\n50.5995\n5.09582\n4.29039\n29.0897\n18.2681\n63.1242\n2.09033\n63.81\n31.9693\n6.17384\n5.10619\n6.68019\n42.536\n4.09548\n3.4429\n\n\n2005\n4.91711\n8.35159\n49.7453\n83.3882\n97.1993\n56.4973\n10.5791\n90.7636\n105.296\n26.0781\n2.01653\n91.6576\n7.07639\n4.54743\n6.21909\n81.1551\n82.2539\n66.2329\n38.9203\n42.4035\n34.3187\n25.8676\n46.6789\n76.7344\n64.8898\n5.43918\n30.6424\n2.33165\n52.4553\n91.959\n64.6505\n30.0304\n11.7071\n13.1922\n4.67216\n15.101\n…\n15.3986\n102.723\n1.24203\n40.1585\n60.0238\n4.69738\n7.39487\n44.8256\n84.4454\n88.0975\n100.822\n18.5353\n65.8773\n15.6797\n2.03329\n7.41937\n46.1397\n3.82687\n3.48746\n28.2689\n68.3489\n55.3921\n62.901\n7.56468\n4.70367\n64.0054\n35.1492\n68.8835\n2.73172\n62.8012\n46.6505\n11.8306\n6.02202\n9.99364\n68.6198\n8.10286\n5.18375\n\n\n2006\n9.9133\n15.2614\n62.6026\n86.7048\n110.185\n79.5273\n42.1272\n95.9663\n112.226\n38.1512\n2.54273\n93.3538\n12.1442\n7.07267\n13.0671\n108.025\n91.2343\n72.6192\n46.4606\n62.0314\n40.9896\n30.2062\n53.5317\n87.8212\n82.6517\n12.2359\n43.9084\n2.5128\n57.2331\n99.4337\n75.4174\n34.9802\n19.7861\n17.8682\n7.28434\n23.9093\n…\n25.9356\n108.658\n1.42394\n64.1687\n60.0857\n5.01116\n11.1231\n60.9527\n91.0056\n90.8372\n105.797\n23.0535\n77.833\n23.8071\n4.35335\n11.7791\n60.2777\n30.4638\n5.02766\n28.2825\n111.783\n70.9333\n75.1837\n13.9199\n6.97652\n105.277\n70.8316\n76.8586\n9.47736\n78.2349\n69.0163\n22.9925\n6.94692\n12.6482\n79.3267\n13.713\n6.71982\n\n\n2007\n18.0167\n23.8782\n76.8284\n77.559\n137.458\n100.962\n63.0765\n101.458\n119.466\n51.1876\n3.30806\n101.06\n22.9147\n12.5405\n23.2247\n130.455\n105.539\n105.849\n61.0557\n72.7795\n39.9574\n33.5993\n64.1626\n94.953\n98.7233\n22.0203\n60.2637\n7.61859\n61.2755\n108.793\n83.7065\n41.2808\n35.5088\n25.1303\n10.551\n32.349\n…\n30.8471\n128.882\n2.17932\n102.122\n59.6944\n5.29914\n17.7366\n71.5609\n112.852\n95.7428\n110.588\n34.8399\n84.4864\n29.8537\n8.28717\n19.2349\n78.945\n29.6445\n7.78345\n43.5929\n110.578\n75.1086\n87.5976\n19.9153\n14.1477\n119.088\n91.174\n82.5923\n21.0203\n99.0675\n86.1507\n53.835\n11.75\n17.8838\n83.715\n21.0029\n9.57239\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n2018\n59.8109\n42.4589\n93.8031\n109.915\n214.842\n131.224\n123.0\n110.033\n124.23\n102.987\n53.2737\n100.164\n76.408\n94.622\n98.9289\n119.395\n139.735\n98.9789\n102.187\n122.57\n64.7802\n99.4566\n101.602\n117.319\n129.268\n92.6023\n147.065\n26.2216\n89.0495\n126.681\n133.27\n116.229\n123.098\n74.3995\n40.5014\n91.189\n…\n104.056\n152.074\n67.9775\n151.669\n111.5\n49.5263\n77.3644\n125.48\n133.04\n119.153\n124.086\n89.2004\n155.551\n87.4952\n43.2279\n74.3997\n175.266\n107.442\n115.089\n102.246\n134.143\n125.533\n94.9086\n75.7302\n58.8754\n127.754\n131.363\n103.935\n71.7444\n107.164\n69.55\n146.138\n88.1403\n44.8811\n157.692\n86.0723\n85.8627\n\n\n2019\n59.6471\n45.8065\n91.1635\n114.953\n209.033\n125.303\n124.614\n109.753\n120.776\n106.328\n53.2996\n100.165\n81.3235\n97.1499\n100.4\n116.617\n129.143\n107.812\n112.255\n123.23\n66.0705\n100.149\n97.3747\n117.112\n129.845\n95.3473\n160.662\n32.2627\n90.9589\n126.921\n130.493\n122.67\n132.574\n83.9044\n39.94\n91.0654\n…\n109.342\n159.347\n65.662\n167.089\n111.823\n51.3194\n77.6791\n134.349\n135.756\n120.569\n125.457\n91.0519\n164.617\n87.1297\n45.9379\n73.7225\n181.222\n115.373\n109.555\n58.7722\n145.716\n124.387\n94.6498\n80.5836\n59.0598\n130.629\n133.237\n105.321\n101.26\n92.5178\n63.8474\n140.193\n90.8331\n43.399\n162.739\n93.0148\n86.4094\n\n\n2020\n58.0461\n43.7804\n91.1881\n103.877\n194.468\n121.181\n120.682\n104.93\n120.132\n101.597\n52.5571\n99.9101\n85.2391\n102.973\n106.002\n114.597\n119.412\n117.717\n106.375\n125.165\n67.5517\n99.8988\n98.6456\n104.722\n119.934\n96.7703\n161.859\n36.426\n84.7748\n127.376\n129.414\n120.497\n138.664\n85.2722\n42.5028\n97.6561\n…\n113.638\n150.259\n63.6671\n159.575\n112.169\n53.1133\n80.1173\n146.875\n133.633\n121.331\n123.549\n104.747\n152.545\n79.1482\n50.4343\n75.164\n162.328\n118.819\n103.911\n58.6544\n134.231\n124.042\n95.3961\n84.0051\n62.2824\n129.341\n133.902\n103.855\n99.4064\n96.9438\n58.1626\n141.656\n82.391\n42.0038\n158.447\n100.235\n84.9604\n\n\n2021\n56.6945\n44.3869\n92.4843\n119.688\n186.301\n130.353\n125.395\n105.119\n121.358\n105.694\n59.7008\n101.463\n94.9183\n112.198\n110.012\n114.912\n128.079\n100.257\n114.912\n127.117\n66.7769\n100.809\n104.825\n112.223\n133.597\n100.331\n173.252\n35.817\n87.4056\n123.212\n136.572\n121.492\n150.344\n80.8081\n47.2883\n95.8615\n…\n115.325\n157.978\n62.1565\n176.973\n115.701\n51.2059\n85.2578\n146.654\n135.268\n123.363\n124.961\n118.627\n149.479\n78.5558\n58.0323\n70.4497\n168.485\n116.225\n102.669\n60.9802\n134.222\n129.846\n99.5415\n86.0163\n65.6221\n135.026\n138.113\n106.325\n102.41\n95.5174\n60.2345\n136.806\n81.6914\n47.809\n163.128\n103.283\n90.2538\n\n\n2022\n56.2655\n66.6008\n98.395\n142.426\n195.625\n131.484\n130.54\n109.469\n121.166\n107.506\n56.26\n101.68\n105.745\n119.463\n109.867\n116.679\n139.636\n101.624\n118.944\n128.315\n67.175\n101.572\n101.245\n114.581\n116.108\n95.1298\n178.205\n38.8218\n91.2387\n123.971\n135.114\n124.195\n161.232\n89.8274\n48.6774\n93.6259\n…\n118.15\n173.005\n62.0735\n183.249\n120.27\n49.6801\n86.7785\n149.052\n136.029\n126.459\n140.782\n121.492\n163.43\n78.6568\n65.4378\n72.2148\n176.223\n126.161\n108.158\n61.7258\n133.632\n131.875\n103.721\n93.016\n70.1401\n120.261\n139.829\n108.786\n105.472\n102.334\n66.4668\n137.852\n81.6203\n46.4534\n160.729\n98.4373\n88.9958\n\n\n\n\n\n\nNow, what if we want to go the other way? That is, suppose we stored the data originally in the format above (cell_wide) but want to return to the version we started with. This can be achieved by using the .unpivot() method, which requires specifying the index column to use. The new columns created to store the data are given the default names variable and value. These can be changed by setting the variable_name and value_name arguments.\n\n(\n  cell_wide\n  .unpivot(index=\"year\")\n)\n\n\nshape: (3_480, 3)\n\n\n\nyear\nvariable\nvalue\n\n\ni64\nstr\nf64\n\n\n\n\n2003\n\"AFG\"\n0.87978\n\n\n2004\n\"AFG\"\n2.54662\n\n\n2005\n\"AFG\"\n4.91711\n\n\n2006\n\"AFG\"\n9.9133\n\n\n2007\n\"AFG\"\n18.0167\n\n\n…\n…\n…\n\n\n2018\n\"ZWE\"\n85.8627\n\n\n2019\n\"ZWE\"\n86.4094\n\n\n2020\n\"ZWE\"\n84.9604\n\n\n2021\n\"ZWE\"\n90.2538\n\n\n2022\n\"ZWE\"\n88.9958\n\n\n\n\n\n\n\n\n\n\n\n\nPivot and unpivot syntax\n\n\n\n\n\nThe current version of Polars on Google Colab does not allow using the c. notation when specifying columns as we do with other Polars methods. Instead, you must specify the column as a string, similar to how we do it in plotnine graphics. This has already been changed in the development version of Polars for pivot and will likely change for unpivot in the near future. For now, remember that both pivot and unpivot require a different way of specifying columns compared to other Polars methods.\n\n\n\nThe .pivot() and .unpivot() methods are powerful tools for data manipulation and constitute the final major methods we need for manipulating tabular datasets. Although they may at first glance seem straightforward (and should be, at least in theory), care is needed when using them to reshape data, because they can easily introduce subtle errors. The best practice is to decide on the unit of observation up front and structure your data accordingly. Use these reshaping functions when necessary, but avoid unnecessary back-and-forth transformations that can introduce mistakes.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#expand-list-columns",
    "href": "04_combine.html#expand-list-columns",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.9 Expand List Columns",
    "text": "4.9 Expand List Columns\nA final type of transformation arises when multiple elements occupy a single cell in a table. This usually occurs only after a data-cleaning step we’ve performed, rather than as raw input from a CSV file. For example, the function str.split, which we’ll learn more about in Chapter 7, can split the text in a DataFrame column into parts. We can use this to split the multiple language codes found in the lang column of our data.\n\n(\n    country\n    .select(c.full_name, c.lang)\n    .with_columns(\n        lang_codes = c.lang.str.split(\"|\")\n    )\n)\n\n\nshape: (135, 3)\n\n\n\nfull_name\nlang\nlang_codes\n\n\nstr\nstr\nlist[str]\n\n\n\n\n\"Senegal\"\n\"pbp|fra|wol\"\n[\"pbp\", \"fra\", \"wol\"]\n\n\n\"Venezuela, Bolivarian Republic…\n\"spa|vsl\"\n[\"spa\", \"vsl\"]\n\n\n\"Finland\"\n\"fin|swe\"\n[\"fin\", \"swe\"]\n\n\n\"United States of America\"\n\"eng\"\n[\"eng\"]\n\n\n\"Sri Lanka\"\n\"sin|sin|tam|tam\"\n[\"sin\", \"sin\", … \"tam\"]\n\n\n…\n…\n…\n\n\n\"Albania\"\n\"sqi\"\n[\"sqi\"]\n\n\n\"Malaysia\"\n\"msa\"\n[\"msa\"]\n\n\n\"El Salvador\"\n\"spa\"\n[\"spa\"]\n\n\n\"Cyprus\"\n\"ell|tur\"\n[\"ell\", \"tur\"]\n\n\n\"Pakistan\"\n\"eng|urd\"\n[\"eng\", \"urd\"]\n\n\n\n\n\n\nIt is hard to work directly with the embedding list structure. Often the easiest way to simplify a data frame that contains list elements is to unravel the lists, duplicating the other columns. Use the .explode() method, passing the column(s) you want to unravel.\n\n(\n    country\n    .select(c.full_name, c.lang)\n    .with_columns(\n        lang_codes = c.lang.str.split(\"|\")\n    )\n    .explode(c.lang_codes)\n)\n\n\nshape: (265, 3)\n\n\n\nfull_name\nlang\nlang_codes\n\n\nstr\nstr\nstr\n\n\n\n\n\"Senegal\"\n\"pbp|fra|wol\"\n\"pbp\"\n\n\n\"Senegal\"\n\"pbp|fra|wol\"\n\"fra\"\n\n\n\"Senegal\"\n\"pbp|fra|wol\"\n\"wol\"\n\n\n\"Venezuela, Bolivarian Republic…\n\"spa|vsl\"\n\"spa\"\n\n\n\"Venezuela, Bolivarian Republic…\n\"spa|vsl\"\n\"vsl\"\n\n\n…\n…\n…\n\n\n\"El Salvador\"\n\"spa\"\n\"spa\"\n\n\n\"Cyprus\"\n\"ell|tur\"\n\"ell\"\n\n\n\"Cyprus\"\n\"ell|tur\"\n\"tur\"\n\n\n\"Pakistan\"\n\"eng|urd\"\n\"eng\"\n\n\n\"Pakistan\"\n\"eng|urd\"\n\"urd\"\n\n\n\n\n\n\nThe term “explode” warns us that the resulting dataset can become very large, particularly if we unravel multiple columns simultaneously.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#coming-from-r-or-pandas",
    "href": "04_combine.html#coming-from-r-or-pandas",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.10 Coming from R or Pandas",
    "text": "4.10 Coming from R or Pandas\nIf you are familiar with joins and pivots in R’s dplyr or in Pandas, the code here should be relatively familiar, except for the specific defaults and argument names. What makes these operations difficult are the concepts, not the code, and those do not change across implementations.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#references",
    "href": "04_combine.html#references",
    "title": "4  EDA III: Restructuring Data",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html",
    "href": "05_collect.html",
    "title": "5  EDA IV: Collecting Data",
    "section": "",
    "text": "5.1 Introduction\nIt’s commonly said in data science that most of our time is spent collecting and cleaning data. If we collect data in a clean format from the start, we can proceed directly to the exploration stage.\nThere are a number of excellent articles that give an extensive overview of how to collect and organize data. Hadley Wickham’s “Tidy Data,” one of the most cited papers in data science, offers an extensive theoretical framework for describing a process for collecting datasets [1]. Karl Broman and Kara Woo’s “Data Organization in Spreadsheets” offers a balance between practical advice and an extended discussion of general principles for collecting datasets [2]. Catherine D’Ignazio and Lauren Klein’s Data Feminism lays out important considerations about bias, inequality, and power when collecting and organizing data [3].\nThis chapter provides a concise set of recommendations for organizing and storing data. Rather than an extensive discussion of pros and cons, it focuses on the specific approaches we recommend. For readers seeking broader coverage, see the sources cited above. Because this chapter doesn’t use advanced spreadsheet functions, any spreadsheet program should work. The screenshots are from Microsoft Excel, but the same approach works in Google Sheets, LibreOffice, and other spreadsheet programs.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>EDA IV: Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#what-tables",
    "href": "05_collect.html#what-tables",
    "title": "5  EDA IV: Collecting Data",
    "section": "5.2 What Tables?",
    "text": "5.2 What Tables?\nThe initial step in designing storage for tabular data is deciding what different tables we will need. As we saw in Chapter 4 when discussing pivots and melts, the most important characteristic that determines the structure of a specific table is the unit of observation. Recall that the unit of observation can be a simple value (such as day or plant) or a combination of values (such as day × plant). So, we need to decide which simple and complex units of observation we need for our specific application. There are two general principles that we should keep in mind in this process:\n\nprefer longer to wider: When we have a situation where we need to decide between a longer and wider formats, prefer the longer format. It is easier to extend, easier to check, and there is usually only one long format instead of multiple possible wide ones.\nrespect unit of observation: When filling in data in a table, a general rule is that we only record data that describes the entire unit of observation. This is always the case for a simple unit of observation, but can easily be broken when we have a complex key.\n\nThe second principle means that if we have a table for day × plant and need to store metadata about each plant, we should create a second table whose unit of observation is plant. This avoids copying data each time we observe a plant and prevents data inconsistencies. If we need all the data together for our analysis, it is better to merge the tables once we have collected all the data.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>EDA IV: Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#what-variables",
    "href": "05_collect.html#what-variables",
    "title": "5  EDA IV: Collecting Data",
    "section": "5.3 What Variables?",
    "text": "5.3 What Variables?\nThe second step, after we have the units of observation associated with each table, is to decide which columns each table will contain. As discussed, selecting the columns is straightforward once we know which variables our analysis requires. It’s a good idea to include one or more columns that serve as a primary key when a natural primary key already exists in your dataset.\nIn addition to choosing what to put in each column, we also need to decide how the data in a column will be formatted. Again, we have a few general rules we can apply to help with this:\n\nprefer numeric: If a variable can be stored as a number we should do so. Put other information (such as units of measurement or currency type) in another variable if needed. As much as possible, use a standard unit of measurement for a variable.\nuse standards: When we have character variables, the best option is to use an externally standardized format (such as USPS state codes or ISO country codes). When these do not exist, create and document your own standards as carefully as possible.\natomic cells: Only put a single thing in each variable value. Avoid, for example, putting in multiple values separated by commas. If you will ever use only part of the variable (say, the area code of a phone number) put it in a separate column.\n\nWe will return to ways to document these decisions in a data dictionary later in the chapter.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>EDA IV: Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#what-variable-names",
    "href": "05_collect.html#what-variable-names",
    "title": "5  EDA IV: Collecting Data",
    "section": "5.4 What Variable Names?",
    "text": "5.4 What Variable Names?\nThe final decision we need to make regarding how to store our data is the names we’ll use for the variables. As we’ve seen, variable names in a dataset are used to describe graphics in the grammar of graphics and to manipulate data with Pandas. If names are too complex or hard to remember, creating data visualizations becomes more difficult. Conversely, names that are too cryptic make it hard to recall their meaning. When variable names contain spaces or other special characters, it can be nearly impossible to work in Python without first cleaning them up after loading the data.\nHere are two general rules for naming columns that we should try to follow in our work:\n\nallowed characters: Use only lower-case letters, numbers, and underscores. The underscores can be used to represent a space between words. Also, always make sure to start the name of a variable with a lower-case letter and not a number or underscore.\nsimple names: Do not include extra metadata in the column names, such as units, unless they are needed to distinguish two columns. Prefer short but full names to complex abbreviations.\n\nNote that these guidelines apply only to variable names. We can and should use spaces and capital letters in actual values where appropriate.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>EDA IV: Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#storing-data-in-spreadsheets",
    "href": "05_collect.html#storing-data-in-spreadsheets",
    "title": "5  EDA IV: Collecting Data",
    "section": "5.5 Storing Data in Spreadsheets",
    "text": "5.5 Storing Data in Spreadsheets\nNow that we have an idea of the main decisions about how to organize our data, which we will call the data schema, we will turn to the task of actually collecting our data in a spreadsheet program. As with the other sections, we can summarize the best practices with the following guidelines:\n\nA1: Always start your data collection in the A1 cell. That is, in the upper-left corner of the spreadsheet. The first row contains the column names. The other rows contain our data.\nsheets as tables: Each table should be a separate sheet. Name the sheets with the names you expect to call the data as Python objects.\njust tables: Do not include any additional information in the sheet outside of the tabular format. If you need notes, include them as an extra column or extra table.\nminimal formatting: Some minimal formatting that helps you enter the data can be helpful. For example, making the first row bold, sizing the column widths, using conditional formatting to highlight data errors. However, you should never use formatting to hold any actual data. Use extra columns if you need them.\n\nMost sources on collecting data suggest saving the results from our spreadsheets in a plain-text format. This is a stripped-down representation of the data that contains no formatting information and is application-agnostic. Excel, Google Sheets, LibreOffice, and any other spreadsheet program should be able to save a dataset in a plain-text format. The most commonly used format for tabular data in data science is the comma-separated values (CSV) file. In a CSV, columns are separated by commas and each row is on its own line. The CSV format stores only a single table and therefore is not ideal for storing multiple tables at once.\nOur recommendation is a bit less strict about the need to export data only as a plain-text file. Plain text is the best way to share and store a dataset once an analysis is finished, but if we will continue adding to and changing the dataset, it may be preferable to store the data as an Excel file (.xlsx) or in Google Sheets. As we have seen, the latter (Google Sheets) can be read directly into Python. This also avoids errors introduced when converting back and forth between Excel and plain-text formats. Data can be loaded directly from an Excel file as well using pd.read_excel(). Once the data collection and cleaning are finished, it’s a good idea to store the data in a plain-text format for sharing and long-term preservation.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>EDA IV: Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#data-dictionary",
    "href": "05_collect.html#data-dictionary",
    "title": "5  EDA IV: Collecting Data",
    "section": "5.6 Data Dictionary",
    "text": "5.6 Data Dictionary\nFinally, it is important to document exactly what information is being stored in each table and its variables. It is also important to note any relationships between primary and foreign keys, especially when they have different names. To do this, we can construct a data dictionary. Ideally, this will explain, for each variable, basic information such as the variable’s name, measurement units, and expected values or categories. Any decisions that needed to be made should also be documented. A data dictionary can be a simple text file, or it can be stored as a structured dataset. Often, when storing data in a spreadsheet program, the data dictionary is included as the first or last sheet.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>EDA IV: Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#special-considerations",
    "href": "05_collect.html#special-considerations",
    "title": "5  EDA IV: Collecting Data",
    "section": "5.7 Special Considerations",
    "text": "5.7 Special Considerations\nThe guidelines above summarize the primary issues to consider when designing dataset formats. Here are some additional considerations that arise from time to time:\n\ndates: Dates and times can be particularly error-prone when building datasets. There are two general approaches that we recommend. Either record each element (year, month, day, hour, and so forth) as separate columns or use the standard formats YYYY-MM-DD and YYYY-MM-DD HH:MM:SS, which are recommended by the International Organization for Standardization (ISO 8601).\nmissing values: Always use a blank entry to represent a missing value in a numeric column. This is a good approach for character variables as well, but it is also possible to record one or more special missing value codes depending on your use-case.\ntext and multimedia: When studying large collections of text, images, video, and other multimedia formats, often our tabular dataset contains just metadata about the records. The multimedia files (such as .mp3, .mp4 or .png) files also contain what we should consider to be data. To incorporate these, include the file name (and full path if they are not all in the same directory) as a column in one of the datasets.\nsynthetic keys: Sometimes we need a primary key in a table even though a good natural key does not exist. We also may want to replace a key with multiple columns with a single key. To do this, we can use an algorithm to fill in unique ids, such as a UUID.\n\nWe will see examples of all of these either in the example below or in other chapters of this text.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>EDA IV: Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#worked-example",
    "href": "05_collect.html#worked-example",
    "title": "5  EDA IV: Collecting Data",
    "section": "5.8 Worked Example",
    "text": "5.8 Worked Example\nWe will now walk through an example of how to put these steps into action by storing information about a set of cookbooks and their authors. We’ll create two tables for this: one with the unit of analysis cookbooks and the other authors. Here is what the cookbook table might look like before any data are entered:\n\n\n\n\n\n\nFigure 5.1: Example of a data table before any information other than the column names has been filled in.\n\n\n\nThen we can fill in the values for the table:\n\n\n\n\n\n\nFigure 5.2: Example of a small cookbook dataset, stored in a tidy format in Excel.\n\n\n\nIf we need to include explanatory notes for some data, we avoid abandoning the rectangular data format; instead, we add an extra notes column. For example, we note that one book is out of print in the notes column shown below. In the example table, the number of pages and the weight of that book are missing because it is out of print. To indicate this, the corresponding cells are left blank. Blank values are the only cross-software way to represent missing values consistently.\n\n\n\n\n\n\nFigure 5.3: Example of a small cookbook dataset stored in a tidy format in Excel. Here, we’ve added explanatory notes in a new column.\n\n\n\nNow let’s consider the dataset of cookbook authors. Below is an example of what the table might look like. Note that the authors table’s primary key is referenced by the books table.\n\n\n\n\n\n\nFigure 5.4: A small dataset in tidy format showing properties of cookbook authors in Excel.\n\n\n\nFinally, here is an example of a data dictionary we could use to describe the dataset:\n\n\n\n\n\n\nFigure 5.5: Example of a data dictionary from the Cookbook Authors dataset.\n\n\n\nAbove, we described the data dictionary as a tabular dataset. It is also possible to present it in a less structured way, depending on how much information is required.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>EDA IV: Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#extensions",
    "href": "05_collect.html#extensions",
    "title": "5  EDA IV: Collecting Data",
    "section": "5.9 Extensions",
    "text": "5.9 Extensions\nWe have provided a consolidation of the information from the research papers mentioned in the introduction, focusing on issues of particular concern for humanities data collection. The first places to look for more information are the papers mentioned above: Hadley Wickham’s “Tidy Data” [1], Karl Broman and Kara Woo’s “Data Organization in Spreadsheets” [2], and Catherine D’Ignazio and Lauren Klein’s Data Feminism [3]. Another source for thinking about data dictionaries, data documentation, and data publishing is the “Datasheets for Datasets” paper [4]. While the paper focuses on predictive modeling, it offers useful examples of the documentation needed when publishing large datasets and provides a critical lens on how choices made during data collection fundamentally affect subsequent data analyses.\nThe kinds of data that need to be collected and how to collect them ultimately depend on one’s underlying research questions. The process of translating a research question into a quantitative framework is outside the scope of this text, but it is an important consideration when working with data to address research questions in the humanities. There are many good guides to research design, though the majority focus on either scientific, hypothesis-driven quantitative designs or purely qualitative data collection. We recommend a few sources that sit at the intersection of these approaches.\nContent analysis is a common social-science technique used across fields. It often mixes qualitative questions with quantitative data, making it a good source of research-design advice for humanities data analysis [5] [6] [7]. Similarly, corpus linguistics sits at the boundary of the humanities and the social sciences and offers many resources on best practices [8]. Corpus linguistics often works with both textual and audio data and has developed specific techniques for working with the rich, multimodal datasets considered in the following chapters. Finally, sociology is another field that mixes humanistic questions with quantitative data, providing additional references on research design [9]. Although these references include domain-specific elements, many general principles can be extended to other domains.\nFinally, notice that we did not start the book with data collection, even though collecting data is most often the first step in data analysis. We saved this until the last core chapter because understanding how to collect and store data often goes hand in hand with understanding how to visualize, organize, and restructure it. A deeper understanding of how to collect data—particularly data with complex components such as text, networks, and images—will develop as we work through the remaining chapters.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>EDA IV: Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#references",
    "href": "05_collect.html#references",
    "title": "5  EDA IV: Collecting Data",
    "section": "References",
    "text": "References\n\n\n\n\n[1] Hadley, W (2014 ). Tidy data. Journal of Statistical Software. 59 1–23\n\n\n[2] Broman, K W and Woo, K H (2018 ). Data organization in spreadsheets. The American Statistician. Taylor & Francis. 72 2–10\n\n\n[3] D’ignazio, C and Klein, L F (2020 ). Data Feminism. MIT press\n\n\n[4] Gebru, T, Morgenstern, J, Vecchione, B, Vaughan, J W, Wallach, H, Iii, H D and Crawford, K (2021 ). Datasheets for datasets. Communications of the ACM. ACM New York, NY, USA. 64 86–92\n\n\n[5] Schreier, M (2012 ). Qualitative content analysis in practice. Qualitative content analysis in practice. Sage. 1–280\n\n\n[6] Neuendorf, K A (2017 ). The Content Analysis Guidebook. Sage publications\n\n\n[7] Krippendorff, K (2018 ). Content Analysis: An Introduction to Its Methodology. Sage publications\n\n\n[8] Paquot, M and Gries, S T (2021 ). A Practical Handbook of Corpus Linguistics. Springer Nature\n\n\n[9] Bernard, H R (2017 ). Research Methods in Anthropology: Qualitative and Quantitative Approaches. Rowman & Littlefield",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>EDA IV: Collecting Data</span>"
    ]
  },
  {
    "objectID": "06_program.html",
    "href": "06_program.html",
    "title": "6  Programming",
    "section": "",
    "text": "6.1 Setup\nLoad all of the modules and datasets needed for the chapter.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "06_program.html#introduction",
    "href": "06_program.html#introduction",
    "title": "6  Programming",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nThe majority of the functions and methods that have been introduced throughout the first two parts of this text are provided by user-contributed packages. Most of these come from a core set of packages that together comprise the modern Python data science ecosystem. Included in this set of packages are Polars, numpy, matplotlib, and plotnine. Benefits of using these libraries include consistent APIs, excellent documentation, and the fact that they are often built to express theoretical models for data analysis (for example, relational database techniques encoded in Polars and vectorized operations in numpy). Downsides can include their computational overhead for simple operations and the learning curve required to understand their abstractions.\nThere are various opinions about the best approaches to data science in Python, from pure Polars workflows to functional programming approaches to object-oriented designs. We will avoid a lengthy discussion of these debates here. As should be clear at this point, this text has been written with the opinion that Polars and the broader scientific Python ecosystem provide an excellent way to do data analysis and an ideal starting point for learning data science in Python. However, eventually it will be useful to learn the underlying built-in methods available within the Python programming language itself.\nThe functions and data structures available directly from Python without importing any third-party packages, commonly known as built-in Python or core Python, will become particularly important as we learn how to do more complex programming and data scraping within this part of the book. In this chapter we will restart from the very basics by describing the fundamental data types and objects within Python. These topics will be made easier by the fact that we have seen many of them indirectly in the preceding chapters. We will also provide an overview of introductory computer science concepts such as control flow and function definition. The material is intended for readers who had no prior programming experience.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "06_program.html#core-data-types",
    "href": "06_program.html#core-data-types",
    "title": "6  Programming",
    "section": "6.3 Core Data Types",
    "text": "6.3 Core Data Types\nIn this and following section we will briefly explore the basic building blocks that are part of the Python programming language. These are used internally by every module that we work with in order to build the higher-level functions that we use in our data science work. It’s important to understand these because we often use them as inputs to functions from Python modules.\nTo start, let’s consider what are called scalar types. These are values that store a single piece of information in an object and we have already talked about them in terms of the kinds of objects stored in a single cell in a DataFrame object.\nPython has two common ways of storing numeric data: integers (whole numbers) and floats (numbers with a fractional part). Python will create an integer when there is no decimal point and a float when there is, even if the decimal point is zero. For example, here are two values where the first is created as an integer and the second as float.\n\nval1 = 2\nval2 = 2.0\nval1 + val2\n\n4.0\n\n\nFor the most part we do not need to worry too much about the difference between integers and floats. As we see in the above example, Python can do mathematics with a mixture of integers and floats by converting them as needed. Here, adding an integer and a float produces a float object to preserve the decimal value in the second element.\nAnother common scalar type are Boolean values, which only have two possible values: True and False. We can create these in Python by typing the words True and False without quotes. These objects have special operators for combining two values: the & (and) operator, which returns True only if both elements are True, and the | (or) operator, which returns True if either element is True. Here is an example:\n\nval1 = True\nval2 = False\nprint(f\"`And` operator: {val1 & val2}\")\nprint(f\"`Or` operator: {val1 | val2}\")\n\n`And` operator: False\n`Or` operator: True\n\n\nThere is a special scalar type called NoneType which has only a single value. We can create it in Python by typing None without quotes. It’s often used to indicate that something is missing. For example, if a modelling function has an option for a pre-processing step of the data, the function may expect None as the argument to use when we do not have any pre-processing that we want to run.\nFinally, we have a data type of string for storing a sequence of characters. We frequently use these as arguments in data science work, such as specifying the columns to use in a DataFrame or defining the color to set in a plot. We can create a string by putting a sequence of any characters inside quotes. As with numbers, there are a large number of functions and operators that we can apply to strings depending on our goals.\n\nval1 = \"apple\"\nval2 = \"sauce\"\nval1 + val2\n\n'applesauce'\n\n\nFor all of the data types mentioned here, in addition to creating them directly with specific constants, we can also create them using special functions that try to turn one object type into another: int(), float(), bool(), and str(). Here is a small example showing how this works to change the way that adding 2 and 2 works:\n\nval1 = 2\nval2 = 2\nprint(f\"String addition: {str(val1) + str(val2)}\")\n\nString addition: 22\n\n\nThere are several other scalar types, including complex, bytes, and memoryview. However, these rarely are needed in user-facing code, so its best to focus on the five types mentioned above: integers, floats, Booleans, NoneType, and strings.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "06_program.html#core-collections",
    "href": "06_program.html#core-collections",
    "title": "6  Programming",
    "section": "6.4 Core Collections",
    "text": "6.4 Core Collections\nIn order to build more complex objects, Python has several different kinds of collections that allow us to combine individual scalar values. Collections can additionally contain other collections in a recursive way in order to create rich, complex data structures.\nA list is an ordered sequence of elements. The elements can be of any other type. This arise frequently when passing options to data science functions, such as setting the breaks in a plot. The format for constructing a list directly is to use square brackets, with the elements seperated by commas. Here are examples of lists, one containing only scalar values and another with a list inside of a list:\n\nvar1 = [1, 1, 2, 3, 5, 8, 13]\nvar2 = [1, 2, [0, 1, 2]]\n\nThere are a number of functions, operators, and methods for working with lists. There are also a variety of special methods for selecting subsets of lists and creating new lists. For example, we can add two lists together to create a larger list with the first list and the second list combined with one another. Here is an example:\n\nvar1 + var2\n\n[1, 1, 2, 3, 5, 8, 13, 1, 2, [0, 1, 2]]\n\n\nA tuple is a collection type that is closely related to a list. The main difference between a tuple and list is that a tuple cannot be changed after it is created. For performance reasons, some functions require us to create tuples as inputs. For examples, the .agg method of a DataFrame can be given a tuple of arguments describing how to summarize a data set. A tuple can be created by using parentheses with the elements seperated by commas:\n\nvar1 = (1, 2)\nvar2 = (2, 2, 3)\n\nA set stores and unordered collection of unique values. It is closely related to the mathematical definition of a set. The best way to create a set is to provide a list to the set() function. Here is example showing how the set ignores the ordering of the values and removes duplicate values:\n\nvar1 = set([5, 1, 1, 1, 1, 2])\nvar1\n\n{1, 2, 5}\n\n\nThe final collection type that we will commonly see are dictionaries. These consist of mappings from keys to values. We can create a new dictionary by using a syntax with curly braces, the keys in quotation marks, and the values following a colon:\n\nval1 = {\n    \"first\": \"Taylor\",\n    \"middle\": \"B.\",\n    \"last\": \"Arnold\"\n}\nval1\n\n{'first': 'Taylor', 'middle': 'B.', 'last': 'Arnold'}\n\n\nWe can select or change a value in a dictionary by putting square brackets after the name of the dictionary and using the key name inside of the brackets. For example, here is how to get the last name from the dictionary we made above:\n\nval1[\"last\"]\n\n'Arnold'\n\n\nIt is important to be able to recognize all four of the collection types described in this section because they come up as arguments to functions and methods. In our own code, we will see that lists and dictionaries are by far the most commonly used in data science work.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "06_program.html#conditional-statements",
    "href": "06_program.html#conditional-statements",
    "title": "6  Programming",
    "section": "6.5 Conditional Statements",
    "text": "6.5 Conditional Statements\nNow that we have the basic building blocks of Python, we can turn to how to use them to write code that makes decisions. A conditional statement allows us to run different blocks of code depending on whether a condition is true or false. The simplest form uses the if keyword followed by a condition that evaluates to a Boolean value.\n\nx = 10\nif x &gt; 5:\n    print(\"x is greater than 5\")\n\nx is greater than 5\n\n\nNotice the structure: the if keyword is followed by a condition (x &gt; 5), then a colon. The code that runs when the condition is true is indented on the following lines. This indentation is not optional in Python; it defines which code belongs to the conditional block.\nWe can extend this with else to specify what happens when the condition is false:\n\nx = 3\nif x &gt; 5:\n    print(\"x is greater than 5\")\nelse:\n    print(\"x is not greater than 5\")\n\nx is not greater than 5\n\n\nFor more than two possibilities, we use elif (short for “else if”) to check additional conditions:\n\nx = 5\nif x &gt; 5:\n    print(\"x is greater than 5\")\nelif x == 5:\n    print(\"x is exactly 5\")\nelse:\n    print(\"x is less than 5\")\n\nx is exactly 5\n\n\nPython evaluates these conditions from top to bottom and runs only the first block whose condition is true. The else block at the end catches anything not matched by the earlier conditions.\nConditions can be combined using and and or. The and operator requires both conditions to be true, while or requires at least one:\n\nx = 7\nif x &gt; 5 and x &lt; 10:\n    print(\"x is between 5 and 10\")\n\nx is between 5 and 10\n\n\nThe not operator inverts a Boolean value:\n\nx = 3\nif not x &gt; 5:\n    print(\"x is not greater than 5\")\n\nx is not greater than 5\n\n\nConditional statements become particularly useful when combined with loops and functions, which we explore in the following sections.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "06_program.html#loops",
    "href": "06_program.html#loops",
    "title": "6  Programming",
    "section": "6.6 Loops",
    "text": "6.6 Loops\nA loop allows us to repeat a block of code multiple times. This is essential when we need to perform the same operation on many items, such as processing each file in a directory or each row in a dataset.\nThe most common type of loop in Python is the for loop, which iterates over a sequence of values. Here is a simple example that prints each item in a list:\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor fruit in fruits:\n    print(fruit)\n\napple\nbanana\ncherry\n\n\nThe structure mirrors what we saw with conditionals: the for keyword, followed by a variable name (fruit), the keyword in, the sequence to iterate over, and a colon. The indented code runs once for each item in the sequence, with the variable taking on each value in turn.\nWe can loop over many types of sequences. The range() function is particularly useful for generating sequences of numbers:\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\nNotice that range(5) produces numbers from 0 to 4, not 1 to 5. This zero-indexing is consistent throughout Python. We can also specify a starting point and step size:\n\nfor i in range(2, 10, 2):\n    print(i)\n\n2\n4\n6\n8\n\n\nOften we need both the index of an item and the item itself. The enumerate() function provides both:\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor i, fruit in enumerate(fruits):\n    print(f\"Item {i}: {fruit}\")\n\nItem 0: apple\nItem 1: banana\nItem 2: cherry\n\n\nThe syntax for i, fruit in enumerate(fruits) unpacks two values on each iteration: the index and the item. This is cleaner than manually tracking an index variable.\nA common pattern is to build up a result by appending to a list inside a loop:\n\nnumbers = [1, 2, 3, 4, 5]\nsquares = []\nfor n in numbers:\n    squares.append(n**2)\n\nsquares\n\n[1, 4, 9, 16, 25]\n\n\nThe .append() method adds a single element to the end of a list. We start with an empty list and add to it on each iteration.\nWhen working with Polars DataFrames, we sometimes need to iterate over rows. The .iter_rows() method provides this capability:\n\nimport polars as pl\n\ndf = pl.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Carol\"],\n    \"age\": [25, 30, 35]\n})\n\nfor row in df.iter_rows(named=True):\n    print(f\"{row['name']} is {row['age']} years old\")\n\nAlice is 25 years old\nBob is 30 years old\nCarol is 35 years old\n\n\nSetting named=True returns each row as a dictionary, making it easy to access values by column name. Without this option, rows are returned as tuples.\nWe can also enumerate rows when we need the index:\n\nfor i, row in enumerate(df.iter_rows(named=True)):\n    print(f\"Row {i}: {row['name']}\")\n\nRow 0: Alice\nRow 1: Bob\nRow 2: Carol\n\n\nThat said, iterating over DataFrame rows should generally be a last resort. Polars is optimized for operations that work on entire columns at once. When possible, use Polars expressions rather than row-by-row iteration.\nPython also has while loops, which continue as long as a condition remains true:\n\ncount = 0\nwhile count &lt; 3:\n    print(count)\n    count = count + 1\n\n0\n1\n2\n\n\nWhile loops are useful when we don’t know in advance how many iterations we need. However, they require care to avoid infinite loops (forgetting to update the condition so it eventually becomes false).",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "06_program.html#list-comprehensions",
    "href": "06_program.html#list-comprehensions",
    "title": "6  Programming",
    "section": "6.7 List Comprehensions",
    "text": "6.7 List Comprehensions\nList comprehensions provide a concise way to create lists. They combine the loop and the list-building pattern into a single expression. Compare this traditional loop:\n\nnumbers = [1, 2, 3, 4, 5]\nsquares = []\nfor n in numbers:\n    squares.append(n**2)\n\nsquares\n\n[1, 4, 9, 16, 25]\n\n\nWith the equivalent list comprehension:\n\nnumbers = [1, 2, 3, 4, 5]\nsquares = [n**2 for n in numbers]\nsquares\n\n[1, 4, 9, 16, 25]\n\n\nThe list comprehension puts the expression (n**2) before the for clause, all enclosed in square brackets. This reads almost like English: “n squared for each n in numbers.”\nWe can add a condition to filter which items are included:\n\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\neven_squares = [n**2 for n in numbers if n % 2 == 0]\neven_squares\n\n[4, 16, 36, 64, 100]\n\n\nThe if clause at the end filters the input: only numbers where n % 2 == 0 (even numbers) are processed.\nList comprehensions can also include an else clause, though the syntax changes. When we want to transform all items but differently based on a condition, we put the conditional expression before the for:\n\nnumbers = [1, 2, 3, 4, 5]\nlabels = [\"even\" if n % 2 == 0 else \"odd\" for n in numbers]\nlabels\n\n['odd', 'even', 'odd', 'even', 'odd']\n\n\nSimilar syntax works for creating dictionaries:\n\nnumbers = [1, 2, 3, 4, 5]\nsquare_dict = {n: n**2 for n in numbers}\nsquare_dict\n\n{1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n\n\nThe curly braces and key: value syntax indicate we’re building a dictionary rather than a list.\nList comprehensions are more than just a shortcut. They often run faster than equivalent loops and make code more readable once you’re familiar with the pattern. However, for complex logic, a traditional loop may be clearer.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "06_program.html#working-with-files",
    "href": "06_program.html#working-with-files",
    "title": "6  Programming",
    "section": "6.8 Working with Files",
    "text": "6.8 Working with Files\nWhen doing data science work, we frequently need to work with files: reading data, saving results, and processing multiple files in a directory. Python’s pathlib module provides an elegant, object-oriented approach to file system operations. A Path object represents a location in the file system. We create one by passing a string:\n\ndata_path = Path(\"data\")\ndata_path\n\nPosixPath('data')\n\n\nPath objects understand the structure of file paths. We can access different parts:\n\nfile_path = Path(\"data/countries.csv\")\nprint(f\"Name: {file_path.name}\")\nprint(f\"Stem (name without extension): {file_path.stem}\")\nprint(f\"Suffix (extension): {file_path.suffix}\")\nprint(f\"Parent directory: {file_path.parent}\")\n\nName: countries.csv\nStem (name without extension): countries\nSuffix (extension): .csv\nParent directory: data\n\n\nOne of pathlib’s most useful features is the / operator for joining paths:\n\nbase = Path(\"data\")\nfull_path = base / \"countries.csv\"\nfull_path\n\nPosixPath('data/countries.csv')\n\n\nThis is cleaner than string concatenation and handles differences between operating systems (forward slashes on Mac/Linux, backslashes on Windows) automatically.\nBefore working with files, we often need to check whether they exist:\n\ndata_dir = Path(\"data\")\nprint(f\"Directory exists: {data_dir.exists()}\")\nprint(f\"Is a directory: {data_dir.is_dir()}\")\nprint(f\"Is a file: {data_dir.is_file()}\")\n\nDirectory exists: True\nIs a directory: True\nIs a file: False\n\n\nThe .iterdir() method returns all items in a directory:\n\ndata_dir = Path(\"data\")\nfor item in data_dir.iterdir():\n    print(item.name)\n\nfrance_departement_sml.geojson\nstorm_gender.csv\nparis_metro_stops.csv\nfrance_departement_gdp.csv\nmovies_50_years_genre.csv\nwiki_uk_page_views.csv\nmovies_50_years_color.csv\nwiki_uk_meta.csv.gz\nemed_words.csv\nflightsrva_airlines.csv.gz\nacs_state.geojson\n.DS_Store\nus_city_population.csv\nacs_cbsa.csv\ncountries.csv\nagnews_pca.parquet\nflightsrva_airports.csv.gz\nimagewoof_1000.parquet\nbirds_1000.csv\nsst5_pca.parquet\nfood.csv\nwiki_uk_page_revisions.csv\nfrance_cities.csv\nfrance_departement_population.csv\nwiki_uk_citations.csv\nacs_state.csv\nshakespeare_words.csv.gz\nacs_cbsa_to_state.csv\nwweia_food.csv\namazon_pca.parquet\nmovies_50_years.csv\nacs_cbsa_geo.geojson\nkeylog-meta.csv.gz\nemed_plays.csv\nacs_cbsa_commute_type.csv\nstorms.csv\nacs_cbsa_commute_time.csv\nacs_cbsa_hh_income.csv\nmovies_50_years_people.csv\nflightsrva_flights.csv.gz\nemed_lines.csv\nwiki_uk_authors_anno_fr.csv.gz\nwiki_uk_authors_anno.csv.gz\nmnist_1000.csv\nimagenette_1000.parquet\nflightsrva_weather.csv.gz\nkeylog.csv.gz\nstorm_codes.csv\ninference_speed_sex_height.csv\nwweia_meta.csv\nwweia_demo.csv\nwiki_uk_authors_text_fr.csv\ninference_sulphinpyrazone.csv\nimdb5k_pca.parquet\nbbc_pca.parquet\nemed_characters.csv\ninference_age_at_mar.csv\ninference_absenteeism.csv\nwiki_uk_authors_text.csv\nwiki_uk_cocitations.csv\ncriterion.csv\nflightsrva_planes.csv.gz\nfrance_departement_covid.csv\nfsac.csv\nimdb_pca.parquet\ncountries_borders.csv\nit_cities.csv\nmajors.csv\ncountries_cellphone.csv\nshakespeare_characters.csv\ngoemotions_pca.parquet\nshakespeare_plays.csv\ninference_possum.csv\nflowers_1000.parquet\nbirds10.parquet\nfashionmnist_10000.csv\nemnist_10000.csv\nshakespeare_lines.csv.gz\nfood_diet_restrictions.csv\nfood_recipes.csv\nit_province_covid.csv\nit_province.geojson\n\n\nTo find files matching a pattern, use .glob(). This is particularly powerful for finding all files of a certain type:\n\ncsv_files = list(data_dir.glob(\".csv\"))\nprint(f\"Found {len(csv_files)} CSV files\")\n\nFound 0 CSV files\n\n\nThe is a wildcard that matches any sequence of characters. So `.csv` matches any file ending in `.csv`. For recursive searching through subdirectories, use:\n\n# Find all CSV files in data/ and any subdirectories\nall_csvs = list(data_dir.glob(\"/.csv\"))\n\nPathlib provides simple methods for reading and writing text files. To read the contents of a file:\n\nfile_path = Path(\"data/sample.txt\")\ncontent = file_path.read_text()\n\nTo write content to a file:\n\noutput_path = Path(\"examples/output.txt\")\noutput_path.write_text(\"Hello, world!\\nThis is line 2.\")\n\n29\n\n\nFor line-by-line processing, we can split the text:\n\ncontent = output_path.read_text()\nlines = content.splitlines()\nfor line in lines:\n    print(f\"Line: {line}\")\n\nLine: Hello, world!\nLine: This is line 2.\n\n\nTo write multiple lines, join them with newline characters:\n\nlines = [\"First line\", \"Second line\", \"Third line\"]\noutput_path.write_text(\"\\n\".join(lines))\n\n33\n\n\nTo create a new directory:\n\nnew_dir = Path(\"results\")\nnew_dir.mkdir(exist_ok=True)\n\nThe exist_ok=True argument prevents an error if the directory already exists. To create nested directories, add parents=True:\n\nnested = Path(\"results/experiment1/plots\")\nnested.mkdir(parents=True, exist_ok=True)\n\nLet’s put these pieces together with a practical example. Suppose we have a directory of image files and want to build a DataFrame containing information about each file. We’ll use the media/birds10 directory, which contains PNG images of birds.\n\nfrom PIL import Image\n\nimage_dir = Path(\"media/birds10\")\n\nFirst, we find all PNG files in the directory:\n\npng_files = list(image_dir.glob(\".png\"))\nprint(f\"Found {len(png_files)} PNG files\")\n\nFound 0 PNG files\n\n\nNow we iterate through the files, loading each image to get its dimensions:\n\nfile_info = []\nfor file_path in png_files:\n    img = Image.open(file_path)\n    width, height = img.size\n    file_info.append({\n        \"filename\": file_path.name,\n        \"width\": width,\n        \"height\": height\n    })\n\nFinally, we convert this list of dictionaries into a Polars DataFrame:\n\nimage_df = pl.DataFrame(file_info)\nimage_df\n\n\nshape: (0, 0)\n\n\n\n\n\n\n\nThis pattern of listing files, processing each one in a loop, collecting results, and building a DataFrame is extremely common in data science workflows. The same approach works for any type of file: CSVs, JSON files, text documents, or any other data format.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "06_program.html#numpy-arrays",
    "href": "06_program.html#numpy-arrays",
    "title": "6  Programming",
    "section": "6.9 NumPy Arrays",
    "text": "6.9 NumPy Arrays\nWhile Polars handles tabular data beautifully, many scientific computing and machine learning tasks require a different data structure: the NumPy array. NumPy (Numerical Python) provides efficient storage and operations for homogeneous numerical data, particularly matrices and higher-dimensional arrays.\n\nimport numpy as np\n\nThe simplest way to create a NumPy array is from a Python list:\n\narr = np.array([1, 2, 3, 4, 5])\narr\n\narray([1, 2, 3, 4, 5])\n\n\nUnlike Python lists, NumPy arrays require all elements to be the same type. NumPy will convert elements to a common type if needed:\n\nmixed = np.array([1, 2.5, 3])\nprint(f\"Array: {mixed}\")\nprint(f\"Data type: {mixed.dtype}\")\n\nArray: [1.  2.5 3. ]\nData type: float64\n\n\nHere, the integer 1 and 3 were converted to floats to match 2.5.\nTwo-dimensional arrays (matrices) are created from nested lists:\n\nmatrix = np.array([[1, 2, 3], [4, 5, 6]])\nmatrix\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\nNumPy provides convenient functions for creating common arrays:\n\nzeros = np.zeros((3, 4))      # 3x4 array of zeros\nones = np.ones((2, 3))        # 2x3 array of ones\nseq = np.arange(0, 10, 2)     # sequence from 0 to 10 by 2\nlinear = np.linspace(0, 1, 5) # 5 evenly spaced values from 0 to 1\n\nprint(f\"Zeros:\\n{zeros}\\n\")\nprint(f\"Sequence: {seq}\")\nprint(f\"Linear space: {linear}\")\n\nZeros:\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n\nSequence: [0 2 4 6 8]\nLinear space: [0.   0.25 0.5  0.75 1.  ]\n\n\nArrays have attributes describing their structure:\n\nmatrix = np.array([[1, 2, 3], [4, 5, 6]])\nprint(f\"Shape: {matrix.shape}\")     # (rows, columns)\nprint(f\"Number of dimensions: {matrix.ndim}\")\nprint(f\"Total elements: {matrix.size}\")\nprint(f\"Data type: {matrix.dtype}\")\n\nShape: (2, 3)\nNumber of dimensions: 2\nTotal elements: 6\nData type: int64\n\n\nNumPy arrays support Python’s indexing syntax, extended to multiple dimensions:\n\narr = np.array([10, 20, 30, 40, 50])\nprint(f\"First element: {arr[0]}\")\nprint(f\"Last element: {arr[-1]}\")\nprint(f\"Elements 1-3: {arr[1:4]}\")\n\nFirst element: 10\nLast element: 50\nElements 1-3: [20 30 40]\n\n\nFor two-dimensional arrays, we specify row and column indices separated by a comma:\n\nmatrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(f\"Element at row 0, col 2: {matrix[0, 2]}\")\nprint(f\"First row: {matrix[0, :]}\")\nprint(f\"First column: {matrix[:, 0]}\")\nprint(f\"Submatrix:\\n{matrix[0:2, 1:3]}\")\n\nElement at row 0, col 2: 3\nFirst row: [1 2 3]\nFirst column: [1 4 7]\nSubmatrix:\n[[2 3]\n [5 6]]\n\n\nThe colon : by itself means “all elements along this dimension.”\nThe real power of NumPy comes from vectorized operations, which apply to entire arrays at once without explicit loops:\n\narr = np.array([1, 2, 3, 4, 5])\nprint(f\"Add 10: {arr + 10}\")\nprint(f\"Multiply by 2: {arr * 2}\")\nprint(f\"Square: {arr**2}\")\n\nAdd 10: [11 12 13 14 15]\nMultiply by 2: [ 2  4  6  8 10]\nSquare: [ 1  4  9 16 25]\n\n\nOperations between arrays of the same shape work element-wise:\n\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nprint(f\"Sum: {a + b}\")\nprint(f\"Product: {a * b}\")\n\nSum: [5 7 9]\nProduct: [ 4 10 18]\n\n\nNumPy includes many mathematical functions that operate on entire arrays:\n\narr = np.array([1, 4, 9, 16, 25])\nprint(f\"Square root: {np.sqrt(arr)}\")\nprint(f\"Sum: {np.sum(arr)}\")\nprint(f\"Mean: {np.mean(arr)}\")\nprint(f\"Standard deviation: {np.std(arr)}\")\n\nSquare root: [1. 2. 3. 4. 5.]\nSum: 55\nMean: 11.0\nStandard deviation: 8.648699324175862\n\n\nVectorized operations are not just convenient; they are dramatically faster than equivalent Python loops. NumPy achieves this by implementing operations in optimized C code and processing data in contiguous memory blocks. For large arrays, the speed difference can be a factor of 100 or more.\nMost scientific Python libraries, including machine learning frameworks like scikit-learn and PyTorch, use NumPy arrays as their primary data exchange format. Images are typically represented as NumPy arrays (height × width × color channels). Time series, audio signals, and neural network weights all use NumPy arrays. Understanding the basics of NumPy is therefore essential for working with these tools.\nPolars and NumPy interoperate smoothly. To convert a Polars Series to a NumPy array:\n\nimport polars as pl\n\ndf = pl.DataFrame({\"values\": [1, 2, 3, 4, 5]})\narr = df[\"values\"].to_numpy()\narr\n\narray([1, 2, 3, 4, 5])\n\n\nTo convert a NumPy array to a Polars Series:\n\narr = np.array([10, 20, 30])\nseries = pl.Series(\"numbers\", arr)\nseries\n\n\nshape: (3,)\n\n\n\nnumbers\n\n\ni64\n\n\n\n\n10\n\n\n20\n\n\n30\n\n\n\n\n\n\nThis interoperability means we can use whichever tool is most appropriate for each task: Polars for data manipulation and aggregation, NumPy for numerical computation and interfacing with machine learning libraries.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "06_program.html#defining-functions",
    "href": "06_program.html#defining-functions",
    "title": "6  Programming",
    "section": "6.10 Defining Functions",
    "text": "6.10 Defining Functions\nAs our code becomes more complex, we benefit from organizing it into reusable pieces. A function packages a block of code that we can call by name, optionally passing in values and getting results back.\nWe define a function using the def keyword:\n\ndef greet(name):\n    return f\"Hello, {name}!\"\n\ngreet(\"World\")\n\n'Hello, World!'\n\n\nThe function definition has several parts: def indicates we’re defining a function, greet is the function’s name, name in parentheses is a parameter (a variable that will receive the input), and the indented block is the function’s body. The return statement specifies what value the function produces.\nFunctions can have multiple parameters:\n\ndef add_numbers(a, b):\n    return a + b\n\nadd_numbers(3, 5)\n\n8\n\n\nWe can provide default values for parameters, making them optional:\n\ndef greet(name, greeting=\"Hello\"):\n    return f\"{greeting}, {name}!\"\n\nprint(greet(\"World\"))\nprint(greet(\"World\", \"Hi\"))\n\nHello, World!\nHi, World!\n\n\nWhen calling a function, we can specify arguments by name for clarity:\n\ngreet(name=\"World\", greeting=\"Welcome\")\n\n'Welcome, World!'\n\n\nNot all functions need to return a value. Some perform actions like printing output or modifying files:\n\ndef print_summary(values):\n    print(f\"Count: {len(values)}\")\n    print(f\"Sum: {sum(values)}\")\n    print(f\"Mean: {sum(values) / len(values)}\")\n\nprint_summary([1, 2, 3, 4, 5])\n\nCount: 5\nSum: 15\nMean: 3.0\n\n\nIf a function doesn’t have a return statement, it implicitly returns None.\nFunctions are particularly useful for tasks we perform repeatedly. Consider our earlier example of processing image files. We could package that logic into a function:\n\ndef get_image_info(file_path):\n    \"\"\"Return a dictionary with image file information.\"\"\"\n    img = Image.open(file_path)\n    width, height = img.size\n    return {\n        \"filename\": file_path.name,\n        \"width\": width,\n        \"height\": height\n    }\n\nThe triple-quoted text after the function definition is a docstring, which documents what the function does. Now we can use this function in a list comprehension:\n\nimage_dir = Path(\"media/birds10\")\npng_files = list(image_dir.glob(\".png\"))\nfile_info = [get_image_info(f) for f in png_files]\nimage_df = pl.DataFrame(file_info)\n\nThis is more readable than the loop version because the function name documents the purpose of the code.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "06_program.html#putting-it-together",
    "href": "06_program.html#putting-it-together",
    "title": "6  Programming",
    "section": "6.11 Putting It Together",
    "text": "6.11 Putting It Together\nLet’s conclude with a more complete example that combines several concepts from this chapter. We’ll write a function that processes a directory of CSV files, reads each one, adds a column indicating the source file, and combines them into a single DataFrame.\n\ndef combine_csv_files(directory):\n    \"\"\"\n    Read all CSV files in a directory and combine them into one DataFrame.\n    Adds a 'source_file' column with the original filename.\n    \"\"\"\n    dir_path = Path(directory)\n    csv_files = list(dir_path.glob(\".csv\"))\n    \n    if len(csv_files) == 0:\n        print(f\"No CSV files found in {directory}\")\n        return None\n    \n    dataframes = []\n    for file_path in csv_files:\n        df = pl.read_csv(file_path)\n        df = df.with_columns(pl.lit(file_path.name).alias(\"source_file\"))\n        dataframes.append(df)\n    \n    combined = pl.concat(dataframes)\n    return combined\n\nThis function demonstrates several key concepts: using pathlib to work with directories and files, looping with a for loop, building a list with append, using conditional statements to handle edge cases, and defining a reusable function with a clear purpose.\nThe same pattern of combining multiple files appears constantly in data science work. Raw data often arrives as multiple files (one per day, one per region, one per experiment), and our first task is to combine them into a single dataset for analysis.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "06_program.html#references",
    "href": "06_program.html#references",
    "title": "6  Programming",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "07_strings.html",
    "href": "07_strings.html",
    "title": "7  Strings",
    "section": "",
    "text": "7.1 Setup\nLoad all of the modules and datasets needed for the chapter.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nwiki = pl.read_csv(\"data/wiki_uk_meta.csv.gz\", ignore_errors=True)",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "07_strings.html#introduction",
    "href": "07_strings.html#introduction",
    "title": "7  Strings",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction\nText data appears everywhere in data science: names, addresses, product descriptions, social media posts, and scraped web content. Working effectively with strings requires learning a small set of operations that allow us to search, extract, and transform text within a DataFrame. In this chapter, we explore the string methods available in Polars, which provide a consistent and efficient way to manipulate text columns. We will also introduce regular expressions, a powerful pattern-matching language that underlies many of these operations.\nTo illustrate these methods, we use a dataset of British writers compiled from Wikipedia. The dataset contains metadata about 75 authors spanning from the medieval period to the present day.\n\nwiki.glimpse()\n\nRows: 75\nColumns: 7\n$ doc_id &lt;str&gt; 'Marie de France', 'Geoffrey Chaucer', 'John Gower', 'William Langland', 'Margery Kempe', 'Thomas Malory', 'Thomas More', 'Edmund Spenser', 'Walter Raleigh', 'Philip Sidney'\n$ born   &lt;i64&gt; 1160, 1343, 1330, 1332, 1373, 1405, 1478, 1552, 1552, 1554\n$ died   &lt;i64&gt; 1215, 1400, 1408, 1386, 1438, 1471, 1535, 1599, 1618, 1586\n$ era    &lt;str&gt; 'Early', 'Early', 'Early', 'Early', 'Early', 'Early', 'Sixteenth C', 'Sixteenth C', 'Sixteenth C', 'Sixteenth C'\n$ gender &lt;str&gt; 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male'\n$ link   &lt;str&gt; 'Marie_de_France', 'Geoffrey_Chaucer', 'John_Gower', 'William_Langland', 'Margery_Kempe', 'Thomas_Malory', 'Thomas_More', 'Edmund_Spenser', 'Walter_Raleigh', 'Philip_Sidney'\n$ short  &lt;str&gt; 'Marie d. F.', 'Chaucer', 'Gower', 'Langland', 'Kempe', 'Malory', 'More', 'Spenser', 'Raleigh', 'Sidney'\n\n\n\nThe doc_id column contains each author’s full name, while short provides a shortened version. The link column gives the Wikipedia URL suffix for each author’s page. We also have birth and death years, a categorical era column, and gender. This mix of structured and semi-structured text gives us plenty of opportunities to practice string manipulation.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "07_strings.html#filtering-with-contains",
    "href": "07_strings.html#filtering-with-contains",
    "title": "7  Strings",
    "section": "7.3 Filtering with Contains",
    "text": "7.3 Filtering with Contains\nA common task is selecting rows where a text column contains a particular pattern. The contains method checks whether each value in a string column matches a given pattern. Let’s find all authors whose names include “William”:\n\n(\n    wiki\n    .filter(c.doc_id.str.contains(\"William\"))\n)\n\n\nshape: (4, 7)\n\n\n\ndoc_id\nborn\ndied\nera\ngender\nlink\nshort\n\n\nstr\ni64\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n\"William Langland\"\n1332\n1386\n\"Early\"\n\"male\"\n\"William_Langland\"\n\"Langland\"\n\n\n\"William Shakespeare\"\n1564\n1616\n\"Sixteenth C\"\n\"male\"\n\"William_Shakespeare\"\n\"Shakespeare\"\n\n\n\"William Blake\"\n1757\n1827\n\"Romantic\"\n\"male\"\n\"William_Blake\"\n\"Blake\"\n\n\n\"William Wordsworth\"\n1770\n1850\n\"Romantic\"\n\"male\"\n\"William_Wordsworth\"\n\"Wordsworth\"\n\n\n\n\n\n\nThe pattern matching is case-sensitive. To find authors from the sixteenth century, we filter on the era column:\n\n(\n    wiki\n    .filter(c.era.str.contains(\"Sixteenth\"))\n)\n\n\nshape: (6, 7)\n\n\n\ndoc_id\nborn\ndied\nera\ngender\nlink\nshort\n\n\nstr\ni64\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n\"Thomas More\"\n1478\n1535\n\"Sixteenth C\"\n\"male\"\n\"Thomas_More\"\n\"More\"\n\n\n\"Edmund Spenser\"\n1552\n1599\n\"Sixteenth C\"\n\"male\"\n\"Edmund_Spenser\"\n\"Spenser\"\n\n\n\"Walter Raleigh\"\n1552\n1618\n\"Sixteenth C\"\n\"male\"\n\"Walter_Raleigh\"\n\"Raleigh\"\n\n\n\"Philip Sidney\"\n1554\n1586\n\"Sixteenth C\"\n\"male\"\n\"Philip_Sidney\"\n\"Sidney\"\n\n\n\"Christopher Marlowe\"\n1564\n1593\n\"Sixteenth C\"\n\"male\"\n\"Christopher_Marlowe\"\n\"Marlowe\"\n\n\n\"William Shakespeare\"\n1564\n1616\n\"Sixteenth C\"\n\"male\"\n\"William_Shakespeare\"\n\"Shakespeare\"\n\n\n\n\n\n\nWe can combine string filters with other conditions using the standard logical operators. Below we find female authors born before 1800:\n\n(\n    wiki\n    .filter(\n        (c.gender == \"female\") & (c.born &lt; 1800)\n    )\n)\n\n\nshape: (11, 7)\n\n\n\ndoc_id\nborn\ndied\nera\ngender\nlink\nshort\n\n\nstr\ni64\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n\"Marie de France\"\n1160\n1215\n\"Early\"\n\"female\"\n\"Marie_de_France\"\n\"Marie d. F.\"\n\n\n\"Margery Kempe\"\n1373\n1438\n\"Early\"\n\"female\"\n\"Margery_Kempe\"\n\"Kempe\"\n\n\n\"Emilia Lanier\"\n1569\n1645\n\"Seventeenth C\"\n\"female\"\n\"Emilia_Lanier\"\n\"Lanier\"\n\n\n\"Katherine Philipps\"\n1632\n1664\n\"Seventeenth C\"\n\"female\"\n\"Katherine_Philipps\"\n\"Philipps\"\n\n\n\"Margaret Cavendish\"\n1623\n1673\n\"Seventeenth C\"\n\"female\"\n\"Margaret_Cavendish\"\n\"Cavendish\"\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Mary Robinson\"\n1757\n1800\n\"Romantic\"\n\"female\"\n\"Mary_Robinson_(poet)\"\n\"Robinson\"\n\n\n\"Mary Wollstonecraft\"\n1759\n1797\n\"Romantic\"\n\"female\"\n\"Mary_Wollstonecraft\"\n\"Wollstonecraft\"\n\n\n\"Ann Radcliffe\"\n1764\n1823\n\"Romantic\"\n\"female\"\n\"Ann_Radcliffe\"\n\"Radcliffe\"\n\n\n\"Jane Austen\"\n1775\n1817\n\"Romantic\"\n\"female\"\n\"Jane_Austen\"\n\"Austen\"\n\n\n\"Felicia Hemans\"\n1793\n1835\n\"Romantic\"\n\"female\"\n\"Felicia_Hemans\"\n\"Hemans\"",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "07_strings.html#regular-expression-basics",
    "href": "07_strings.html#regular-expression-basics",
    "title": "7  Strings",
    "section": "7.4 Regular Expression Basics",
    "text": "7.4 Regular Expression Basics\nSo far we have searched for literal text, but the contains method (and most other string methods in Polars) actually accepts regular expressions by default. Regular expressions are patterns that describe sets of strings. They give us far more flexibility than searching for exact text.\nLet’s start with two simple but powerful patterns: ^ matches the start of a string, and $ matches the end. To find all authors whose names start with the letter “M”:\n\n(\n    wiki\n    .filter(c.doc_id.str.contains(r\"^M\"))\n)\n\n\nshape: (6, 7)\n\n\n\ndoc_id\nborn\ndied\nera\ngender\nlink\nshort\n\n\nstr\ni64\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n\"Marie de France\"\n1160\n1215\n\"Early\"\n\"female\"\n\"Marie_de_France\"\n\"Marie d. F.\"\n\n\n\"Margery Kempe\"\n1373\n1438\n\"Early\"\n\"female\"\n\"Margery_Kempe\"\n\"Kempe\"\n\n\n\"Margaret Cavendish\"\n1623\n1673\n\"Seventeenth C\"\n\"female\"\n\"Margaret_Cavendish\"\n\"Cavendish\"\n\n\n\"Mary Robinson\"\n1757\n1800\n\"Romantic\"\n\"female\"\n\"Mary_Robinson_(poet)\"\n\"Robinson\"\n\n\n\"Mary Wollstonecraft\"\n1759\n1797\n\"Romantic\"\n\"female\"\n\"Mary_Wollstonecraft\"\n\"Wollstonecraft\"\n\n\n\"Matthew Arnold\"\n1822\n1888\n\"Victorian\"\n\"male\"\n\"Matthew_Arnold\"\n\"Arnold\"\n\n\n\n\n\n\nThe r before the string creates a raw string literal, which is a Python convention for regular expressions. It prevents Python from treating backslashes as escape characters, which becomes important with more complex patterns.\nTo find names that end with the letter “e”:\n\n(\n    wiki\n    .filter(c.doc_id.str.contains(r\"e$\"))\n)\n\n\nshape: (17, 7)\n\n\n\ndoc_id\nborn\ndied\nera\ngender\nlink\nshort\n\n\nstr\ni64\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n\"Marie de France\"\n1160\n1215\n\"Early\"\n\"female\"\n\"Marie_de_France\"\n\"Marie d. F.\"\n\n\n\"Margery Kempe\"\n1373\n1438\n\"Early\"\n\"female\"\n\"Margery_Kempe\"\n\"Kempe\"\n\n\n\"Thomas More\"\n1478\n1535\n\"Sixteenth C\"\n\"male\"\n\"Thomas_More\"\n\"More\"\n\n\n\"Christopher Marlowe\"\n1564\n1593\n\"Sixteenth C\"\n\"male\"\n\"Christopher_Marlowe\"\n\"Marlowe\"\n\n\n\"William Shakespeare\"\n1564\n1616\n\"Sixteenth C\"\n\"male\"\n\"William_Shakespeare\"\n\"Shakespeare\"\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Oscar Wilde\"\n1854\n1900\n\"Victorian\"\n\"male\"\n\"Oscar_Wilde\"\n\"Wilde\"\n\n\n\"James Joyce\"\n1882\n1941\n\"Twentieth C\"\n\"male\"\n\"James_Joyce\"\n\"Joyce\"\n\n\n\"D. H. Lawrence\"\n1885\n1930\n\"Twentieth C\"\n\"male\"\n\"D._H._Lawrence\"\n\"Lawrence\"\n\n\n\"A. A. Milne\"\n1882\n1956\n\"Twentieth C\"\n\"male\"\n\"A._A._Milne\"\n\"Milne\"\n\n\n\"Louis MacNeice\"\n1907\n1963\n\"Twentieth C\"\n\"male\"\n\"Louis_MacNeice\"\n\"MacNeice\"\n\n\n\n\n\n\nWe can combine these anchors with character classes. Square brackets define a set of characters to match. The pattern [A-Z] matches any uppercase letter, while [a-z] matches any lowercase letter. To find names where the first character is lowercase (unusual for English names), we write:\n\n(\n    wiki\n    .filter(c.doc_id.str.contains(r\"^[a-z]\"))\n)\n\n\nshape: (0, 7)\n\n\n\ndoc_id\nborn\ndied\nera\ngender\nlink\nshort\n\n\nstr\ni64\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n\n\n\n\nNo results—every name starts with a capital letter, as expected. But what about names that contain a word starting with a lowercase letter? The pattern \\s[a-z] matches a space followed by a lowercase letter:\n\n(\n    wiki\n    .filter(c.doc_id.str.contains(r\"\\s[a-z]\"))\n)\n\n\nshape: (2, 7)\n\n\n\ndoc_id\nborn\ndied\nera\ngender\nlink\nshort\n\n\nstr\ni64\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n\"Marie de France\"\n1160\n1215\n\"Early\"\n\"female\"\n\"Marie_de_France\"\n\"Marie d. F.\"\n\n\n\"Daphne du Maurier\"\n1907\n1989\n\"Twentieth C\"\n\"female\"\n\"Daphne_du_Maurier\"\n\"Maurier\"\n\n\n\n\n\n\nThis finds “Marie de France,” whose name includes the lowercase “de.” The \\s is a shorthand character class meaning “any whitespace character.” There are several useful shorthands: \\d matches any digit, \\w matches any “word character” (letters, digits, and underscore), and \\S, \\D, and \\W match the opposite of their lowercase counterparts.\nQuantifiers let us specify repetition. The + means “one or more” of the preceding element. The pattern \\d+ matches one or more consecutive digits:\n\n(\n    wiki\n    .filter(c.link.str.contains(r\"\\d+\"))\n)\n\n\nshape: (0, 7)\n\n\n\ndoc_id\nborn\ndied\nera\ngender\nlink\nshort\n\n\nstr\ni64\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n\n\n\n\nThis finds authors whose Wikipedia link contains numbers, such as “Mary_I_of_England.” Other quantifiers include * (zero or more) and ? (zero or one).\nIf you want to search for literal text and avoid interpreting special characters as regex operators, set literal=True:\n\n(\n    wiki\n    .filter(c.era.str.contains(\"C\", literal=True))\n)\n\n\nshape: (36, 7)\n\n\n\ndoc_id\nborn\ndied\nera\ngender\nlink\nshort\n\n\nstr\ni64\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n\"Thomas More\"\n1478\n1535\n\"Sixteenth C\"\n\"male\"\n\"Thomas_More\"\n\"More\"\n\n\n\"Edmund Spenser\"\n1552\n1599\n\"Sixteenth C\"\n\"male\"\n\"Edmund_Spenser\"\n\"Spenser\"\n\n\n\"Walter Raleigh\"\n1552\n1618\n\"Sixteenth C\"\n\"male\"\n\"Walter_Raleigh\"\n\"Raleigh\"\n\n\n\"Philip Sidney\"\n1554\n1586\n\"Sixteenth C\"\n\"male\"\n\"Philip_Sidney\"\n\"Sidney\"\n\n\n\"Christopher Marlowe\"\n1564\n1593\n\"Sixteenth C\"\n\"male\"\n\"Christopher_Marlowe\"\n\"Marlowe\"\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Stephen Spender\"\n1909\n1995\n\"Twentieth C\"\n\"male\"\n\"Stephen_Spender\"\n\"Spender\"\n\n\n\"Christopher Isherwood\"\n1904\n1986\n\"Twentieth C\"\n\"male\"\n\"Christopher_Isherwood\"\n\"Isherwood\"\n\n\n\"Edward Upward\"\n1903\n2009\n\"Twentieth C\"\n\"male\"\n\"Edward_Upward\"\n\"Upward\"\n\n\n\"Rex Warner\"\n1905\n1986\n\"Twentieth C\"\n\"male\"\n\"Rex_Warner\"\n\"Warner\"\n\n\n\"Seamus Heaney\"\n1939\n1939\n\"Twentieth C\"\n\"male\"\n\"Seamus_Heaney\"\n\"Heaney\"",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "07_strings.html#aggregating-strings",
    "href": "07_strings.html#aggregating-strings",
    "title": "7  Strings",
    "section": "7.5 Aggregating Strings",
    "text": "7.5 Aggregating Strings\nWhen working with grouped data, we sometimes want to combine text values into a single string. The join method concatenates all values in a string column, separated by a delimiter you specify. This is particularly useful after grouping.\nFor example, we can list all authors in each era, separated by commas:\n\n(\n    wiki\n    .group_by(c.era)\n    .agg(\n        authors = c.short.sort().str.join(\", \")\n    )\n)\n\n\nshape: (7, 2)\n\n\n\nera\nauthors\n\n\nstr\nstr\n\n\n\n\n\"Restoration\"\n\"Boswell, Dryden, Johnson, Lock…\n\n\n\"Sixteenth C\"\n\"Marlowe, More, Raleigh, Shakes…\n\n\n\"Seventeenth C\"\n\"Cavendish, Donne, Herbert, Hob…\n\n\n\"Early\"\n\"Chaucer, Gower, Kempe, Langlan…\n\n\n\"Victorian\"\n\"Arnold, Browning, C. Brontë, D…\n\n\n\"Twentieth C\"\n\"Auden, Beckett, C. S. Lewis, C…\n\n\n\"Romantic\"\n\"Austen, Blake, Burns, Byron, C…\n\n\n\n\n\n\nNotice that we sorted the names before joining to produce alphabetized lists. You can also call unique() before joining if you want to eliminate duplicates. The combination of sort, unique, and join is a common pattern for producing clean, readable summaries.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "07_strings.html#extracting-text-with-patterns",
    "href": "07_strings.html#extracting-text-with-patterns",
    "title": "7  Strings",
    "section": "7.6 Extracting Text with Patterns",
    "text": "7.6 Extracting Text with Patterns\nThe extract method pulls out the first match of a regular expression pattern. This is used inside with_columns to create new columns. For example, we can extract the first word from each author’s full name using the pattern ^\\w+, which means “one or more word characters at the start of the string”:\n\n(\n    wiki\n    .select(c.doc_id)\n    .with_columns(\n        first_name = c.doc_id.str.extract(r\"^\\w+\")\n    )\n)\n\n\nshape: (75, 2)\n\n\n\ndoc_id\nfirst_name\n\n\nstr\nstr\n\n\n\n\n\"Marie de France\"\nnull\n\n\n\"Geoffrey Chaucer\"\nnull\n\n\n\"John Gower\"\nnull\n\n\n\"William Langland\"\nnull\n\n\n\"Margery Kempe\"\nnull\n\n\n…\n…\n\n\n\"Stephen Spender\"\nnull\n\n\n\"Christopher Isherwood\"\nnull\n\n\n\"Edward Upward\"\nnull\n\n\n\"Rex Warner\"\nnull\n\n\n\"Seamus Heaney\"\nnull\n\n\n\n\n\n\nSimilarly, the pattern \\w+$ extracts the last word:\n\n(\n    wiki\n    .select(c.doc_id)\n    .with_columns(\n        last_word = c.doc_id.str.extract(r\"\\w+$\")\n    )\n)\n\n\nshape: (75, 2)\n\n\n\ndoc_id\nlast_word\n\n\nstr\nstr\n\n\n\n\n\"Marie de France\"\nnull\n\n\n\"Geoffrey Chaucer\"\nnull\n\n\n\"John Gower\"\nnull\n\n\n\"William Langland\"\nnull\n\n\n\"Margery Kempe\"\nnull\n\n\n…\n…\n\n\n\"Stephen Spender\"\nnull\n\n\n\"Christopher Isherwood\"\nnull\n\n\n\"Edward Upward\"\nnull\n\n\n\"Rex Warner\"\nnull\n\n\n\"Seamus Heaney\"\nnull",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "07_strings.html#capture-groups",
    "href": "07_strings.html#capture-groups",
    "title": "7  Strings",
    "section": "7.7 Capture Groups",
    "text": "7.7 Capture Groups\nWhen a pattern contains parentheses, they create capture groups that let you extract specific parts of a match. By default, extract returns group 1 (the first set of parentheses). You can specify which group to extract by passing a second argument.\nConsider a pattern that matches a first name, then anything in the middle, then a last word: ^(\\w+).*\\s(\\w+)$. The first group captures the opening word, and the second captures the final word. We can extract each separately:\n\n(\n    wiki\n    .select(c.doc_id)\n    .with_columns(\n        first_name = c.doc_id.str.extract(r\"^(\\w+).*\\s(\\w+)$\", 1),\n        last_word = c.doc_id.str.extract(r\"^(\\w+).*\\s(\\w+)$\", 2)\n    )\n)\n\n\nshape: (75, 3)\n\n\n\ndoc_id\nfirst_name\nlast_word\n\n\nstr\nstr\nstr\n\n\n\n\n\"Marie de France\"\n\"Marie\"\n\"France\"\n\n\n\"Geoffrey Chaucer\"\n\"Geoffrey\"\n\"Chaucer\"\n\n\n\"John Gower\"\n\"John\"\n\"Gower\"\n\n\n\"William Langland\"\n\"William\"\n\"Langland\"\n\n\n\"Margery Kempe\"\n\"Margery\"\n\"Kempe\"\n\n\n…\n…\n…\n\n\n\"Stephen Spender\"\n\"Stephen\"\n\"Spender\"\n\n\n\"Christopher Isherwood\"\n\"Christopher\"\n\"Isherwood\"\n\n\n\"Edward Upward\"\n\"Edward\"\n\"Upward\"\n\n\n\"Rex Warner\"\n\"Rex\"\n\"Warner\"\n\n\n\"Seamus Heaney\"\n\"Seamus\"\n\"Heaney\"\n\n\n\n\n\n\nThe . in the pattern matches any character, and .* matches zero or more of any character—this is how we skip over the middle portion of the name.\nIf you want all matches of a pattern rather than just the first, use extract_all, which returns a list:\n\n(\n    wiki\n    .select(c.doc_id)\n    .with_columns(\n        capital_letters = c.doc_id.str.extract_all(r\"[A-Z]\")\n    )\n)\n\n\nshape: (75, 2)\n\n\n\ndoc_id\ncapital_letters\n\n\nstr\nlist[str]\n\n\n\n\n\"Marie de France\"\n[\"M\", \"F\"]\n\n\n\"Geoffrey Chaucer\"\n[\"G\", \"C\"]\n\n\n\"John Gower\"\n[\"J\", \"G\"]\n\n\n\"William Langland\"\n[\"W\", \"L\"]\n\n\n\"Margery Kempe\"\n[\"M\", \"K\"]\n\n\n…\n…\n\n\n\"Stephen Spender\"\n[\"S\", \"S\"]\n\n\n\"Christopher Isherwood\"\n[\"C\", \"I\"]\n\n\n\"Edward Upward\"\n[\"E\", \"U\"]\n\n\n\"Rex Warner\"\n[\"R\", \"W\"]\n\n\n\"Seamus Heaney\"\n[\"S\", \"H\"]",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "07_strings.html#replacing-text",
    "href": "07_strings.html#replacing-text",
    "title": "7  Strings",
    "section": "7.8 Replacing Text",
    "text": "7.8 Replacing Text\nThe replace method substitutes the first occurrence of a pattern with a new string, while replace_all substitutes every occurrence. Below we standardize the era labels by removing the ” C” suffix:\n\n(\n    wiki\n    .select(c.era)\n    .with_columns(\n        era_clean = c.era.str.replace(\" C\", \"\")\n    )\n    .unique()\n)\n\n\nshape: (7, 2)\n\n\n\nera\nera_clean\n\n\nstr\nstr\n\n\n\n\n\"Seventeenth C\"\n\"Seventeenth\"\n\n\n\"Sixteenth C\"\n\"Sixteenth\"\n\n\n\"Twentieth C\"\n\"Twentieth\"\n\n\n\"Romantic\"\n\"Romantic\"\n\n\n\"Early\"\n\"Early\"\n\n\n\"Victorian\"\n\"Victorian\"\n\n\n\"Restoration\"\n\"Restoration\"\n\n\n\n\n\n\nReplacements can use capture groups from the pattern. In the replacement string, $1 refers to the first capture group, $2 to the second, and so on. Here we swap the first and last words of each name:\n\n(\n    wiki\n    .select(c.doc_id)\n    .with_columns(\n        swapped = c.doc_id.str.replace(r\"^(\\w+)(.*\\s)(\\w+)$\", \"$3$2$1\")\n    )\n)\n\n\nshape: (75, 2)\n\n\n\ndoc_id\nswapped\n\n\nstr\nstr\n\n\n\n\n\"Marie de France\"\n\"France de Marie\"\n\n\n\"Geoffrey Chaucer\"\n\"Chaucer Geoffrey\"\n\n\n\"John Gower\"\n\"Gower John\"\n\n\n\"William Langland\"\n\"Langland William\"\n\n\n\"Margery Kempe\"\n\"Kempe Margery\"\n\n\n…\n…\n\n\n\"Stephen Spender\"\n\"Spender Stephen\"\n\n\n\"Christopher Isherwood\"\n\"Isherwood Christopher\"\n\n\n\"Edward Upward\"\n\"Upward Edward\"\n\n\n\"Rex Warner\"\n\"Warner Rex\"\n\n\n\"Seamus Heaney\"\n\"Heaney Seamus\"",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "07_strings.html#working-with-substrings",
    "href": "07_strings.html#working-with-substrings",
    "title": "7  Strings",
    "section": "7.9 Working with Substrings",
    "text": "7.9 Working with Substrings\nSometimes we need to extract a fixed portion of a string rather than matching a pattern. The slice method returns a substring given a starting position and length. Positions are zero-indexed, meaning the first character is at position 0.\n\n(\n    wiki\n    .select(c.doc_id)\n    .with_columns(\n        initials = c.doc_id.str.slice(0, 1)\n    )\n)\n\n\nshape: (75, 2)\n\n\n\ndoc_id\ninitials\n\n\nstr\nstr\n\n\n\n\n\"Marie de France\"\n\"M\"\n\n\n\"Geoffrey Chaucer\"\n\"G\"\n\n\n\"John Gower\"\n\"J\"\n\n\n\"William Langland\"\n\"W\"\n\n\n\"Margery Kempe\"\n\"M\"\n\n\n…\n…\n\n\n\"Stephen Spender\"\n\"S\"\n\n\n\"Christopher Isherwood\"\n\"C\"\n\n\n\"Edward Upward\"\n\"E\"\n\n\n\"Rex Warner\"\n\"R\"\n\n\n\"Seamus Heaney\"\n\"S\"\n\n\n\n\n\n\nFor cleaning whitespace, strip_chars removes leading and trailing spaces (or other characters you specify). The case conversion methods to_lowercase, to_uppercase, and to_titlecase are useful for standardizing text:\n\n(\n    wiki\n    .select(c.era)\n    .with_columns(\n        era_upper = c.era.str.to_uppercase(),\n        era_lower = c.era.str.to_lowercase()\n    )\n    .unique()\n)\n\n\nshape: (7, 3)\n\n\n\nera\nera_upper\nera_lower\n\n\nstr\nstr\nstr\n\n\n\n\n\"Twentieth C\"\n\"TWENTIETH C\"\n\"twentieth c\"\n\n\n\"Seventeenth C\"\n\"SEVENTEENTH C\"\n\"seventeenth c\"\n\n\n\"Restoration\"\n\"RESTORATION\"\n\"restoration\"\n\n\n\"Romantic\"\n\"ROMANTIC\"\n\"romantic\"\n\n\n\"Early\"\n\"EARLY\"\n\"early\"\n\n\n\"Victorian\"\n\"VICTORIAN\"\n\"victorian\"\n\n\n\"Sixteenth C\"\n\"SIXTEENTH C\"\n\"sixteenth c\"",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "07_strings.html#counting-strings",
    "href": "07_strings.html#counting-strings",
    "title": "7  Strings",
    "section": "7.10 Counting Strings",
    "text": "7.10 Counting Strings\nSeveral string methods return numeric values rather than text. The len_chars method returns the number of characters in each string:\n\n(\n    wiki\n    .select(c.doc_id)\n    .with_columns(\n        name_length = c.doc_id.str.len_chars()\n    )\n    .sort(c.name_length, descending=True)\n)\n\n\nshape: (75, 2)\n\n\n\ndoc_id\nname_length\n\n\nstr\nu32\n\n\n\n\n\"Elizabeth Barrett Browning\"\n26\n\n\n\"Samuel Taylor Coleridge\"\n23\n\n\n\"Alfred, Lord Tennyson\"\n21\n\n\n\"Christopher Isherwood\"\n21\n\n\n\"Percy Bysshe Shelley\"\n20\n\n\n…\n…\n\n\n\"John Locke\"\n10\n\n\n\"Lord Byron\"\n10\n\n\n\"John Clare\"\n10\n\n\n\"John Keats\"\n10\n\n\n\"Rex Warner\"\n10\n\n\n\n\n\n\nThe count_matches method counts how many times a pattern appears in each string. Here we count the number of spaces in each name, which tells us how many words it contains (plus one):\n\n(\n    wiki\n    .select(c.doc_id)\n    .with_columns(\n        spaces = c.doc_id.str.count_matches(\" \"),\n        word_count = c.doc_id.str.count_matches(\" \") + 1\n    )\n)\n\n\nshape: (75, 3)\n\n\n\ndoc_id\nspaces\nword_count\n\n\nstr\nu32\nu32\n\n\n\n\n\"Marie de France\"\n2\n3\n\n\n\"Geoffrey Chaucer\"\n1\n2\n\n\n\"John Gower\"\n1\n2\n\n\n\"William Langland\"\n1\n2\n\n\n\"Margery Kempe\"\n1\n2\n\n\n…\n…\n…\n\n\n\"Stephen Spender\"\n1\n2\n\n\n\"Christopher Isherwood\"\n1\n2\n\n\n\"Edward Upward\"\n1\n2\n\n\n\"Rex Warner\"\n1\n2\n\n\n\"Seamus Heaney\"\n1\n2\n\n\n\n\n\n\nThe find method returns the starting index of the first match, or -1 if not found:\n\n(\n    wiki\n    .select(c.doc_id)\n    .with_columns(\n        space_pos = c.doc_id.str.find(\" \")\n    )\n)\n\n\nshape: (75, 2)\n\n\n\ndoc_id\nspace_pos\n\n\nstr\nu32\n\n\n\n\n\"Marie de France\"\n5\n\n\n\"Geoffrey Chaucer\"\n8\n\n\n\"John Gower\"\n4\n\n\n\"William Langland\"\n7\n\n\n\"Margery Kempe\"\n7\n\n\n…\n…\n\n\n\"Stephen Spender\"\n7\n\n\n\"Christopher Isherwood\"\n11\n\n\n\"Edward Upward\"\n6\n\n\n\"Rex Warner\"\n3\n\n\n\"Seamus Heaney\"\n6",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "07_strings.html#splitting-strings",
    "href": "07_strings.html#splitting-strings",
    "title": "7  Strings",
    "section": "7.11 Splitting Strings",
    "text": "7.11 Splitting Strings\nThe split method divides a string into a list of substrings based on a delimiter. This creates a list column, which often needs further processing. The most common follow-up is to use explode to expand each list element into its own row.\n\n(\n    wiki\n    .select(c.doc_id)\n    .with_columns(\n        name_parts = c.doc_id.str.split(\" \")\n    )\n)\n\n\nshape: (75, 2)\n\n\n\ndoc_id\nname_parts\n\n\nstr\nlist[str]\n\n\n\n\n\"Marie de France\"\n[\"Marie\", \"de\", \"France\"]\n\n\n\"Geoffrey Chaucer\"\n[\"Geoffrey\", \"Chaucer\"]\n\n\n\"John Gower\"\n[\"John\", \"Gower\"]\n\n\n\"William Langland\"\n[\"William\", \"Langland\"]\n\n\n\"Margery Kempe\"\n[\"Margery\", \"Kempe\"]\n\n\n…\n…\n\n\n\"Stephen Spender\"\n[\"Stephen\", \"Spender\"]\n\n\n\"Christopher Isherwood\"\n[\"Christopher\", \"Isherwood\"]\n\n\n\"Edward Upward\"\n[\"Edward\", \"Upward\"]\n\n\n\"Rex Warner\"\n[\"Rex\", \"Warner\"]\n\n\n\"Seamus Heaney\"\n[\"Seamus\", \"Heaney\"]\n\n\n\n\n\n\nWhen we explode the list column, each word becomes a separate row:\n\n(\n    wiki\n    .select(c.doc_id, c.short)\n    .with_columns(\n        name_parts = c.doc_id.str.split(\" \")\n    )\n    .explode(c.name_parts)\n)\n\n\nshape: (164, 3)\n\n\n\ndoc_id\nshort\nname_parts\n\n\nstr\nstr\nstr\n\n\n\n\n\"Marie de France\"\n\"Marie d. F.\"\n\"Marie\"\n\n\n\"Marie de France\"\n\"Marie d. F.\"\n\"de\"\n\n\n\"Marie de France\"\n\"Marie d. F.\"\n\"France\"\n\n\n\"Geoffrey Chaucer\"\n\"Chaucer\"\n\"Geoffrey\"\n\n\n\"Geoffrey Chaucer\"\n\"Chaucer\"\n\"Chaucer\"\n\n\n…\n…\n…\n\n\n\"Edward Upward\"\n\"Upward\"\n\"Upward\"\n\n\n\"Rex Warner\"\n\"Warner\"\n\"Rex\"\n\n\n\"Rex Warner\"\n\"Warner\"\n\"Warner\"\n\n\n\"Seamus Heaney\"\n\"Heaney\"\n\"Seamus\"\n\n\n\"Seamus Heaney\"\n\"Heaney\"\n\"Heaney\"\n\n\n\n\n\n\nThis pattern is useful for tasks like building a vocabulary of unique words or counting word frequencies across a corpus.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "07_strings.html#regex-reference",
    "href": "07_strings.html#regex-reference",
    "title": "7  Strings",
    "section": "7.12 RegEx Reference",
    "text": "7.12 RegEx Reference\nPolars uses Rust-based regular expressions. The full syntax is documented on the regex crate page. Here is a summary of the patterns we have used in this chapter, along with a few additional ones. A full summary is provided in Chapter 21.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "07_strings.html#coming-from-r-or-pandas",
    "href": "07_strings.html#coming-from-r-or-pandas",
    "title": "7  Strings",
    "section": "7.13 Coming from R or Pandas",
    "text": "7.13 Coming from R or Pandas\nIf you have used the stringi package in R or the .str accessor in Pandas, the methods here will feel familiar. Polars uses the same general approach of namespacing string operations under .str. The main differences are in function names and the specific regular expression engine (Rust’s regex crate, which is similar to but not identical to PCRE). The core concepts of pattern matching, extraction, and replacement transfer directly.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "07_strings.html#references",
    "href": "07_strings.html#references",
    "title": "7  Strings",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "08_dataformats.html",
    "href": "08_dataformats.html",
    "title": "8  Data Formats",
    "section": "",
    "text": "8.1 Setup\nLoad all of the modules and datasets needed for the chapter. In addition to the standard modules, here we also use duckdb to work with databases, json to parse JSON data, and lxml to parse XML and HTML.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nimport duckdb\nimport json\nfrom lxml import etree, html",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Formats</span>"
    ]
  },
  {
    "objectID": "08_dataformats.html#introduction",
    "href": "08_dataformats.html#introduction",
    "title": "8  Data Formats",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\nThroughout this text, we have primarily worked with tabular data stored in CSV files. As we have seen, this format is surprisingly flexible while also keeping the data organized in a way that is optimized for using the grammar of graphics and data manipulation verbs to perform exploratory data analysis. Getting data organized for analysis often takes significant time. In fact, preparing data frequently takes more time than the analysis itself. Whether we are creating our own datasets or converting between formats, collecting and organizing data is a time-consuming yet essential task in data science.\nThere are many other formats available for storing tabular datasets. We mentioned the use of the Excel format in Chapter 5 as a useful option for data entry. Later, we will use the special GeoJSON format for storing tabular data along with geospatial information in Chapter 16 and the Parquet format for storing embeddings in Chapter 15. For most other common tabular data formats, there is likely to be at least one Python function or package that can read—and in most cases write—data stored in that format. Even within Polars, we can load many variations, such as tables that use other delimiters or fixed-width columns.\nIn some cases, data will be available in a format that is not initially organized into the kinds of tables we introduced in Chapter 1 or that requires intentional manipulation before we can access it in a tabular format. Some common types of non-tabular data that we may encounter include raw text, JSON (JavaScript Object Notation), XML (Extensible Markup Language), and HTML (HyperText Markup Language). All of these can be loaded into Python using built-in functions and packages specifically designed to parse them. The challenge, however, is that when read into Python these formats will be in the form of dictionaries, lists, or other custom objects. Parsing these formats into a DataFrame requires writing custom code that takes into account the specific information stored in the data. Often this requires using new functions or query languages, such as regular expressions or XPath queries, to facilitate the conversion process.\nIn addition to file formats, this chapter also introduces databases. Databases are specialized software systems designed to efficiently store, organize, and retrieve large amounts of structured data. While CSV files work well for small to medium datasets that fit comfortably in memory, databases become essential when working with larger datasets, when multiple users need to access the same data simultaneously, or when data changes frequently. Understanding how to interact with databases is a valuable skill that bridges the gap between exploratory analysis and production data systems.\nIn this chapter, we introduce these common formats and show examples of how they can be used to represent different kinds of information. Along the way, we demonstrate specific functions for parsing data stored in these file types.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Formats</span>"
    ]
  },
  {
    "objectID": "08_dataformats.html#json",
    "href": "08_dataformats.html#json",
    "title": "8  Data Formats",
    "section": "8.3 JSON",
    "text": "8.3 JSON\nJavaScript Object Notation, usually abbreviated as JSON, is a popular open-standard data format. It was originally designed for the JavaScript language but is now supported in virtually every modern programming language. The format of JSON closely resembles native data structures found in Python, making it particularly natural to work with. In part because of the importance of JavaScript as a core programming language in modern web browsers, JSON has become one of the most popular formats for data storage and transfer. This is particularly true for data distributed over web-based APIs, as we will see in Chapter 9.\nAs with CSV, JSON data is stored in a plaintext format. This means we can open the file in any text editor and see the data in a human-readable format. This transparency is one of JSON’s key advantages: we can inspect the data structure directly without needing specialized software. We created a small example of a JSON file displaying information about two of the authors in our Wikipedia dataset:\n{\n  \"data\": [\n    {\n      \"name\": \"Charlotte Brontë\",\n      \"age_at_death\": 38,\n      \"date\": {\n        \"born\": \"21 April 1816\",\n        \"died\": \"31 March 1855\"\n      },\n      \"profession\": [\n        \"novelist\",\n        \"poet\",\n        \"governess\"\n      ]\n    },\n    {\n      \"name\": \"Virginia Woolf\",\n      \"age_at_death\": 59,\n      \"date\": {\n        \"born\": \"25 January 1882\",\n        \"died\": \"28 March 1941\"\n      },\n      \"profession\": [\n        \"novelist\",\n        \"essayist\",\n        \"critic\"\n      ]\n    }\n  ]\n}\nData stored in the JSON format is highly structured. In some ways, the format is more strict than CSV files because it enforces a consistent syntax and is relatively easy to parse programmatically. Looking at the example above, we see many of the basic element types of JSON data. In fact, there are only six core data types available:\n\nan empty value called null\na number (either integer or floating-point)\na string (enclosed in double quotes)\na Boolean value equal to true or false\nan object of named value pairs, with names equal to strings and values equal to any other data type (similar to a Python dictionary)\nan ordered array of objects coming from any other type (similar to a Python list)\n\nObjects are defined by curly braces and arrays are defined with square brackets. The reason that JSON can become complex even with these limited types is that, as in the example above, it is possible to create deeply nested structures using the object and array types. A JSON array can contain objects, which themselves contain arrays of more objects, and so on indefinitely. This flexibility makes JSON excellent for representing hierarchical data but can make converting it to a flat tabular format challenging.\nTo read a JSON object into Python, we can use the built-in json module. This module is part of Python’s standard library, so no additional installation is required. Below, we load a version of the example dataset presented above.\n\nobj_json = json.loads(Path(\"examples/author.json\").read_text())\n\ntype(obj_json)\n\ndict\n\n\nThe output object obj_json is a Python dictionary. In general, Python turns JSON arrays into lists and objects into dictionaries. Numbers, strings, and Boolean objects become the corresponding Python types, and null becomes Python’s None. This mapping between JSON and Python types is intuitive and makes working with JSON data feel natural.\nTo create a structured dataset from the output, we can use standard Python dictionary and list operations. Here, we extract the author names using a list comprehension.\n\n[author['name'] for author in obj_json['data']]\n\n['Charlotte Brontë', 'Virginia Woolf']\n\n\nThis approach of navigating through nested dictionaries and lists is the fundamental technique for working with JSON data. Each level of nesting requires an additional key lookup (for dictionaries) or index access (for lists). In the example above, obj_json['data'] returns the list of author objects, and then we iterate through each author to extract the 'name' field.\nWe can use a for-loop over the JSON data to construct a DataFrame object containing all of the author metadata in a tabular form. The key insight here is that we need to decide which pieces of information belong in our output table and how to handle nested structures.\n\nmeta = []\nfor author in obj_json['data']:\n    meta.append({\n        'name': author['name'],\n        'age_at_death': author['age_at_death'],\n        'born': author['date']['born'],\n        'died': author['date']['died']\n    })\n\nmeta = pl.DataFrame(meta)\nmeta\n\n\nshape: (2, 4)\n\n\n\nname\nage_at_death\nborn\ndied\n\n\nstr\ni64\nstr\nstr\n\n\n\n\n\"Charlotte Brontë\"\n38\n\"21 April 1816\"\n\"31 March 1855\"\n\n\n\"Virginia Woolf\"\n59\n\"25 January 1882\"\n\"28 March 1941\"\n\n\n\n\n\n\nNotice how we “flattened” the nested date object by extracting its born and died fields as separate columns in our output. This is a common pattern when converting JSON to tabular format: nested objects often become multiple columns rather than a single complex column.\nThe JSON object also associates each author with a set of professions. JSON naturally handles such nested structures, where a single entity has multiple related values. However, this can be difficult to represent in a simple rectangular table. To create a dataset mapping each author to all of their professions, we need to create multiple rows for authors with multiple professions.\n\nprof = []\nfor author in obj_json['data']:\n    name = author['name']\n    for profession in author['profession']:\n        prof.append({\n            'name': name,\n            'profession': profession\n        })\n\nprof = pl.DataFrame(prof)\nprof\n\n\nshape: (6, 2)\n\n\n\nname\nprofession\n\n\nstr\nstr\n\n\n\n\n\"Charlotte Brontë\"\n\"novelist\"\n\n\n\"Charlotte Brontë\"\n\"poet\"\n\n\n\"Charlotte Brontë\"\n\"governess\"\n\n\n\"Virginia Woolf\"\n\"novelist\"\n\n\n\"Virginia Woolf\"\n\"essayist\"\n\n\n\"Virginia Woolf\"\n\"critic\"\n\n\n\n\n\n\nThis result illustrates a fundamental tension between JSON’s hierarchical structure and tabular data’s rectangular structure. In JSON, Charlotte Brontë appears once with an array of three professions. In our tabular format, she appears in three separate rows, once for each profession. Neither representation is inherently better; they simply serve different purposes. The tabular format makes it easy to filter, group, and join on profession, which would be awkward with nested arrays.\nIt is often the case that one JSON file needs to be turned into multiple tabular datasets. In our example, we created two separate tables: one with author metadata and one with author-profession pairs. These tables could be linked together using the author’s name as a key, following the relational concepts we learned in Chapter 4. Deciding what tables to build and how to link them together is the core task of turning JSON data into tabular data. The difficulty of this varies greatly depending on the level of nesting in the JSON data as well as the consistency from record to record. Real-world JSON data can be messy, with some records missing fields that others have, or with different records having different structures entirely. Robust code for parsing JSON often needs to handle these inconsistencies gracefully.\n\n\n\n\n\n\nWorking with large JSON files\n\n\n\n\n\nFor very large JSON files that don’t fit comfortably in memory, you may want to consider streaming parsers that can process the file incrementally. The ijson package provides this functionality for Python. Additionally, some JSON files store one JSON object per line (a format called JSON Lines or NDJSON), which can be processed line by line without loading the entire file.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Formats</span>"
    ]
  },
  {
    "objectID": "08_dataformats.html#xml-and-html",
    "href": "08_dataformats.html#xml-and-html",
    "title": "8  Data Formats",
    "section": "8.4 XML and HTML",
    "text": "8.4 XML and HTML\nExtensible Markup Language (XML) is another popular format for transferring and storing data. Like JSON, the format is quite flexible and typically results in nested, tree-like structures that require some work to turn into a rectangular data format. However, XML has a different philosophy than JSON. Much of the formal standards for XML are concerned with describing how groups can produce specific “extensible” dialects of XML that have consistent names and structures to describe particular kinds of data. Popular examples include XML-RDF (Resource Description Framework) for describing linked open data, XML-TEI (Text Encoding Initiative) for providing markup to textual data, and numerous domain-specific formats in fields ranging from finance to biology.\nThe XML format organizes data inside of hierarchically nested tags. Each tag has a name enclosed in angle brackets, and tags come in pairs: an opening tag like &lt;author&gt; and a closing tag like &lt;/author&gt;. Everything between the opening and closing tags is the content of that element. Tags can also have attributes, which are name-value pairs that appear inside the opening tag. Below is an example of how the data from the previous JSON example could have been stored in an XML dataset.\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;root&gt;\n  &lt;author&gt;\n    &lt;name&gt;Charlotte Brontë&lt;/name&gt;\n    &lt;life&gt;\n      &lt;item name=\"born\"&gt;21 April 1816&lt;/item&gt;\n      &lt;item name=\"died\"&gt;31 March 1855&lt;/item&gt;\n    &lt;/life&gt;\n    &lt;ageatdeath&gt;38&lt;/ageatdeath&gt;\n    &lt;professions&gt;\n      &lt;profession&gt;novelist&lt;/profession&gt;\n      &lt;profession&gt;poet&lt;/profession&gt;\n      &lt;profession&gt;governess&lt;/profession&gt;\n    &lt;/professions&gt;\n  &lt;/author&gt;\n  &lt;author&gt;\n    &lt;name&gt;Virginia Woolf&lt;/name&gt;\n    &lt;life&gt;\n      &lt;item name=\"born\"&gt;25 January 1882&lt;/item&gt;\n      &lt;item name=\"died\"&gt;28 March 1941&lt;/item&gt;\n    &lt;/life&gt;\n    &lt;ageatdeath&gt;59&lt;/ageatdeath&gt;\n    &lt;professions&gt;\n      &lt;profession&gt;novelist&lt;/profession&gt;\n      &lt;profession&gt;essayist&lt;/profession&gt;\n      &lt;profession&gt;critic&lt;/profession&gt;\n    &lt;/professions&gt;\n  &lt;/author&gt;\n&lt;/root&gt;\nComparing this to the JSON version, we can see several differences. Information can be stored either as text content within tags (like the author’s name inside &lt;n&gt;...&lt;/n&gt;) or as attributes (like name=\"born\" in the &lt;item&gt; tag). XML tends to be more verbose than JSON because of all the repeated tag names, but it offers more flexibility in how data can be structured and annotated.\nPython provides several ways to parse XML. One of the most powerful is the lxml library, which provides both speed and a comprehensive feature set. Here is the code to get started loading our example data into Python.\n\nxml_path = Path(\"examples/author.xml\")\ntree = etree.parse(str(xml_path))\nroot = tree.getroot()\nroot\n\n&lt;Element root at 0x1129be8c0&gt;\n\n\nThe tree object represents the entire XML document, while root is the top-level element (the &lt;root&gt; tag in our example). Unlike JSON, which loads directly into Python dictionaries and lists, XML loads into specialized element objects that we need to navigate using XML-specific methods.\nTo work with the data, we will use a special query language called XPath. XPath allows us to locate elements within an XML document by specifying their position in the document’s tree structure, their tag names, their attributes, or combinations of these criteria. You can think of XPath as similar to regular expressions but designed for navigating XML structures rather than matching text patterns. For example, we can use the expression /root/author to get all of the tags &lt;author&gt; that are directly contained within the tag &lt;root&gt;.\n\nauthor_nodes = root.xpath(\"/root/author\")\nauthor_nodes\n\n[&lt;Element author at 0x1129c00c0&gt;, &lt;Element author at 0x1129c0600&gt;]\n\n\nThe XPath expression /root/author means “starting from the root of the document, find the root element, then find all author elements directly inside it.” The result is a list of element objects, one for each author in our data. The leading slash indicates we’re starting from the absolute top of the document, while the forward slashes separate each level of the hierarchy.\nNow, to build a DataFrame of the author-level metadata, we need to cycle through each of the author nodes. For each author node, we use further XPath expressions to extract all of the author-level information.\n\nauthor_df = []\n\nfor i, author in enumerate(author_nodes, start=1):\n    author_df.append(\n        {\n            \"author_id\": i,\n            \"name\": author.xpath(\"name\")[0].text,\n            \"born\": author.xpath(\"life/item[@name='born']\")[0].text,\n            \"died\": author.xpath(\"life/item[@name='died']\")[0].text,\n            \"age_at_death\": int(author.xpath(\"ageatdeath\")[0].text),\n        }\n    )\n\nauthor_df = pl.DataFrame(author_df)\nauthor_df\n\n\nshape: (2, 5)\n\n\n\nauthor_id\nname\nborn\ndied\nage_at_death\n\n\ni64\nstr\nstr\nstr\ni64\n\n\n\n\n1\n\"Charlotte Brontë\"\n\"21 April 1816\"\n\"31 March 1855\"\n38\n\n\n2\n\"Virginia Woolf\"\n\"25 January 1882\"\n\"28 March 1941\"\n59\n\n\n\n\n\n\nNotice several important details in this code. First, when we use XPath from an element (like author.xpath(\"n\")), the path is relative to that element rather than starting from the document root. Second, the expression [@name='born'] is an XPath predicate that filters elements based on their attributes—it selects only &lt;item&gt; elements where the name attribute equals \"born\". Third, the .text property of an element gives us the text content between the opening and closing tags.\nWe can use a similar approach to parse the professions, creating a separate table that we could later join with the author metadata.\n\nprofession_df = []\n\nfor i, author in enumerate(author_nodes, start=1):\n    for p in author.xpath(\"professions/profession\"):\n        profession_df.append({\n            \"author_id\": i,\n            \"profession\": p.text,\n        })\n\nprofession_df = pl.DataFrame(profession_df)\nprofession_df\n\n\nshape: (6, 2)\n\n\n\nauthor_id\nprofession\n\n\ni64\nstr\n\n\n\n\n1\n\"novelist\"\n\n\n1\n\"poet\"\n\n\n1\n\"governess\"\n\n\n2\n\"novelist\"\n\n\n2\n\"essayist\"\n\n\n2\n\"critic\"\n\n\n\n\n\n\nThe path professions/profession navigates from each author element into the &lt;professions&gt; container and then selects all &lt;profession&gt; elements within it. This pattern of iterating through parent elements and then through their children is common when converting hierarchical XML into flat tables.\n\n\n\n\n\n\nXPath expressions\n\n\n\n\n\nXPath is a rich query language with many features beyond what we’ve shown here. Some useful expressions include // (find elements anywhere in the document, not just at a specific path), * (match any element name), [position()] (select elements by their position), and various functions for string manipulation and numeric comparisons. For complex XML documents, investing time in learning XPath can significantly simplify your parsing code.\n\n\n\nHTML, short for HyperText Markup Language, is a closely related format to XML that is primarily intended to be displayed in a web browser. While HTML’s primary purpose is not to store arbitrary data, we often need to extract data from HTML documents. This is particularly common when we want to collect data from websites that don’t offer a structured API. The process of extracting data from web pages is sometimes called “web scraping.”\nWe can parse HTML using the same lxml library, but with its HTML-specific module instead of the XML parser. Here is a simple example:\n\nhtml_content = \"\"\"\n&lt;html&gt;\n  &lt;body&gt;\n    &lt;table&gt;\n      &lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Born&lt;/th&gt;&lt;/tr&gt;\n      &lt;tr&gt;&lt;td&gt;Charlotte Brontë&lt;/td&gt;&lt;td&gt;1816&lt;/td&gt;&lt;/tr&gt;\n      &lt;tr&gt;&lt;td&gt;Virginia Woolf&lt;/td&gt;&lt;td&gt;1882&lt;/td&gt;&lt;/tr&gt;\n    &lt;/table&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\ndoc = html.fromstring(html_content)\nrows = doc.xpath(\"//tr\")\nfor row in rows:\n    cells = row.xpath(\"td/text() | th/text()\")\n    print(cells)\n\n['Name', 'Born']\n['Charlotte Brontë', '1816']\n['Virginia Woolf', '1882']\n\n\nHTML often has additional whitespace within its tags, and real-world web pages frequently have messy, inconsistent structure. When extracting data from HTML, it can be helpful to use the .text_content() method of elements instead of the .text property. The .text_content() method concatenates all text within an element, including text in nested child elements, while .text only returns the text directly inside the element before any child elements.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Formats</span>"
    ]
  },
  {
    "objectID": "08_dataformats.html#duckdb",
    "href": "08_dataformats.html#duckdb",
    "title": "8  Data Formats",
    "section": "8.5 DuckDB",
    "text": "8.5 DuckDB\nDatabases are specialized software systems designed to store, organize, and retrieve structured data. They offer significant advantages over simple file-based storage: they can efficiently handle datasets too large to fit in memory, they support concurrent access by multiple users, they enforce data integrity constraints, and they provide powerful query capabilities. If you find yourself working in a corporate environment with larger, complex datasets that change frequently, it is almost certainly the case that you will use a database to fetch data and perform initial processing before pulling subsets into Python for detailed analysis and visualization. Even for local work, if you are dealing with datasets that are difficult to load into memory all at once, a database can provide an efficient solution.\nMost databases have well-maintained Python modules for interacting with them. In this section, we illustrate the process of using DuckDB, a modern database implementation that is particularly well-suited for analytical workloads. DuckDB is designed to be embedded directly within applications and requires no external setup—it can be managed entirely from within Python. This makes it an excellent choice for data science work where you want the power of a database without the complexity of setting up and maintaining a separate database server. Other database systems like PostgreSQL, MySQL, or SQLite function similarly but require more involved setup steps. That’s not necessarily a problem if you’re accessing a database maintained by an IT team, but for local analytical work, DuckDB is an excellent choice.\nTo start, we will create a local DuckDB database, which is nothing more than a highly optimized file stored on our machine. We will be working with the countries.csv dataset and will begin by creating a DuckDB connection to a new file called countries.duckdb.\n\ncon = duckdb.connect(\"examples/countries.duckdb\")\n\nThe con object represents our connection to the database. All subsequent operations will go through this connection object.\nInteracting with databases typically involves using a specialized language called SQL (Structured Query Language). SQL is used both to define the structure of data (creating tables, defining relationships) and to query and manipulate data that is already stored. Many of SQL’s query operations mirror the Polars data manipulation commands we learned in Chapter 2 and Chapter 4. If you understand filtering, selecting, grouping, and joining in Polars, you already understand the core concepts of SQL—only the syntax differs.\nWe will not provide a complete introduction to SQL here, but we will highlight the key commands that you are most likely to encounter. First, to load a dataset from a CSV file into the database, we use the following command:\n\ncon.execute(f\"\"\"\n    CREATE TABLE IF NOT EXISTS country AS\n    SELECT *\n    FROM read_csv_auto('data/countries.csv')\n\"\"\")\n\n&lt;_duckdb.DuckDBPyConnection at 0x108036fb0&gt;\n\n\nLet’s break down this SQL statement. CREATE TABLE IF NOT EXISTS country tells the database to create a new table called country, but only if a table with that name doesn’t already exist. The AS SELECT * FROM read_csv_auto('data/countries.csv') part says to populate this table by reading all columns (*) from our CSV file. DuckDB’s read_csv_auto function automatically detects column types and handles common CSV formatting issues.\nThis loading step only needs to be done once. Once the data is in the database, it persists across Python sessions (unlike DataFrames in memory, which disappear when you close Python). We can then query the data using SELECT statements.\n\ndf = con.execute(\"\"\"\n    SELECT iso, full_name, region, hdi, gdp\n    FROM country\n    ORDER BY hdi DESC\n    LIMIT 10\n\"\"\").pl()\ndf\n\n\nshape: (10, 5)\n\n\n\niso\nfull_name\nregion\nhdi\ngdp\n\n\nstr\nstr\nstr\nf64\ni64\n\n\n\n\n\"ISL\"\n\"Iceland\"\n\"Europe\"\n0.972\n67444\n\n\n\"CHE\"\n\"Switzerland\"\n\"Europe\"\n0.97\n84311\n\n\n\"NOR\"\n\"Norway\"\n\"Europe\"\n0.97\n94896\n\n\n\"DNK\"\n\"Denmark\"\n\"Europe\"\n0.962\n77337\n\n\n\"DEU\"\n\"Germany\"\n\"Europe\"\n0.959\n64355\n\n\n\"SWE\"\n\"Sweden\"\n\"Europe\"\n0.959\n65597\n\n\n\"AUS\"\n\"Australia\"\n\"Oceania\"\n0.958\n61943\n\n\n\"NLD\"\n\"Netherlands, Kingdom of the\"\n\"Europe\"\n0.955\n73384\n\n\n\"BEL\"\n\"Belgium\"\n\"Europe\"\n0.951\n65096\n\n\n\"IRL\"\n\"Ireland\"\n\"Europe\"\n0.949\n119406\n\n\n\n\n\n\nThis query selects five specific columns from our country table, sorts the results by hdi in descending order, and returns only the top 10 rows. The .pl() method at the end converts the result into a Polars DataFrame, allowing us to continue working with the data using all the Polars methods we’ve learned. Notice how the SQL keywords (SELECT, FROM, ORDER BY, LIMIT) directly correspond to Polars operations (.select(), reading from a table, .sort(), .head()).\nOften we use a database because the data is too large to fit easily in Python memory in its full form. We can limit the number of rows that we read in using a WHERE clause, which is similar to a filter in Polars. This allows us to extract just the subset of data we need for a particular analysis.\n\ndf = con.execute(\"\"\"\n    SELECT iso, full_name, lexp, gdp\n    FROM country\n    WHERE region = 'Americas'\n      AND lexp &gt;= 75\n      AND gdp &gt;= 20000\n    ORDER BY gdp DESC\n\"\"\").pl()\ndf\n\n\nshape: (8, 4)\n\n\n\niso\nfull_name\nlexp\ngdp\n\n\nstr\nstr\nf64\ni64\n\n\n\n\n\"USA\"\n\"United States of America\"\n79.83\n78389\n\n\n\"CAN\"\n\"Canada\"\n83.15\n58422\n\n\n\"PAN\"\n\"Panama\"\n81.17\n38412\n\n\n\"URY\"\n\"Uruguay\"\n78.5\n33758\n\n\n\"CHL\"\n\"Chile\"\n81.39\n31425\n\n\n\"CRI\"\n\"Costa Rica\"\n81.33\n28390\n\n\n\"MEX\"\n\"Mexico\"\n76.88\n22375\n\n\n\"BRA\"\n\"Brazil\"\n77.09\n20500\n\n\n\n\n\n\nThe WHERE clause supports multiple conditions combined with AND or OR, along with comparison operators like =, &gt;=, &lt;, and !=. This query finds countries in the Americas with high life expectancy and GDP.\nWe can also perform aggregation directly inside the database using GROUP BY:\n\ndf = con.execute(\"\"\"\n    SELECT\n        region,\n        COUNT(*) AS n_countries\n    FROM country\n    GROUP BY region\n    ORDER BY n_countries DESC\n\"\"\").pl()\ndf\n\n\nshape: (5, 2)\n\n\n\nregion\nn_countries\n\n\nstr\ni64\n\n\n\n\n\"Asia\"\n39\n\n\n\"Europe\"\n38\n\n\n\"Africa\"\n37\n\n\n\"Americas\"\n19\n\n\n\"Oceania\"\n2\n\n\n\n\n\n\nThis query counts the number of countries in each region, exactly like we would do with .group_by() and .agg() in Polars. The AS n_countries part gives a name to the computed column, just like providing a column name in a Polars aggregation.\nAll of this computation happens in DuckDB before sending the data back to Python, making it efficient for larger datasets. The database engine is optimized for these operations and can often process queries faster than equivalent Python code, especially for large datasets that benefit from disk-based processing.\n\n\n\n\n\n\nSQL and Polars\n\n\n\n\n\nIf you’re curious about the relationship between SQL and Polars, you might be interested to know that DuckDB and Polars share many design principles. Both use columnar data storage and lazy evaluation for query optimization. In fact, Polars can execute SQL queries directly on DataFrames using .sql(), and DuckDB can read Polars DataFrames directly without conversion. For data scientists, learning both SQL and Polars is valuable: SQL is essential for working with traditional databases and is widely used in industry, while Polars provides a more Pythonic interface that integrates naturally with the broader Python ecosystem for analysis and visualization.\n\n\n\nSQL provides a somewhat limited set of operations compared to Python’s full expressiveness. More involved modeling, visualization, and complex data transformations typically occur directly in Python after fetching the relevant data from the database. Combining the two tools provides a powerful workflow: use SQL and databases for storage and initial filtering of large datasets, then use Python for detailed analysis and visualization. Understanding how to move between these two worlds is an essential skill for working with data at scale.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Formats</span>"
    ]
  },
  {
    "objectID": "08_dataformats.html#conclusions",
    "href": "08_dataformats.html#conclusions",
    "title": "8  Data Formats",
    "section": "8.6 Conclusions",
    "text": "8.6 Conclusions\nIn this chapter, we explored several common data formats beyond the CSV files that have been our primary focus throughout this text. JSON provides a flexible, hierarchical format that maps naturally to Python’s dictionaries and lists, making it popular for web APIs and configuration files. XML offers similar hierarchical capabilities with a more verbose syntax and powerful querying through XPath. Both formats require us to think carefully about how to flatten nested structures into rectangular tables suitable for analysis.\nWe also introduced DuckDB as a representative database system, demonstrating how SQL queries can filter, select, and aggregate data before bringing it into Python. Databases become essential when working with datasets too large for memory or when data needs to be shared and updated by multiple users.\nThe common theme across all these formats is the need to transform data into a structure amenable to analysis. Whether parsing JSON objects, navigating XML trees with XPath, or writing SQL queries, the goal remains the same: extract the information we need and organize it into the tabular format that enables the exploratory data analysis techniques we’ve developed throughout this book. Mastering these transformations expands the range of data sources we can work with and prepares us for the diverse data landscape we’ll encounter in practice.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Formats</span>"
    ]
  },
  {
    "objectID": "08_dataformats.html#references",
    "href": "08_dataformats.html#references",
    "title": "8  Data Formats",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Formats</span>"
    ]
  },
  {
    "objectID": "09_requests.html",
    "href": "09_requests.html",
    "title": "9  Requests",
    "section": "",
    "text": "9.1 Setup\nLoad all of the modules and datasets needed for the chapter. We will also use requests and the requests-cache modules to make API requests and lxml to parse the results.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nfrom lxml import html\nimport requests\nimport requests_cache",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Requests</span>"
    ]
  },
  {
    "objectID": "09_requests.html#introduction",
    "href": "09_requests.html#introduction",
    "title": "9  Requests",
    "section": "9.2 Introduction",
    "text": "9.2 Introduction\nThroughout this book, we have worked with datasets that were either bundled with our code or available as downloadable files. In practice, however, much of the data we need as data scientists lives on remote servers, updated continuously or made available only through programmatic interfaces. Weather forecasts, stock prices, social media posts, and government statistics are just a few examples of data that change too frequently—or are too large—to distribute as static files.\nThis chapter introduces techniques for fetching data directly from the internet using Python. We will explore three increasingly sophisticated approaches. First, we will learn how to call web-based APIs (Application Programming Interfaces), which provide structured access to data from services like Wikipedia, weather providers, and financial databases. Second, we will see how to extract information directly from web pages—a technique called web scraping—when no formal API exists. Third, we will examine how to query structured knowledge bases using SPARQL, a powerful query language for linked data. Along the way, we will also see how to call local services running on our own machine, opening up possibilities for integrating large language models and other resource-intensive tools into our data pipelines.\nThese techniques expand what we can analyze far beyond pre-packaged datasets. By the end of this chapter, you will be able to gather real-time data from across the internet and integrate it seamlessly into your Polars workflows.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Requests</span>"
    ]
  },
  {
    "objectID": "09_requests.html#apis-and-http-requests",
    "href": "09_requests.html#apis-and-http-requests",
    "title": "9  Requests",
    "section": "9.3 APIs and HTTP Requests",
    "text": "9.3 APIs and HTTP Requests\nAn application programming interface (API) is a generic term for a specific interface that two computers can communicate across. While APIs can take many forms, the most common type for data access communicates over the internet using the Hypertext Transfer Protocol (HTTP). HTTP is the foundational protocol of the World Wide Web—it defines how your web browser requests pages from servers and how those servers respond. When you visit a website, your browser sends an HTTP request and receives an HTTP response containing the page content. APIs work the same way, except instead of returning human-readable web pages, they return structured data (usually in JSON format) designed for programs to consume.\nHTTP defines several methods that describe what kind of action a request is performing. The most common are GET (retrieve data), POST (send data to create something new), PUT (update existing data), and DELETE (remove data). For data retrieval, we almost always use GET requests. The server responds with a status code indicating whether the request succeeded: 200 means success, 404 means the requested resource wasn’t found, 500 means the server encountered an error, and so on. Understanding these basics helps when debugging why a request might fail.\nThere are many APIs available online for accessing a variety of different types of data. Some require setting up an account and obtaining an API key—a unique identifier that authenticates your requests. Some APIs require payment for each request, while others offer free access or a free tier for occasional users. Most frequently, APIs provide access to data that changes frequently, such as news stories, weather forecasts, or stock prices. Increasingly, though, even static datasets are being put behind API access rather than allowing straightforward downloads. Fortunately, Python’s requests library makes it relatively easy to make and parse API calls. For more details on the requests library, see its excellent documentation at https://requests.readthedocs.io/.\nWe will demonstrate API calls using the MediaWiki API that powers Wikipedia. This API is particularly nice for learning because it is freely available and requires no signup or authentication. Anyone can call the API directly and retrieve data related to Wikipedia and other Wikimedia projects. For comprehensive documentation on this API, see https://www.mediawiki.org/wiki/API:Main_page.\nBefore we get started, let’s build a local cache that stores the results of our API calls. This is important for two reasons. First, it avoids overwhelming the server with repeated requests for the same data—a courtesy that many APIs require and all appreciate. Second, it speeds up our code during development, since cached responses return instantly without network latency.\n\nsession = requests_cache.CachedSession(\n    cache_name=\"examples/requests_cache\",\n    backend=\"sqlite\",\n    allowable_methods=('GET', 'HEAD', 'POST'),\n    expire_after=None\n)\n\nThe requests_cache library creates a drop-in replacement for the standard requests session. Behind the scenes, it stores responses in a SQLite database file. Setting expire_after=None means cached responses never expire—useful for historical data that won’t change, though you might want a shorter expiration for frequently updated data.\nTo make an API call, we need three things: a base URL pointing to the API endpoint, parameters specifying what data we want, and headers identifying who we are. The parameters are specific to each API—you’ll need to consult the documentation to learn what options are available. The User-Agent header is a polite way to identify your application; many APIs require it, and it helps server administrators contact you if your requests cause problems.\n\nbase_url = \"https://en.wikipedia.org/w/api.php\"\nparams = {\n    'action': 'query',\n    'format': 'json',\n    'prop': 'pageviews',\n    'titles': 'Emily Brontë'\n}\n\nheaders = {\n    \"User-Agent\": \"DataScienceBook/1.0 (tarnold2@richmond.edu)\"\n}\n\nresponse = session.get(base_url, params=params, headers=headers)\n\nIn this example, we’re asking the MediaWiki API for pageview statistics about the Wikipedia article on Emily Brontë. The action parameter tells the API we want to query for information, format specifies we want the response in JSON, prop indicates we want pageview data, and titles names the specific article.\nWe can check the response code to verify the request succeeded. A status code of 200 indicates success.\n\nresponse.status_code\n\n200\n\n\nThe data returned from this API, as with most web-based APIs, is in JSON format. We can access and parse it with a simple method attached to the response object.\n\ndata = response.json()\n\nNow we are back in a situation similar to what we encountered in Chapter 8: we need to parse the JSON data into one or more tabular data structures. JSON from APIs is often deeply nested, requiring us to navigate through multiple levels to reach the data we want. Let’s extract the pageview data into a Polars DataFrame.\n\npages = data['query']['pages']\npage_id = list(pages.keys())[0]\npageviews = pages[page_id].get('pageviews', {})\n\npageview_data = []\nfor date, views in pageviews.items():\n    pageview_data.append({\n        'date': date,\n        'views': views,\n        'doc_id': 'Emily Brontë'\n    })\n\npage_views_df = pl.DataFrame(pageview_data)\npage_views_df\n\n\nshape: (60, 3)\n\n\n\ndate\nviews\ndoc_id\n\n\nstr\ni64\nstr\n\n\n\n\n\"2025-11-06\"\n1809\n\"Emily Brontë\"\n\n\n\"2025-11-07\"\n1923\n\"Emily Brontë\"\n\n\n\"2025-11-08\"\n1977\n\"Emily Brontë\"\n\n\n\"2025-11-09\"\n3909\n\"Emily Brontë\"\n\n\n\"2025-11-10\"\n2923\n\"Emily Brontë\"\n\n\n…\n…\n…\n\n\n\"2025-12-31\"\n2567\n\"Emily Brontë\"\n\n\n\"2026-01-01\"\n2590\n\"Emily Brontë\"\n\n\n\"2026-01-02\"\n3231\n\"Emily Brontë\"\n\n\n\"2026-01-03\"\n3221\n\"Emily Brontë\"\n\n\n\"2026-01-04\"\n3098\n\"Emily Brontë\"\n\n\n\n\n\n\nThe structure of API responses varies widely between services. Some APIs return flat, table-like data that converts easily to DataFrames. Others, like this one, nest data several levels deep. The key skill is learning to explore the JSON structure (often by printing intermediate results or consulting the API documentation) and then writing code to extract exactly what you need. With practice, you’ll develop an intuition for common patterns.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Requests</span>"
    ]
  },
  {
    "objectID": "09_requests.html#web-scraping",
    "href": "09_requests.html#web-scraping",
    "title": "9  Requests",
    "section": "9.4 Web Scraping",
    "text": "9.4 Web Scraping\nSometimes the data we need isn’t available through a formal API—it exists only on web pages designed for human readers. Web scraping is the technique of programmatically extracting data from HTML pages. While APIs provide structured data explicitly intended for programs, web scraping requires us to parse the visual structure of a page and extract the relevant pieces.\nThe requests module can fetch any URL, not just API endpoints. When we request a regular web page, we get back HTML—the markup language that defines the structure of web pages. We can then parse this HTML to extract the data we want.\nFor this example, we’ll grab headlines from CNN Lite, a simplified version of the CNN website that’s particularly easy to parse. CNN Lite presents news headlines as a simple list without the complex JavaScript and advertisements of the main site.\n\nurl = \"https://lite.cnn.com/\"\nheaders = {\n    \"User-Agent\": \"DataScienceBook/1.0 (tarnold2@richmond.edu)\"\n}\n\nresponse = session.get(url, headers=headers)\nhtml_text = response.text\n\nAt this point, the response is just a long string containing raw HTML markup. To extract meaningful data, we need to parse this HTML and navigate its structure. We use the lxml library’s HTML parser, which we introduced in Chapter 8.\n\ntree = html.fromstring(html_text)\nstories = tree.xpath(\"//li/a\")\nstories_df = pl.DataFrame({\n  \"headline\": [x.text_content().strip() for x in stories],\n  \"link\": [x.get('href') for x in stories]\n})\nstories_df\n\n\nshape: (100, 2)\n\n\n\nheadline\nlink\n\n\nstr\nstr\n\n\n\n\n\"Pentagon moves to cut Sen. Mar…\n\"/2026/01/05/politics/pentagon-…\n\n\n\"Greenland, Cuba, Iran and more…\n\"/2026/01/05/world/greenland-cu…\n\n\n\"2026 Movie Preview: Get ready …\n\"/2026/01/05/entertainment/2026…\n\n\n\"Minnesota Gov. Tim Walz ends r…\n\"/2026/01/05/politics/tim-walz-…\n\n\n\"Cleveland Browns fire Kevin St…\n\"/2026/01/05/sport/football-nfl…\n\n\n…\n…\n\n\n\"Suspected mountain lion attack…\n\"/2026/01/02/us/mountain-lion-a…\n\n\n\"In India, door deliveries can …\n\"/2026/01/01/india/india-gig-wo…\n\n\n\"Mayor Zohran Mamdani doubles d…\n\"/2026/01/01/politics/zohran-ma…\n\n\n\"No. 1 Indiana routs No. 9 Alab…\n\"/2026/01/01/sport/football-nca…\n\n\n\"Zohran Mamdani’s inauguration …\n\"/2026/01/01/politics/nyc-mayor…\n\n\n\n\n\n\nThe XPath expression //li/a finds all anchor (&lt;a&gt;) tags that are direct children of list item (&lt;li&gt;) tags anywhere in the document. For each matching element, we extract the text content (the headline) and the href attribute (the link URL). This pattern—fetch HTML, parse it, extract elements matching a pattern—is the core workflow for web scraping.\nA few words of caution about web scraping. First, websites change their structure frequently, which can break your scraping code without warning. Second, some websites prohibit scraping in their terms of service. Third, scraping too aggressively can overload servers—always use caching and rate limiting. Finally, always respect the robots.txt file that websites use to indicate which pages should not be accessed by automated tools. For more on the lxml library and XPath expressions, see https://lxml.de/.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Requests</span>"
    ]
  },
  {
    "objectID": "09_requests.html#calling-local-services",
    "href": "09_requests.html#calling-local-services",
    "title": "9  Requests",
    "section": "9.5 Calling Local Services",
    "text": "9.5 Calling Local Services\nSo far, we’ve focused on remote services accessed over the internet. But the same HTTP-based techniques work for services running locally on your own machine. This opens up powerful possibilities, particularly for integrating large language models (LLMs) into your data pipelines.\nTools like LM Studio allow you to run powerful language models locally without needing to set up complex Python environments or manage GPU configurations directly. LM Studio provides a simple graphical interface for downloading and running models, and it exposes them through an API that follows the OpenAI standard—meaning any code written for OpenAI’s API will work with local models too.\nWhy would you want to run models locally rather than using a cloud service? Privacy is one reason: your data never leaves your machine. Cost is another: after the initial setup, local inference is free. And for educational purposes, running models locally helps demystify how these systems work.\nLet’s use a locally-running model to classify the news headlines we scraped earlier. The code below assumes you have LM Studio running with a model loaded and serving on the default port. If you don’t have LM Studio set up, you can skip this section—the concepts transfer directly to cloud-based APIs like OpenAI’s.\n\nBASE_URL = \"http://127.0.0.1:1234/v1\"\nMODEL = \"openai/gpt-oss-20b\"\n\nheadline = \"Senate passes bipartisan bill to reform aviation safety rules\"\n\nsystem_prompt = (\n    \"You are a strict news desk classifier. \"\n    \"Return exactly one label from this set:\\n\"\n    'sports, local politics, national politics, world politics, science, law, economics, culture, other.\\n'\n    \"Return only the label, with no punctuation or extra words.\"\n)\n\npayload = {\n    \"model\": MODEL,\n    \"messages\": [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": f\"Headline: {headline}\\nLabel:\"},\n    ],\n    \"temperature\": 0\n}\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    # Some servers accept/ignore this; it doesn't hurt to include.\n    \"Authorization\": \"Bearer lm-studio\"\n}\n\nresp = session.post(\n  f\"{BASE_URL}/chat/completions\", json=payload, headers=headers, timeout=60\n)\nresp.raise_for_status()\n\ndata = resp.json()\nlabel = data[\"choices\"][0][\"message\"][\"content\"].strip()\nlabel\n\n'national politics'\n\n\nThis code sends a POST request to the local LM Studio server. The payload follows the OpenAI chat completions format: a system message that sets the model’s behavior, followed by a user message containing the headline to classify. Setting temperature to 0 makes the model’s output deterministic, which is important for reproducible classification.\nBecause our requests_cache session intercepts all HTTP requests, even POST requests to local endpoints are cached automatically. This means if you run the same classification twice, the second call returns instantly from the cache rather than invoking the model again.\nNow let’s classify all of the headlines from our scraped dataset. We’ll loop through each headline, send it to the model, and collect the results.\n\ncategories = []\n\nfor headline in stories_df[\"headline\"]:\n    payload = {\n        \"model\": MODEL,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": f\"Headline: {headline}\\nLabel:\"},\n        ],\n        \"temperature\": 0,\n    }\n\n    resp = session.post(\n        f\"{BASE_URL}/chat/completions\",\n        json=payload,\n        headers=headers,\n        timeout=60,\n    )\n    resp.raise_for_status()\n\n    category = resp.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n    categories.append(category)\n\nWith our classifications complete, we can add them back into the DataFrame using with_columns.\n\nstories_df = stories_df.with_columns(\n    pl.Series(\"category\", categories)\n)\n\nNow we can analyze the distribution of news categories. What topics dominate today’s news cycle?\n\n(\n    stories_df\n    .group_by(c.category)\n    .agg(\n        n = pl.len()\n    )\n    .sort(c.n)\n)\n\n\nshape: (9, 2)\n\n\n\ncategory\nn\n\n\nstr\nu32\n\n\n\n\n\"economics\"\n3\n\n\n\"science\"\n3\n\n\n\"other\"\n5\n\n\n\"sports\"\n8\n\n\n\"culture\"\n8\n\n\n\"local politics\"\n11\n\n\n\"law\"\n13\n\n\n\"national politics\"\n17\n\n\n\"world politics\"\n32\n\n\n\n\n\n\nWith just a few lines of code, we’ve built a pipeline that scrapes current news headlines from the web and classifies them using a large language model—all without manually downloading data or labeling anything by hand. This illustrates the power of combining web requests with modern machine learning tools.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Requests</span>"
    ]
  },
  {
    "objectID": "09_requests.html#sparql-and-wikidata",
    "href": "09_requests.html#sparql-and-wikidata",
    "title": "9  Requests",
    "section": "9.6 SPARQL and Wikidata",
    "text": "9.6 SPARQL and Wikidata\nSo far we’ve fetched data from APIs that return JSON and from web pages that return HTML. A third approach uses SPARQL (pronounced “sparkle”), a query language designed specifically for querying knowledge graphs. If you’re familiar with SQL for relational databases, SPARQL serves a similar purpose for a different type of data structure.\nTraditional relational databases store data in tables with rows and columns. Knowledge graphs instead store data as a network of interconnected facts, where each fact is represented as a triple: a subject, a predicate, and an object. For example, the fact “Paris is the capital of France” would be stored as:\n\nSubject: Paris\nPredicate: is capital of\nObject: France\n\nThis triple structure—also called RDF (Resource Description Framework)—allows knowledge graphs to represent complex, interconnected information flexibly. You can add new types of relationships without modifying a schema, and you can easily traverse connections between entities.\nWikidata is one of the largest freely available knowledge graphs, containing structured data about millions of entities—people, places, organizations, concepts, and more. It powers the information boxes you see on Wikipedia and is freely queryable through the Wikidata Query Service. For an introduction to Wikidata and its data model, see https://www.wikidata.org/wiki/Wikidata:Introduction.\n\nSPARQL_ENDPOINT = \"https://query.wikidata.org/sparql\"\n\nSPARQL will feel somewhat familiar to the SQL queries we saw in Chapter 8, though with important differences. A SPARQL query typically contains the following components. PREFIX declarations define shortcuts for the long URIs (Uniform Resource Identifiers) used to identify entities and properties. Wikidata’s query service provides default prefixes, so we often don’t need to declare them explicitly. The SELECT clause specifies which variables to return, similar to SQL. Variables in SPARQL are prefixed with a question mark, like ?country or ?population. The WHERE block contains triple patterns that describe what we’re looking for. Each pattern has the form subject predicate object, where any component can be a variable. The query engine finds all combinations of values that make the patterns true. Optional clauses handle missing data gracefully—if an entity lacks a particular property, it won’t be excluded from results (similar to a LEFT JOIN in SQL). FILTER, ORDER BY, and LIMIT clauses work much like their SQL counterparts.\nIn Wikidata specifically, you’ll encounter these patterns frequently:\n\nwd:Q... refers to items (entities), where Q followed by a number is the unique identifier. For example, wd:Q6256 represents the concept “country.”\nwdt:P... refers to “truthy” properties—the current, most authoritative value. For example, wdt:P36 means “capital.”\nThe SERVICE wikibase:label clause automatically retrieves human-readable labels for entities in your preferred language.\n\nFor a comprehensive tutorial on SPARQL and Wikidata, see https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial.\nLet’s write a query that retrieves all countries along with their capitals and populations. We’ll break down each component.\n\nquery = \"\"\"\nSELECT ?country ?countryLabel ?capitalLabel ?population WHERE {\n  ?country wdt:P31 wd:Q6256.\n  OPTIONAL { ?country wdt:P36 ?capital. }\n  OPTIONAL { ?country wdt:P1082 ?population. }\n\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n}\nORDER BY ?countryLabel\nLIMIT 500\n\"\"\"\n\nLet’s trace through what this query does. The line ?country wdt:P31 wd:Q6256 finds all entities where property P31 (“instance of”) equals Q6256 (“country”). In plain English: “find all things that are instances of the concept ‘country’.”\nThe OPTIONAL blocks retrieve each country’s capital (P36) and population (P1082). Using OPTIONAL means countries missing this data will still appear in results with null values, rather than being excluded entirely. The SERVICE clause is Wikidata-specific magic that automatically looks up human-readable labels. Without it, we’d get only the Q-numbers and P-numbers, not the actual names. Finally, we order alphabetically by country name and limit to 500 results.\nTo execute the query, we send an HTTP GET request to the Wikidata endpoint. We specify that we want results in JSON format.\n\nheaders = {\n    \"Accept\": \"application/sparql+json\",\n    \"User-Agent\": \"DataScienceBook/1.0 (tarnold2@richmond.edu)\"\n}\n\nresp = session.get(\n    SPARQL_ENDPOINT,\n    params={\"query\": query, \"format\": \"json\"},\n    headers=headers,\n    timeout=30,\n)\nresp.raise_for_status()\nprint(resp.headers[\"Content-Type\"])\n\napplication/sparql-results+json;charset=utf-8\n\n\nThe response comes back as JSON with a specific structure for SPARQL results. We need to parse this into a format suitable for creating a DataFrame.\n\ndata = resp.json()\nvars_ = data[\"head\"][\"vars\"]\nrows = []\n\nfor binding in data[\"results\"][\"bindings\"]:\n    row = {}\n    for var in vars_:\n        if var in binding:\n            row[var] = binding[var][\"value\"]\n        else:\n            row[var] = None\n    rows.append(row)\n\ndf = pl.DataFrame(rows).with_columns(population = c.population.cast(pl.Int64))\ndf\n\n\nshape: (239, 4)\n\n\n\ncountry\ncountryLabel\ncapitalLabel\npopulation\n\n\nstr\nstr\nstr\ni64\n\n\n\n\n\"http://www.wikidata.org/entity…\n\"Afghanistan\"\n\"Kabul\"\n41454761\n\n\n\"http://www.wikidata.org/entity…\n\"Albania\"\n\"Tirana\"\n2811655\n\n\n\"http://www.wikidata.org/entity…\n\"Algeria\"\n\"Algiers\"\n46164219\n\n\n\"http://www.wikidata.org/entity…\n\"Andorra\"\n\"Andorra la Vella\"\n87486\n\n\n\"http://www.wikidata.org/entity…\n\"Angola\"\n\"Luanda\"\n36749906\n\n\n…\n…\n…\n…\n\n\n\"http://www.wikidata.org/entity…\n\"Yemen\"\n\"Sanaa\"\n28250420\n\n\n\"http://www.wikidata.org/entity…\n\"Yemen\"\n\"Aden\"\n28250420\n\n\n\"http://www.wikidata.org/entity…\n\"Zambia\"\n\"Lusaka\"\n19610769\n\n\n\"http://www.wikidata.org/entity…\n\"Zimbabwe\"\n\"Harare\"\n15178979\n\n\n\"http://www.wikidata.org/entity…\n\"sub-Roman Britain\"\nnull\nnull\n\n\n\n\n\n\nThe SPARQL result format includes a head section listing the variable names and a results section containing the actual data. Each “binding” is a dictionary mapping variable names to their values. We iterate through the bindings, extract the values, and construct a list of dictionaries that Polars can convert directly into a DataFrame. Note that we cast the population column to Int64 after creating the DataFrame. SPARQL returns all values as strings, so numeric columns need explicit type conversion.\nThis query demonstrates just a fraction of SPARQL’s power. You can write queries that traverse multiple relationships (find all cities in countries with population over 100 million), aggregate data (count how many Nobel laureates each country has produced), or find paths through the knowledge graph (how is person A related to person B?). The Wikidata Query Service even includes a visual query builder at https://query.wikidata.org/ to help you explore and construct queries.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Requests</span>"
    ]
  },
  {
    "objectID": "09_requests.html#conclusions",
    "href": "09_requests.html#conclusions",
    "title": "9  Requests",
    "section": "9.7 Conclusions",
    "text": "9.7 Conclusions\nThis chapter introduced three powerful techniques for gathering data from the internet: calling web APIs, scraping HTML pages, and querying knowledge graphs with SPARQL. Each approach has its place in a data scientist’s toolkit.\nAPIs are the preferred approach when available. They provide structured data explicitly designed for programmatic access, with documented formats and (usually) stable interfaces. Many organizations now offer APIs for their data, from social media platforms to government agencies to scientific databases.\nWeb scraping fills the gap when no API exists. It’s more fragile than API access—websites change their HTML structure without warning—but it opens up vast amounts of information that would otherwise require manual collection. Use scraping responsibly: cache your requests, rate-limit your access, and respect sites’ terms of service.\nSPARQL and knowledge graphs represent a different paradigm entirely. Rather than thinking in tables, you think in relationships and connections. Wikidata’s comprehensive knowledge base, freely available and constantly updated by a global community, makes it an invaluable resource for augmenting your datasets with contextual information.\nWe also saw how HTTP-based communication extends beyond remote servers to local services. Running large language models locally through tools like LM Studio gives you privacy, cost savings, and educational insight into how these systems work—all while using the same requests-based code patterns you’d use for cloud APIs.\nAs you work on your own projects, you’ll likely combine these techniques. You might scrape a list of company names from a web page, look up additional information about each company from Wikidata, call an API for their stock prices, and use a local language model to summarize the results. The common thread is HTTP: a simple, universal protocol that lets your Python code communicate with services anywhere—whether across the internet or running on your own machine.",
    "crumbs": [
      "Part II: Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Requests</span>"
    ]
  },
  {
    "objectID": "10_inference.html",
    "href": "10_inference.html",
    "title": "10  Inference",
    "section": "",
    "text": "10.1 Setup\nLoad all of the modules and datasets needed for the chapter. There are a number of relatively small datasets specifically designed to satisfy the assumptions of the statistical models presented below. As always, full details for each can be found in Chapter 22.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nmarriage = pl.read_csv(\"data/inference_age_at_mar.csv\")\nabsent = pl.read_csv(\"data/inference_absenteeism.csv\")\nsulph = pl.read_csv(\"data/inference_sulphinpyrazone.csv\")\nspeed = pl.read_csv(\"data/inference_speed_sex_height.csv\")\npossum = pl.read_csv(\"data/inference_possum.csv\")",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#introduction",
    "href": "10_inference.html#introduction",
    "title": "10  Inference",
    "section": "10.2 Introduction",
    "text": "10.2 Introduction\nThroughout this text, we have focused on methods for organizing, visualizing, and describing datasets. These techniques form the foundation of exploratory data analysis, where the goal is to understand the structure and patterns present in the data we have collected. In this chapter, we turn to a different but equally important task: using data to make claims about the world beyond our immediate observations. This is the domain of statistical inference, a set of methods that allow us to draw conclusions about populations based on samples.\nThe distinction between a sample and a population is fundamental to everything that follows. A population consists of all individuals or observations we are interested in studying. This might include every adult in a country, every manufactured part from a factory, or every possible measurement of a particular phenomenon. In most practical situations we cannot observe the entire population directly. Instead, we collect a sample — a subset of the population that we measure and analyze. The central question of statistical inference is how to use what we learn from a sample to make inferences about the population from which it was drawn.\nBefore we can engage with inference, however, we need to establish a common vocabulary for describing the basic properties of data. The next two sections introduce measures of central tendency and variation, which are the building blocks of nearly all statistical analysis. Even if you have encountered these concepts before, it is worth reviewing them carefully, as a precise understanding of these measures is essential for interpreting the inferential methods that follow.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#measuring-central-tendency",
    "href": "10_inference.html#measuring-central-tendency",
    "title": "10  Inference",
    "section": "10.3 Measuring Central Tendency",
    "text": "10.3 Measuring Central Tendency\nWhen we collect numerical data, one of the first questions we ask is what value best represents the dataset as a whole. This notion of a typical value is captured by measures of central tendency, which summarize the location of a distribution’s center. The three most common measures are the mean, the median, and quantiles.\nThe mean, commonly called the average, is computed by summing all the values in a dataset and dividing by the number of observations. If we have n observations denoted \\(x_1, x_2, \\dots, x_n\\), the mean is the sum of all \\(x_i\\) divided by \\(n\\). We typically denote the sample mean by \\(\\bar{x}\\) and write the formula as follows.\n\\[\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n\\]\nThe mean has the appealing property of using every observation in the dataset and represents the balance point of the data: if you placed the observations on a number line and put a fulcrum at the mean, the line would balance perfectly.\n\n\n\n\n\n\nNote on Equations\n\n\n\n\n\nI originally wrote this chapter with almost no mathematical notation in order to be as accessible as possible to all readers. After teaching the material several times, I noticed that the lack of precise definitions made the text difficult for those more accustomed to seeing mathematical descriptions of each concept, so I added them back into the text. I suggest that all readers treat the mathematical definitions as a reference for the textual descriptions of each method. However, if you ultimately find the mathematical definitions hard to follow, your understanding should be fine if you focus only on the text and code.\n\n\n\nThe median is the middle value when all observations are arranged in order from smallest to largest. If we denote the ordered observations as \\(x_{(1)}\\), \\(x_{(2)}\\), …, \\(x_{(n)}\\), where \\(x_{(1)}\\) is the smallest and \\(x_{(n)}\\) is the largest, the median depends on whether \\(n\\) is odd or even. When \\(n\\) is odd, we take the middle value as the median; when \\(n\\) is even, we take the average of the two middle values. As an equation, we can write the median as follows.\n\\[\n\\text{median} =\n\\begin{cases}\nx_{((n+1)/2)} & \\text{if } n \\text{ is odd},\\\\\n\\frac{1}{2}\\left(x_{(n/2)} + x_{(n/2+1)}\\right) & \\text{if } n \\text{ is even}.\n\\end{cases}\n\\]\nUnlike the mean, the median is not influenced by extreme values, making it a more robust measure of central tendency when dealing with data that may contain outliers or have a skewed distribution.\nQuantiles generalize the concept of the median to other positions in the ordered data. For a given proportion \\(p\\) between \\(0\\) and \\(1\\), the \\(p\\)-th quantile is the value \\(Q(p)\\) such that a fraction \\(p\\) of the observations fall at or below \\(Q(p)\\). If we denote the ordered observations as \\(x_{(1)}\\), \\(x_{(2)}\\), …, \\(x_{(n)}\\), we can write this formally as follows.\n\\[\nQ(p) = x_{(\\lceil np \\rceil)}\n\\]\nHere, the ceiling function returns the smallest integer greater than or equal to n * p. For example, if we have n = 100 observations and want the 0.20 quantile (p = 0.20), then n * p = 20, so the 0.20 quantile is the 20th smallest observation in the ordered data. The median is the special case p = 0.5, giving the observation at position ceiling(n/2) in the ordered data.\nThe median is approximately the 0.50 quantile, meaning that 50 percent of the observations fall below it. Similarly, the 0.20 quantile is the value below which 20 percent of the observations fall. Quantiles are useful for understanding the shape of a distribution beyond its center.\nThe code below demonstrates how to compute these measures using Polars. It uses the dataset marriage, which has a single column indicating the age at first marriage for 5,534 U.S. women from the 2010 National Survey of Family Growth (NSFG) conducted by the CDC. We calculate the mean, median, and 0.20 quantile of the age variable in the marriage dataset.\n\n(\n    marriage\n    .select(\n        mu=c.age.mean(),\n        med=c.age.median(),\n        q20=c.age.quantile(0.20)\n    )\n)\n\n\nshape: (1, 3)\n\n\n\nmu\nmed\nq20\n\n\nf64\nf64\nf64\n\n\n\n\n23.440188\n23.0\n19.0\n\n\n\n\n\n\nIn this example, the mean and median are fairly close, suggesting the distribution of ages at marriage is roughly symmetric. When the two measures differ substantially, it often indicates the data are skewed, with the mean pulled toward the longer tail. To better understand the distribution, we can visualize it.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#measuring-variation",
    "href": "10_inference.html#measuring-variation",
    "title": "10  Inference",
    "section": "10.4 Measuring Variation",
    "text": "10.4 Measuring Variation\nKnowing the center of a distribution tells only part of the story. Two datasets can have the same mean but look very different: one may have values clustered tightly around the mean, while the other has values spread widely. Measures of variation quantify this spread, indicating how much the observations differ from one another and from the center.\nThe variance is the most fundamental measure of spread. It is computed by taking the difference between each observation and the mean, squaring these differences, and then averaging the squared differences. The squaring serves two purposes: it ensures that positive and negative deviations do not cancel each other out, and it gives greater weight to observations that are far from the mean. We denote the sample variance by s^2 and write it as follows:\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\n\n\n\n\n\n\nBessel’s correction\n\n\n\n\n\nYou may notice that we divide by n − 1 rather than n. This adjustment, known as Bessel’s correction, makes the sample variance an unbiased estimator of the population variance. Intuitively, this happens because we lose one degree of freedom when we use the sample mean in our calculation instead of the true population mean. Many statistics textbooks spend a disproportionate amount of time discussing this correction, which has very little effect for reasonable sample sizes. I suggest not worrying too much about the correction.\n\n\n\nWhile the variance captures the spread of the data, it has an awkward property of being expressed in squared units. If the original data are measured in years, the variance is measured in years squared, which is difficult to interpret directly. The standard deviation addresses this problem by taking the square root of the variance, returning the measure of spread to the original units of measurement.\n\\[\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\]\nThe standard deviation measures the typical distance of observations from the mean.\nA useful heuristic for interpreting the standard deviation comes from considering normal (bell-shaped) distributions. For data that approximately follow this shape, roughly 68 percent of observations fall within plus or minus one standard deviation of the mean, about 95 percent fall within two standard deviations of the mean, and nearly all observations fall within three standard deviations of the mean. While this rule of thumb does not apply exactly to all distributions, it provides a useful starting point for thinking about what the standard deviation tells us.\nWe can compute the standard deviation and variance using the .select method in Polars along with the summary methods .std() and .var(). Note that we would use .agg to compute these statistics within specific groups of the data.\n\n(\n    marriage\n    .select(\n        std=c.age.std(),\n        var=c.age.var()\n    )\n)\n\n\nshape: (1, 2)\n\n\n\nstd\nvar\n\n\nf64\nf64\n\n\n\n\n4.721365\n22.291289\n\n\n\n\n\n\nThe output shows both the standard deviation and the variance for the age-at-marriage data. Note that the variance is the square of the standard deviation. In practice, we usually report and interpret the standard deviation because it has more intuitive units (the same units as the original data).",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#statistical-inference",
    "href": "10_inference.html#statistical-inference",
    "title": "10  Inference",
    "section": "10.5 Statistical Inference",
    "text": "10.5 Statistical Inference\nThe goal of statistical inference is to learn about a large population based on a sample drawn from that population. Generally, we can compute nearly any summary statistic from the sample data we have collected. The question is how to infer the population from the sample. We can never be certain about the exact relationship, because, due to random chance, our sample might not be representative of the larger population. Therefore, we focus on probabilistic statements about what would happen for most samples we could draw from the population.\nThe biggest challenge in understanding statistical inference is the need to describe probabilities precisely. This can only be done rigorously through the formal structures of a calculus-based probability course. Here we will do our best using analogies, without spending months developing the underlying mathematical structure of probability. To make this more accessible, we will focus on a specific inference task: measuring the difference in means between two populations.\nConsider a sample of the heights of 45 randomly selected Virginia residents. After summarizing the heights, we find that the 15 residents who played college athletics have an average height of 181 centimeters, and the other 30 residents who did not play college athletics have an average height of 172 centimeters. Knowing nothing else, our best guess for the difference in athlete and non-athlete heights in the entire state is 9 centimeters. This best guess is what we would call the point estimate of the unknown difference between the means. The goal of statistical inference is to determine to what extent this sample difference can serve as a good approximation of the true population difference in heights. There are generally two different approaches to doing this.\n\n\n\n\n\n\nFigure 10.1: Visualization of the statistical inference process, showing a sample of measured heights at the top used to estimate the true difference in means for the larger population at the bottom.\n\n\n\nA confidence interval is a range of values used to estimate a population quantity from sample data. Confidence intervals are reported with a corresponding confidence level, most commonly 95 percent. If we denote the unknown population parameter as \\(\\theta\\) and construct a confidence interval with lower bound \\(L\\) and upper bound \\(U\\), then a 95 percent confidence interval satisfies the following property. The symbol \\(\\mathbb{P}(\\cdots)\\) is used as a compact way of writing “the probability of \\(\\cdots\\)”.\n\\[\n\\mathbb{P}(L \\leq \\theta \\leq U) = 0.95\n\\]\nThis statement means that the method of calculating the interval will produce intervals that contain the true value in 95% of repeated experiments. This is a subtle but important point: the confidence level refers to the reliability of the procedure over many repetitions, not to the probability that any particular interval contains the true value. Once we compute a specific interval from our data, the true parameter either is in that interval or not; the probability statement applies to the method, not to the specific result.\nThe other approach is to use a statistical hypothesis test. We start with the null hypothesis, denoted \\(H_0\\). This is a statement about the data that we test to see whether there is strong evidence against it. In the case of our example above, a typical null hypothesis would be that there is no difference between the heights of athletes and non-athletes. The hypothesis test provides a p-value, which we can write formally as follows.\n\\[\n\\text{p-value} = \\mathbb{P}(\\text{observing data at least as extreme as the observed data} \\mid H_0)\n\\]\nThe p-value is the probability of obtaining test results at least as extreme as those observed, assuming the null hypothesis is true. If the p-value is sufficiently small (common cutoffs are 0.05 or 0.01), we reject the null hypothesis; otherwise, we fail to reject it.\nIt is worth pausing to understand what a p-value does and does not tell us. A small p-value indicates that the observed data would be unlikely if the null hypothesis were true and thus provides evidence against the null. However, a p-value is not the probability that the null hypothesis is true, nor is it the probability that we have made an error by rejecting the null hypothesis. These common misinterpretations can lead to flawed reasoning, so it is important to keep the precise definition in mind.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#inference-for-one-mean",
    "href": "10_inference.html#inference-for-one-mean",
    "title": "10  Inference",
    "section": "10.6 Inference for One Mean",
    "text": "10.6 Inference for One Mean\nThe simplest inferential problem arises when we want to make claims about the mean of a single population based on a sample. Suppose we have collected data on the age at first marriage from a sample of individuals and want to estimate the true average age at first marriage in the population. The sample mean provides a point estimate, but we would like to know how much uncertainty surrounds that estimate.\nThe one-sample t-test addresses this question. The test is based on a quantity called the t-statistic, which measures how far the sample mean is from a hypothesized population mean, divided by the standard error of the mean. If we denote the hypothesized population mean by \\(\\mu_0\\) (often zero), the t-statistic is computed as follows:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\nThe denominator, s divided by the square root of n, is called the standard error of the mean. It represents the standard deviation of the sampling distribution of the sample mean and decreases as the sample size increases. Larger samples provide more precise estimates.\nUnder certain assumptions, most importantly that the sample is large enough, the t-statistic follows a known probability distribution called the t-distribution with \\(n-1\\) degrees of freedom. This allows us to compute p-values and construct confidence intervals because we understand what values of \\(t\\) are reasonable under the assumption that the null hypothesis is true. The confidence interval for the population mean takes the form shown below, where \\(t^*\\) is a critical value from the t-distribution corresponding to the desired confidence level. When \\(n\\) is large and the confidence level is set at \\(0.95\\), the value of \\(t^*\\) will be close to \\(1.96\\).\n\\[\n\\bar{x} \\pm t^* \\cdot \\frac{s}{\\sqrt{n}}\n\\]\nTo apply a one-sample t-test, we will use the ttest1 method from a wrapper class called DSStatsmodels, which uses the Python statsmodels package. The model-fitting process is designed to be used as part of a chain of methods called via the .pipe function, as we did in Chapter 3 with ggplot. The model-fitting function takes a string that defines a formula using the consistent model description we will use throughout this chapter. A formula has a column name followed by the ~ (tilde) symbol followed by one or more additional column names. The idea is that we are trying to “predict” the first column in terms of zero or more of the other columns. In the case of a one-sample t-test, there are no predictor variables; therefore we use the notation 1 to indicate that the left-hand column is modeled using only a single mean.\n\n(\n    marriage\n    .pipe(DSStatsmodels.ttest1, \"age ~ 1\")\n)\n\n\nshape: (1, 5)\n\n\n\nmean\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n23.440188\n369.328715\n0.0\n23.315768\n23.564608\n\n\n\n\n\n\nThe output includes the sample mean, a confidence interval for the population mean, and a p-value testing whether the population mean equals zero. In most practical applications, that null hypothesis is not especially informative. The confidence interval is more valuable: it provides a range of plausible values for the true population mean based on the sample.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#inference-for-two-means",
    "href": "10_inference.html#inference-for-two-means",
    "title": "10  Inference",
    "section": "10.7 Inference for Two Means",
    "text": "10.7 Inference for Two Means\nA common inferential task is comparing the means of two groups. We might want to know whether average test scores differ between students who received a new teaching intervention and those who did not, or whether average recovery times differ between patients receiving two different treatments. The two-sample t-test provides the framework for addressing such questions.\nThe logic of the two-sample t-test parallels that of the one-sample test. We compute the difference in sample means between the two groups, then assess how large this difference is relative to what we would expect from random sampling variation alone. If the two groups have sample means \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\), sample sizes \\(n_1\\) and \\(n_2\\), and sample standard deviations \\(s_1\\) and \\(s_2\\), the t-statistic for the two-sample test is:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\nThe denominator is called the standard error of the difference between means. It accounts for the variability in both groups. If the observed difference is large compared to this expected variation, we have evidence that the population means differ.\nThe null hypothesis for a two-sample t-test is typically that the two population means are equal, or equivalently, that the difference in population means is zero. We can write this as \\(H_0: \\mu_1 = \\mu_2\\). A small p-value provides evidence against this null hypothesis, suggesting that the groups differ.\nTo illustrate the two-sample t-test, we will use a historical dataset of absenteeism in a school district in Australia. Here is the full description from the source:\n\nResearchers, interested in the relationship between school absenteeism and certain demographic characteristics of children, collected data from 146 randomly sampled students in rural New South Wales, Australia, during a particular school year.\n\nThe dataset includes the number of days a student was absent during a school year, along with demographic data such as sex, age (in buckets), and learning status. We will use a two-sample t-test to determine whether absenteeism differs significantly between male and female students. We again use DSStatsmodels, this time the method .ttest2. The formula now has both a left-hand side (the column whose mean we’re interested in) and a right-hand side that indicates the two groups the data are divided into.\n\n(\n    absent\n    .pipe(DSStatsmodels.ttest2, \"days ~ sex\")\n)\n\n\nshape: (1, 9)\n\n\n\nlevel1\nlevel2\nmean1\nmean2\nmean_diff\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"F\"\n\"M\"\n15.225\n17.954545\n2.729545\n1.01\n0.314189\n-2.612188\n8.071278\n\n\n\n\n\n\nThe output shows the difference in mean days of absenteeism between males and females, along with a confidence interval for this difference and a p-value for the null hypothesis of no difference. The confidence interval is particularly useful because it not only indicates whether there is a statistically significant difference but also provides a sense of the magnitude of that difference. In this example, there is no statistically significant evidence of a difference in absenteeism based solely on the student’s sex.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#inference-for-several-means",
    "href": "10_inference.html#inference-for-several-means",
    "title": "10  Inference",
    "section": "10.8 Inference for Several Means",
    "text": "10.8 Inference for Several Means\nWhen comparing means across more than two groups, the t-test is no longer appropriate. Instead, we use a technique called a one-way analysis of variance, commonly abbreviated as ANOVA. Despite its name, ANOVA is fundamentally a method for comparing means rather than variances. The name comes from the mathematical machinery underlying the test, which partitions the total variability in the data into components associated with between-group differences and within-group random variation.\nThe null hypothesis in ANOVA is that all group means are equal: for \\(k\\) groups with population means \\(\\mu_1,\\ldots,\\mu_k\\), the null hypothesis is \\(\\mu_1=\\mu_2=\\cdots=\\mu_k\\).\n\\[\nH_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k\n\\]\nThe alternative hypothesis is that at least one group mean differs from the others. Notice that rejecting the null hypothesis tells us only that the groups are not all the same; it does not indicate which specific groups differ. Additional analyses, called post-hoc tests, are often needed to determine where the differences lie.\nANOVA relies on the F-statistic, which compares variability between group means to variability within groups. The F-statistic is computed as the ratio of two mean-square estimates: the between-group mean square (MSB) and the within-group mean square (MSW).\n\\[\nF = \\frac{\\text{MS}_{\\text{between}}}{\\text{MS}_{\\text{within}}}\n  = \\frac{\\text{between-group variance}}{\\text{within-group variance}}\n\\]\nIf between-group variability is large relative to within-group variability, the F-statistic will be large and the p-value small, providing evidence against the null hypothesis that the group means are equal.\nWe will again use the absenteeism data to illustrate the one-way ANOVA test by determining whether the number of days absent is related to the students’ age bucket (F0, F1, F2, and F3). To do this, we use the DSStatsmodels class with the .anova() method. The formula works exactly as it did in the two-sample case: put the column whose mean you want to compare on the left-hand side and the column that describes the groups on the right-hand side.\n\n(\n    absent\n    .pipe(DSStatsmodels.anova, \"days ~ age\")\n)\n\n\nshape: (2, 5)\n\n\n\nindex\nsum_sq\ndf\nF\nPR(&gt;F)\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"age\"\n2535.132447\n3.0\n3.354745\n0.020744\n\n\n\"Residual\"\n35769.120978\n142.0\nnull\nnull\n\n\n\n\n\n\nThe output provides the F-statistic and the associated p-value for the test of whether mean absenteeism differs across age groups. The p-value is 0.02, which indicates that the observed variation is unlikely to have arisen from a population in which absenteeism is unrelated to age group. A visualization can help us understand the pattern of differences.\n\n(\n    absent\n    .pipe(ggplot, aes(\"age\", \"days\"))\n    + geom_boxplot()\n)\n\n\n\n\n\n\n\n\nThe boxplot shows the distribution of days absent for each age group, making it easier to see which groups tend to have higher or lower numbers of absent days. While ANOVA indicates whether statistically significant differences exist, the visualization helps us understand the nature and direction of those differences.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#chi-squared-test",
    "href": "10_inference.html#chi-squared-test",
    "title": "10  Inference",
    "section": "10.9 Chi-squared Test",
    "text": "10.9 Chi-squared Test\nThe inferential methods we’ve discussed so far all involve comparing means of numerical variables. When both variables of interest are categorical, we need a different approach. The chi-squared test (also written chi-square or represented by the Greek letter χ²) assesses whether there is an association between two categorical variables.\nWe can understand how the test works by constructing a contingency table that shows the counts of observations for each combination of categories. As an example, we use experimental data that study the efficacy of treating patients who had a heart attack with a drug called sulphinpyrazone. The data indicate whether a patient was in the treatment group (given the drug) or the control group (given a placebo), and whether the patient lived or died. Using Polars functions, we can build a table showing the number of patients in each group by outcome.\n\n(\n    sulph\n    .group_by(c.outcome, c.group)\n    .agg(n = pl.len())\n    .pivot(index=c.outcome, columns=c.group, values=c.n)\n)\n\n\nshape: (2, 3)\n\n\n\noutcome\ntreatment\ncontrol\n\n\nstr\nu32\nu32\n\n\n\n\n\"lived\"\n692\n682\n\n\n\"died\"\n41\n60\n\n\n\n\n\n\nThe chi-squared test compares the observed counts in each cell to the counts expected if the two variables are independent. Under independence, knowing the value of one variable tells us nothing about the likely value of the other. If we denote the observed count in row \\(i\\) and column \\(j\\) by \\(O_{ij}\\) and the expected count under independence by \\(E_{ij}\\), the chi-squared statistic is computed as follows:\n\\[\n\\chi^2 = \\sum_{i,j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\n\n\n\n\n\n\nExpected counts\n\n\n\n\n\nThe expected counts \\(E_{ij}\\) can be calculated using some basic probability theory. Let \\(R_i\\) be the total number of observations in row \\(i\\), \\(C_j\\) be the total number of observations in column \\(j\\), and \\(N\\) the total number of observations. The proportion of data in this row is \\(R_i / N\\) and the proportion of data in this column is \\(C_j / N\\). Probability theory states that, for independent events, the probability of both occurring is the product of their individual probabilities. Therefore, we would expect the probability that a specific observation falls in row \\(i\\) and column \\(j\\) to be \\((R_i \\cdot C_j) / N^2\\). To get the expected count, we multiply this by the number of observations (\\(N\\)) to obtain the formula \\(E_{ij} = (R_i \\cdot C_j) / N\\).\n\n\n\nWe can compute the chi-squared test using the .chi2() method of DSStatsmodels and specifying the two variables in the formula. Below is an example with the sulphinpyrazone dataset.\n\n(\n    sulph\n    .pipe(DSStatsmodels.chi2, \"outcome ~ group\")\n)\n\n\nshape: (1, 3)\n\n\n\nχ²\ndf\nP&gt;|χ²|\n\n\nf64\ni64\nf64\n\n\n\n\n3.212077\n1\n0.073097\n\n\n\n\n\n\nThe output provides the chi-squared statistic and a p-value (0.07). The p-value is not sufficiently small to indicate evidence that the drug improves patient outcomes. Note that the order of the left- and right-hand sides of the equation is not important; flipping them produces the exact same result.\nThe G-test (also called the log-likelihood ratio test) is an alternative to the chi-squared test for contingency tables. Like the chi-squared test, it compares observed counts to expected counts under the assumption of independence, but it uses a different formula based on the likelihood ratio. The G-statistic is computed as follows:\n\\[\nG = 2 \\sum_{i,j} O_{ij}\\log\\left(\\frac{O_{ij}}{E_{ij}}\\right)\n\\]\nThe expected counts are calculated in the same way as for the chi-squared test. The factor of 2 in front of the summation ensures that the G-statistic approximately follows the same distribution as the chi-squared statistic under the null hypothesis, allowing us to use the same critical values and p-value calculations. We can replace the method “.chi2” with “.gtest” to obtain the G-test result.\n\n(\n    sulph\n    .pipe(DSStatsmodels.gtest, \"outcome ~ group\")\n)\n\n\nshape: (1, 3)\n\n\n\nG\ndf\nP&gt;|G|\n\n\nf64\ni64\nf64\n\n\n\n\n3.229159\n1\n0.072338\n\n\n\n\n\n\nIn practice, the G-test and chi-squared test typically give very similar results, especially for large samples. For example, this is evident in the output above. The advantages of the G-test become clearer when some rows or columns are rare. We will see a specific application where it is particularly useful Chapter 19. However, the chi-squared test is better known in the sciences and social sciences and is therefore the recommended method for reporting results of statistical inference tests on the relationship between two categorical variables.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#simple-ols",
    "href": "10_inference.html#simple-ols",
    "title": "10  Inference",
    "section": "10.10 Simple OLS",
    "text": "10.10 Simple OLS\nWhile the methods above focus on comparing group means or testing for associations, many research questions involve understanding the relationship between two numerical variables. Ordinary least squares (OLS) regression provides a framework for modeling such relationships. In its simplest form, simple linear regression describes how one variable (the response or dependent variable) varies as another (the predictor or independent variable) changes.\nThe basic idea is to fit a straight line to the data that best captures the relationship between the variables. The equation for this line takes the form shown below, where \\(y\\) is the response variable, \\(x\\) is the predictor variable, \\(\\alpha\\) is the y-intercept (the predicted value of \\(y\\) when \\(x\\) equals zero), and \\(\\beta\\) is the slope (the predicted change in \\(y\\) for a one-unit increase in \\(x\\)).\n\\[\ny = \\alpha + \\beta x + \\varepsilon\n\\]\nThe term \\(\\varepsilon\\) denotes the error (residual), which captures each observation’s deviation from the fitted line. The least-squares method finds the values of \\(\\alpha\\) and \\(\\beta\\) that minimize the sum of squared residuals.\n\\[\n\\min_{\\alpha,\\beta} \\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)^2\n\\]\nLeast squares refers to the method used to find the best-fitting line. Among all possible lines we could draw through the data, the least squares line is the one that minimizes the sum of the squared vertical distances from each data point to the line. These distances are called residuals; squaring them ensures that positive and negative deviations are treated equally while giving greater weight to larger deviations. The solutions to this minimization problem can be written in closed form. We will write these down for reference, but note that it is difficult to gain much insight into the mathematics here without performing the derivation yourself.\n\\[\n\\hat{\\beta} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}, \\quad \\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\bar{x}\n\\]\nThere are two separate hypothesis tests that we can construct with a simple linear regression. We can test the null hypothesis \\(H_0: \\alpha = 0\\) (the intercept is zero) and the null hypothesis \\(H_0: \\beta = 0\\). The second is usually the most important because it tests whether there is a change in the mean of the response \\(y\\) when there is a change in the predictor \\(x\\). We will not go into the specific formulas for these hypothesis tests, but note that they take the form of a \\(t\\)-distribution with \\(n-2\\) degrees of freedom.\nTo illustrate the application of a linear regression model, we will use a survey dataset of 1,325 UCLA students who completed a survey asking about their height, their fastest driving speed, and their gender. We will fit a model predicting their fastest driving speed (MPH) as a function of their height (inches).\nOnce again, running linear regression involves using DSStatsmodels function, this time with the .ols method. The formula object specifies the response variable on the left-hand side and the predictor variable on the right. This is intuitive because the formula format comes directly from the standard way of writing the equation of a line in mathematics (\\(y = mx + b\\)).\n\n(\n    speed\n    .pipe(DSStatsmodels.ols, \"speed ~ height\")\n)\n\n\nshape: (2, 7)\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Intercept\"\n10.3958\n8.393\n1.239\n0.216\n-6.07\n26.861\n\n\n\"height\"\n1.2391\n0.127\n9.787\n0.0\n0.991\n1.487\n\n\n\n\n\n\nThe output shows the estimated intercept and slope, along with standard errors, t-statistics, and p-values for each coefficient. The p-value for the slope tests the null hypothesis that there is no linear relationship between the predictor and response (that is, that the true slope equals zero). As usual, a small p-value provides evidence that the relationship is statistically significant. Here we see a very small p-value (when rounded, it equals zero), indicating strong evidence of a relationship between top-driven speed and height.\nWe already discussed the idea of a line of best fit in Chapter 3, where we used the geom_smooth function to fit a line to a scatterplot. This line is produced by the same method shown in the model above. Let’s visualize this line here to compare it to the model.\n\n(\n    speed\n    .pipe(ggplot, aes(\"height\", \"speed\"))\n    + geom_point(color=\"#bbbbbb\")\n    + geom_smooth(method=\"lm\", se=False)\n)\n\n\n\n\n\n\n\n\nThe visualization shows the data points along with the fitted regression line. Notice that, as is often true, there is no direct interpretation of the value \\(\\alpha\\); it would represent the speed of someone with a height of zero. The interpretation of \\(\\beta\\) directly addresses the study question: how much faster, on average, is a person’s top driving speed for each additional inch of height?",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#multivariate-ols",
    "href": "10_inference.html#multivariate-ols",
    "title": "10  Inference",
    "section": "10.11 Multivariate OLS",
    "text": "10.11 Multivariate OLS\nSimple linear regression captures the relationship between one predictor variable and one response variable. Real-world relationships are often more complex. Multiple regression extends the linear model to include several predictors simultaneously, allowing us to examine the relationship between each predictor and the response while controlling for the effects of the other predictors.\nThe model extends to include multiple predictors as follows: we have p predictor variables \\(x_1\\) through \\(x_p\\), each with its own coefficient \\(\\beta_j\\).\n\\[\ny = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k + \\varepsilon\n\\]\nEach coefficient beta_j represents the expected change in y for a one-unit increase in x_j, holding all other predictors constant. This “holding constant” interpretation is crucial: it means we are isolating the unique contribution of each predictor after accounting for the others.\nLet’s look at a dataset collected from 104 possums captured in Australia and New Guinea. The dataset includes measurements of the sizes of parts of their bodies and location data about where they were captured. We will try to predict the width of the possum’s skull based on other body measurements. Using simple linear regression, we see a positive relationship between skull width and tail length: each additional millimeter of tail length is associated with about 0.4 millimeters of extra skull width.\n\n(\n    possum\n    .pipe(DSStatsmodels.ols, \"skull_w ~ tail_l\")\n)\n\n\nshape: (2, 7)\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Intercept\"\n41.8346\n5.636\n7.422\n0.0\n30.655\n53.014\n\n\n\"tail_l\"\n0.4066\n0.152\n2.674\n0.009\n0.105\n0.708\n\n\n\n\n\n\nIn order to run a multiple regression model, we add terms (with a plus sign) to the right-hand side of the formula object. The resulting table provides the estimates of \\(\\alpha\\) and all of the corresponding \\(\\beta_j\\)s. Let’s add a variable for the possum’s total length in addition to tail length.\n\n(\n    possum\n    .pipe(DSStatsmodels.ols, \"skull_w ~ total_l + tail_l\")\n)\n\n\nshape: (3, 7)\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Intercept\"\n25.2\n5.826\n4.325\n0.0\n13.643\n36.757\n\n\n\"total_l\"\n0.4054\n0.074\n5.48\n0.0\n0.259\n0.552\n\n\n\"tail_l\"\n-0.0978\n0.163\n-0.601\n0.549\n-0.421\n0.225\n\n\n\n\n\n\nThe result here is quite interesting and illustrates a challenge when interpreting multivariate models. We see that an increase in body length of 1 millimeter leads to an average increase in skull width of about 0.4 millimeters. However, the coefficient for tail length (-0.0978) is completely different: it is actually negative. Why would possums with longer tails have smaller skulls? The reason is that the coefficient indicates the change in skull width with all other variables held fixed. If we increase tail length while keeping total length fixed, that implies body length must decrease. From this perspective, in the language of multivariate regression, the negative sign becomes more reasonable. This example still highlights the difficulty of interpreting multivariate regression coefficients, particularly when there is no physical relationship between the variables to help clarify the situation, as we have in this case.\n\n\n\n\n\n\nOrder of coefficents\n\n\n\n\n\nThe order of the coefficients on the right-hand side of the formula object for a linear model has no effect on the results other than the ordering of the coefficients in the table. Regardless of the order and the number of terms, the interpretation is always the same: \\(\\beta_j\\) indicates the expected change in the mean response associated with a one-unit change in the jth predictor, with all other predictor variables held fixed.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#indicator-variables",
    "href": "10_inference.html#indicator-variables",
    "title": "10  Inference",
    "section": "10.12 Indicator Variables",
    "text": "10.12 Indicator Variables\nIn many cases, we want to fit a multivariate regression model where some predictors are categorical rather than numeric. Fortunately, this is an easy challenge to overcome.\nConsider again the speed dataset, where we are trying to estimate the average maximum speed a student reported driving based on demographic information. We already used height alone and found that taller students are more likely to report driving at higher speeds. Male students tend to be taller and are sometimes thought—particularly when younger—to be a bit more reckless on average. It’s possible that the relationship with height exists only because it’s acting as a proxy for student sex. A multivariate regression could help here, but how do we include sex in the model? A common technique is to create a numeric variable that’s 1 for one category and 0 for the other. We will create a variable called is_male and add it to the model as follows.\n\n(\n    speed\n    .with_columns(\n        is_male = (c.sex == \"male\").cast(pl.Int64)\n    )\n    .pipe(DSStatsmodels.ols, \"speed ~ is_male + height\")\n)\n\n\nshape: (3, 7)\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Intercept\"\n33.6458\n10.326\n3.258\n0.001\n13.388\n53.903\n\n\n\"is_male\"\n5.2458\n1.371\n3.826\n0.0\n2.556\n7.935\n\n\n\"height\"\n0.8608\n0.16\n5.376\n0.0\n0.547\n1.175\n\n\n\n\n\n\nIn the results, we see that the is_male coefficient is 5.2458. In the prediction, this coefficient is added to any prediction where the student is male (since is_male equals 1) and ignored for students who are female. This means that its statistical test and associated p-value (here, 0.0) test the following null hypothesis: “\\(H_0\\): sex has no effect on maximum driving speed after accounting for height”. The height coefficient is smaller (1.2391 -&gt; 0.8608), but still positive. Its p-value (also 0.0) tests the null hypothesis: “\\(H_0\\): height has no effect on driving speed after accounting for sex”. The results show that there is strong evidence to reject both null hypotheses. In other words, it seems that both height and being male are associated with an increase in the maximum reported driving speed.\nThe new column is_male that we created above is called an indicator variable because it indicates (with a 0 or 1) whether a certain feature is present. This is such a common approach when working with categorical variables in regression that statsmodels lets us include categorical variables directly in the model and will create the indicator variables for us. Let’s repeat the above analysis by including height and sex directly in the model.\n\n(\n    speed\n    .pipe(DSStatsmodels.ols, \"speed ~ height + sex\")\n)\n\n\nshape: (3, 7)\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Intercept\"\n33.6458\n10.326\n3.258\n0.001\n13.388\n53.903\n\n\n\"sex[T.male]\"\n5.2458\n1.371\n3.826\n0.0\n2.556\n7.935\n\n\n\"height\"\n0.8608\n0.16\n5.376\n0.0\n0.547\n1.175\n\n\n\n\n\n\nNotice that the values are identical. The only difference is the name of the indicator variable (sex[T.male]), which has a specific form that makes it easy to identify the original variable (sex) and the category the indicator represents.\n\n\n\n\n\n\nWhy no female term?\n\n\n\n\n\nA natural and common question about the above example is why there is no term for female students. It seems like they are being left out. This is not the case, however, because they are fully captured by the intercept term (33.6458). Male students simply have a different starting point (33.6458 + 5.2458 = 38.8916) before accounting for height. An equivalent model would use an is_female indicator with an intercept of 38.8916 and a female coefficient of -5.2458 (the p-value is the same, and the t-statistic and confidence-interval bounds have the opposite sign). You could also imagine a model with both is_male and is_female variables and no global intercept. That would make the two starting values explicit, but it has a serious downside: you lose the direct t-test of the difference between the two groups (often the parameter of primary interest). So the setup here is the standard one. By default, the alphabetically first group becomes the reference (starting point) and the other group becomes the offset.\n\n\n\n\n\n\n\n\n\nThree or more categories\n\n\n\n\n\nWe can use the same approach with a categorical variable that has three or more categories. The output will include indicator variables for all but one category (by default, the first alphabetically). This works well when the goal is to control for that categorical variable while focusing on one or more other coefficients. It is not recommended to directly interpret the coefficients of the indicator variables themselves for statistical inference when there are more than two categories.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#logistic-regression",
    "href": "10_inference.html#logistic-regression",
    "title": "10  Inference",
    "section": "10.13 Logistic Regression",
    "text": "10.13 Logistic Regression\nIndicator variables solve the problem of integrating categorical predictors into regression models. What about when the response is a categorical variable? We’ll restrict ourselves here to predicting a binary response (two categories).\nOne approach to building a model that predicts between two categories is to pick one category as a reference point and then fit a linear model that predicts the probability, denoted \\(p\\), that each observation \\(y_i\\) is in the reference category. Mathematically, we might have something like this for every observation:\n\\[\np = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k\n\\]\nWe could then pick parameters that make the probabilities as close as possible to \\(1\\) when \\(y_i\\) is in the reference category and as close as possible to \\(0\\) when it is in the other category. Conceptually, this works well. However, some theoretical issues arise if we apply this exactly as written. For one thing, the predicted probability can be less than \\(0\\) or greater than \\(1\\), neither of which is interpretable. Logistic regression fixes this by adding a transformation function between the left and the right:\n\\[\np = \\operatorname{logistic}\\left(\\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k\\right)\n\\]\nHere, \\(\\text{logistic}(x)\\) is defined as \\(1 /(1 + e^{-x})\\), which transforms any numeric value into a number between 0 and 1.\n\n\n\n\n\n\nLogistic transformation\n\n\n\n\n\nThe exact form of logistic regression has a solid theoretical foundation that is difficult to justify precisely without venturing far beyond the scope of this text, yet we can still understand why the logistic function is a natural fit. Its shape reflects how evidence realistically affects probability: it provides a smooth, gradual transition from unlikely to likely outcomes rather than abrupt jumps. Its symmetry around the midpoint ensures that positive and negative evidence shift probabilities in a balanced way. Near the extremes, it approaches 0 and 1 without ever forcing absolute certainty, which mirrors the idea that even strong predictors seldom guarantee outcomes. This structure also aligns with how predictors combine additively as evidence while translating into multiplicative changes in the odds, making model coefficients intuitively interpretable and the resulting probabilities both stable and meaningful.\n\n\n\nLet’s finish by returning to the possum dataset, this time predicting whether a possum comes from Victoria. To run the model, we use the logit method of DSStatsmodels along with the corresponding formula object. Unlike indicator variables used as predictors, we must manually create the indicator variable used as the response in logistic regression.\n\n(\n    possum\n    .with_columns(\n        is_pre = (c.pop == \"Vic\").cast(pl.Int64)\n    )\n    .pipe(DSStatsmodels.logit, \"is_pre ~ total_l + tail_l + sex\")\n)\n\n\nshape: (4, 7)\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Intercept\"\n22.1545\n7.505\n2.952\n0.003\n7.444\n36.865\n\n\n\"sex[T.m]\"\n-1.4983\n0.617\n-2.429\n0.015\n-2.707\n-0.289\n\n\n\"total_l\"\n0.4071\n0.097\n4.211\n0.0\n0.218\n0.597\n\n\n\"tail_l\"\n-1.5461\n0.298\n-5.185\n0.0\n-2.13\n-0.962\n\n\n\n\n\n\nThe output shows the estimated coefficients for each predictor in the logistic regression model. Direct interpretation of these coefficients is difficult because of the transformation used in logistic regression. Generally, we focus on the p-values of each coefficient and the signs (positive or negative) of the coefficients to determine whether they are positively or negatively associated with the reference class after accounting for all other terms.\n\n\n\n\n\n\nMultivariate regression\n\n\n\n\n\nThe next natural extension is to consider predicting the categories of a variable with three or more unique values. This is possible using multinomial logistic regression, which effectively creates simultaneous logistic regressions for each category. This goes well beyond what can usually be interpreted from a statistical inference perspective. We will introduce the technique in the predictive context in Chapter 11 and use it extensively throughout the following chapters.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#relationships-to-causation",
    "href": "10_inference.html#relationships-to-causation",
    "title": "10  Inference",
    "section": "10.14 Relationships to Causation",
    "text": "10.14 Relationships to Causation\nWe conclude with a brief discussion of the relationship between statistical inference and causal analysis. The p-values from the tests above provide evidence of an association between the variables in question. This could mean different means across groups, dependence between two categorical variables, or a relationship between the mean of one variable and the values of one or more predictors (as in linear regression). Although these tests establish evidence of association, they do not provide evidence of causation.\nWhy can we not jump from a relationship to causation? Consider the relationship between stress and coffee consumption. The data analysis showed a positive relationship between them. But this could be due to several possible explanations:\n\nIncreased coffee consumption makes someone more stressed.\nStressed people deal with their stress by consuming coffee.\nPeople with a heavy workload are both stressed and drink a lot of coffee.\n\nThe only definitive way to distinguish these situations is a controlled study in which participants are randomly assigned to groups told to drink different amounts of coffee and then have their stress levels measured. If there is a statistically significant change in stress levels, then — and only then — can we be sure the result is caused by coffee consumption and not by some external factor.\nFinally, you may be concerned that we are still not sure whether increased coffee consumption simply makes it harder to sleep, and that change in sleep leads to the increase in stress. That is correct—we can’t rule that out. However, even if that were true, we can still say that the coffee causes the increase in stress. It just functions indirectly by changing sleep patterns.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "10_inference.html#conclusion",
    "href": "10_inference.html#conclusion",
    "title": "10  Inference",
    "section": "10.15 Conclusion",
    "text": "10.15 Conclusion\nThis chapter introduced the fundamental concepts and methods of statistical inference. We began with the building blocks of descriptive statistics, including measures of central tendency (the mean and median) and measures of variation (the variance and standard deviation). We then explored how these sample statistics can be used to make inferences about population parameters using confidence intervals and hypothesis tests.\nThe specific methods we covered range from simple one-sample and two-sample t-tests to more complex techniques, including ANOVA for comparing multiple groups, the chi-squared test for categorical associations, and both simple and multiple linear regression for modeling relationships between numerical variables. We concluded with logistic regression for binary outcomes, which extends our toolkit to handle a common type of response variable.\nThroughout, we have emphasized the importance of understanding what these methods do — and do not — tell us. Statistical significance is not the same as practical importance, and p-values require careful interpretation. Confidence intervals often provide more useful information than hypothesis tests alone. No statistical method can substitute for careful thinking about the data, the research question, and the assumptions underlying our analyses.\nAs you apply these methods in your own work, remember that statistical inference is a tool for reasoning about uncertainty, not a magic formula for finding truth. The value of these methods lies in their ability to quantify how much our conclusions might differ if we had drawn a different sample and to help us distinguish patterns likely to be real from those that might have arisen by chance. When used thoughtfully, they are powerful aids to understanding; when used mechanically, they can mislead. The goal is always to combine statistical analysis with substantive knowledge and clear thinking about the problem at hand.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "11_supervised.html",
    "href": "11_supervised.html",
    "title": "11  Supervised Learning",
    "section": "",
    "text": "11.1 Setup\nLoad all of the modules and datasets needed for the chapter.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\ncountry = pl.read_csv(\"data/countries.csv\")",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "11_supervised.html#introduction",
    "href": "11_supervised.html#introduction",
    "title": "11  Supervised Learning",
    "section": "11.2 Introduction",
    "text": "11.2 Introduction\nThe previous chapter introduced statistical inference, a set of methods for drawing conclusions about populations based on samples. In that context, the primary goal was understanding relationships between variables: determining whether differences between groups are statistically significant, whether two categorical variables are associated, or whether changes in one numerical variable are related to changes in another. Hypothesis tests and confidence intervals provided the framework for making such claims while accounting for sampling variability.\nIn this chapter, we shift our focus from inference to prediction. Rather than asking whether a relationship exists, we ask: given what we know about some variables, how well can we predict the value of another variable? This is the domain of supervised learning, a collection of methods designed to learn patterns from data that can be applied to make predictions on new, unseen observations.\nThe distinction between inference and prediction is subtle but important. In statistical inference, we typically care about the parameters of a model — the estimated coefficients, their standard errors, and their statistical significance. We ask questions like “Is there evidence that height is associated with driving speed?” In predictive modeling, we care primarily about the accuracy of predictions — how close our predicted values are to the actual values. We ask questions like “Given a student’s height and sex, what driving speed would we predict?” A model that performs well for inference may not perform well for prediction, and vice versa.\nThe term “supervised” in supervised learning refers to the fact that we train our models using data where the correct answers are already known. We have a set of input variables (called predictors, features, or independent variables) and an output variable (called the response, target, or dependent variable). The model learns from examples where both the inputs and outputs are observed, and then applies what it has learned to make predictions for cases where only the inputs are known.\nThis chapter introduces the fundamental concepts and techniques of supervised learning, beginning with the distinction between regression and classification problems. We will revisit linear regression from a predictive perspective, explore regularization techniques that improve prediction accuracy, and introduce gradient boosting as a powerful alternative to linear models. Throughout, we emphasize the importance of evaluating models on data they have not seen during training, a principle that separates predictive modeling from the descriptive statistics we have encountered earlier.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "11_supervised.html#predictive-modeling",
    "href": "11_supervised.html#predictive-modeling",
    "title": "11  Supervised Learning",
    "section": "11.3 Predictive Modeling",
    "text": "11.3 Predictive Modeling\nThe goal of predictive modeling is to generate predictions for one column in a dataset using information contained in one or more other columns. This might seem paradoxical at first: why would we want to predict values that we already have? The answer is that our ultimate goal is to apply the model to new data where the predictors are known but the value we want to predict is not. A hospital might use patient characteristics to predict the likelihood of readmission; a retailer might use purchase history to predict future spending; a researcher might use environmental measurements to predict species abundance. In each case, the model is trained on historical data where outcomes are known and then applied to future cases where outcomes have not yet occurred.\nThere are several types of predictive models, distinguished primarily by the data type of the variable being predicted. When we predict a numeric variable, the task is called regression. The name comes from the statistical technique of linear regression, though the term now encompasses many methods beyond simple linear models. When we predict a categorical variable — assigning observations to discrete groups or classes — the task is called classification. These two tasks require different evaluation criteria because the nature of prediction errors differs fundamentally between them.\nFor regression problems, we need a way to measure how far our predictions are from the actual values. The most common measure is the mean squared error (MSE), which computes the average of the squared differences between predicted and actual values. If we denote the actual values as \\(y_1, y_2, \\ldots, y_n\\) and the corresponding predictions as \\(\\hat{y}_1, \\hat{y}_2, \\ldots, \\hat{y}_n\\), the MSE is defined as follows.\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\nSquaring the differences ensures that positive and negative errors do not cancel each other out and gives greater weight to larger errors. A model that makes occasional large mistakes will have a higher MSE than one that makes consistent small mistakes, even if the average error magnitude is similar.\nBecause the MSE is expressed in squared units of the response variable, it can be difficult to interpret directly. The root mean squared error (RMSE) addresses this by taking the square root of the MSE, returning the measure to the original units.\n\\[\n\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n\\]\nThe RMSE can be interpreted as the typical magnitude of prediction errors. An RMSE of 5 years in predicting life expectancy means that predictions are typically off by about 5 years in either direction.\nFor classification problems, the most straightforward measure of performance is the error rate, which simply counts the proportion of predictions that are incorrect. If we make \\(n\\) predictions and \\(m\\) of them are wrong, the error rate is \\(m/n\\). Equivalently, the accuracy is \\((n-m)/n\\), the proportion of correct predictions.\n\\[\n\\text{Error Rate} = \\frac{\\text{Number of incorrect predictions}}{\\text{Total number of predictions}}\n\\]\nWhile the error rate provides a simple summary, it can be misleading in certain situations. If we are predicting a rare outcome — such as whether a patient has a rare disease — a model that always predicts “no disease” might have a very low error rate while being completely useless for its intended purpose. More sophisticated evaluation measures exist for such situations, but the error rate remains a useful starting point for understanding classification performance.\nThe process of building a predictive model is called training. During training, we use existing data to find parameter values that minimize prediction error. The general approach involves defining a model with numerical parameters — such as the coefficients in a linear regression — and then using optimization techniques to find the parameter values that produce the best predictions on the training data.\nWe encountered this idea in Chapter 10 when fitting linear regression models. The ordinary least squares method finds the intercept and slope values that minimize the sum of squared residuals. Predictive modeling extends this principle to a wide variety of model types, many of which require more sophisticated optimization techniques than simple calculus.\nAfter training a model, we must evaluate how well it performs. A natural approach would be to compare predictions to the actual values in our data and compute the MSE or error rate. However, this approach has a fundamental flaw: the model was optimized specifically to fit this data, so its performance on this data will be overly optimistic.\nConsider an extreme example. If we had a model flexible enough to memorize every observation in our dataset, it could achieve perfect predictions on that data — zero MSE, zero error rate. Yet such a model would be useless for predicting new observations because it has learned nothing general about the relationships in the data. It has simply memorized the answers.\nThis problem is called overfitting, and it represents one of the central challenges in predictive modeling. A model that fits the training data too closely captures not only the true underlying patterns but also the random noise specific to that particular sample. When applied to new data, the noise patterns will be different, and the model’s performance will suffer.\nThe solution is to evaluate models on data they have not seen during training. We create two subsets of our data: a training dataset used to fit the model and a testing dataset used to evaluate performance. The model is trained using only the training data, and then predictions are made for the testing data. The MSE or error rate computed on the testing set — called the test error — provides a more realistic assessment of how the model will perform on new observations.\nTypically, we randomly assign observations to training and testing sets, often using proportions like 80 percent training and 20 percent testing. This random split ensures that both sets are representative of the overall data. Some situations require more careful splitting — for example, when data have a time component, we might train on earlier observations and test on later ones — but random splitting is a reasonable default for many applications.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "11_supervised.html#linear-regression-revisited",
    "href": "11_supervised.html#linear-regression-revisited",
    "title": "11  Supervised Learning",
    "section": "11.4 Linear Regression Revisited",
    "text": "11.4 Linear Regression Revisited\nHaving established the framework for predictive modeling, we can now revisit linear regression from a new perspective. In Chapter 10, we used linear regression primarily for inference: testing whether coefficients were significantly different from zero, constructing confidence intervals, and interpreting the relationships between variables. Here, we focus on prediction: how well can a linear model predict new observations?\nThe mathematical form of linear regression remains the same. Given a response variable \\(y\\) and predictor variables \\(x_1, x_2, \\ldots, x_k\\), we model the relationship as:\n\\[\ny = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k + \\varepsilon\n\\]\nThe coefficients \\(\\alpha\\) and \\(\\beta_1, \\ldots, \\beta_k\\) are estimated by minimizing the sum of squared residuals on the training data. Once these coefficients are estimated, we can make predictions for any observation by plugging in the predictor values and computing the predicted response.\nTo fit predictive models, we will use a class called DSSklearn, which provides a consistent interface to the scikit-learn machine learning library in Python. Like the DSStatsmodels class from the previous chapter, DSSklearn is designed to work within a Polars pipe chain. It takes as arguments the target variable (the column to predict) and a list of features (the columns to use as predictors).\n\nmodel = (\n    country\n    .pipe(\n        DSSklearn.linear_regression,\n        target=c.lexp,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy]\n    )\n)\n\nThis code fits a linear regression model predicting life expectancy (lexp) using four predictors: the Human Development Index (hdi), GDP per capita (gdp), cellphone subscriptions per 100 people (cellphone), and a happiness index (happy). The function automatically splits the data into training and testing sets, fits the model on the training data, and stores everything needed to evaluate and use the model.\nThe resulting model object provides several methods for examining the results. The score method returns a dictionary containing the RMSE for both the training and testing sets.\n\nmodel.score()\n\n{'train': np.float64(2.916788304946737), 'test': np.float64(3.103181695026965)}\n\n\nThe training RMSE tells us how well the model fits the data it was trained on, while the testing RMSE provides a more realistic estimate of prediction accuracy on new data. A large gap between training and testing RMSE can indicate overfitting — the model has learned patterns specific to the training data that do not generalize well.\nThe coef method returns a DataFrame containing the estimated coefficients for each predictor.\n\nmodel.coef()\n\n\nshape: (5, 2)\n\n\n\nname\nparam\n\n\nstr\nf64\n\n\n\n\n\"Intercept\"\n75.147766\n\n\n\"hdi\"\n6.061369\n\n\n\"happy\"\n0.859377\n\n\n\"gdp\"\n-0.041938\n\n\n\"cellphone\"\n-0.696483\n\n\n\n\n\n\nThese coefficients have the same interpretation as in the inferential context: each coefficient represents the expected change in the response for a one-unit change in the corresponding predictor, holding all other predictors constant. However, in the predictive context, we are less concerned with testing whether these coefficients are significantly different from zero and more concerned with whether they collectively produce accurate predictions.\nThe predict method returns a DataFrame with columns indicating whether each observation was in the training or testing set (index_), the actual response value (target_), and the model’s prediction (prediction_).\n\nmodel.predict()\n\n\nshape: (135, 3)\n\n\n\nindex_\ntarget_\nprediction_\n\n\nstr\nf64\nf64\n\n\n\n\n\"train\"\n70.43\n66.342595\n\n\n\"train\"\n76.18\n73.382149\n\n\n\"test\"\n82.84\n83.170286\n\n\n\"train\"\n79.83\n82.999008\n\n\n\"train\"\n78.51\n74.634218\n\n\n…\n…\n…\n\n\n\"train\"\n79.67\n77.20665\n\n\n\"test\"\n76.03\n77.392255\n\n\n\"train\"\n76.98\n72.158468\n\n\n\"train\"\n81.77\n81.14838\n\n\n\"train\"\n66.71\n66.771474\n\n\n\n\n\n\nThis output allows us to examine individual predictions and understand where the model performs well or poorly. We can also add the predictions to the original data using the with_columns method.\n\n(\n    country\n    .with_columns(\n        model.predict()\n    )\n)\n\n\nshape: (135, 18)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\nindex_\ntarget_\nprediction_\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\nstr\nf64\nf64\n\n\n\n\n\"SEN\"\n\"Senegal\"\n\"Africa\"\n\"Western Africa\"\n18.932\n70.43\n14.366667\n-14.283333\n0.53\n4871\n38.1\n50.93\n66.0\n54.93987\n\"pbp|fra|wol\"\n\"train\"\n70.43\n66.342595\n\n\n\"VEN\"\n\"Venezuela, Bolivarian Republic…\n\"Americas\"\n\"South America\"\n28.517\n76.18\n8.0\n-67.0\n0.709\n8899\n44.8\n57.65\n96.8\n95.66913\n\"spa|vsl\"\n\"train\"\n76.18\n73.382149\n\n\n\"FIN\"\n\"Finland\"\n\"Europe\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n\"fin|swe\"\n\"test\"\n82.84\n83.170286\n\n\n\"USA\"\n\"United States of America\"\n\"Americas\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n\"eng\"\n\"train\"\n79.83\n82.999008\n\n\n\"LKA\"\n\"Sri Lanka\"\n\"Asia\"\n\"Southern Asia\"\n23.229\n78.51\n7.0\n81.0\n0.776\n14380\n39.3\n36.02\n83.1\n90.77437\n\"sin|sin|tam|tam\"\n\"train\"\n78.51\n74.634218\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"ALB\"\n\"Albania\"\n\"Europe\"\n\"Southern Europe\"\n2.772\n79.67\n41.0\n20.0\n0.81\n20362\n30.8\n54.45\n91.9\n98.5473\n\"sqi\"\n\"train\"\n79.67\n77.20665\n\n\n\"MYS\"\n\"Malaysia\"\n\"Asia\"\n\"South-eastern Asia\"\n35.978\n76.03\n3.7805111\n102.314362\n0.819\n35990\n46.2\n58.68\n118.2\n95.69194\n\"msa\"\n\"test\"\n76.03\n77.392255\n\n\n\"SLV\"\n\"El Salvador\"\n\"Americas\"\n\"Central America\"\n6.366\n76.98\n13.668889\n-88.866111\n0.678\n12221\n38.3\n64.82\n126.9\n86.19786\n\"spa\"\n\"train\"\n76.98\n72.158468\n\n\n\"CYP\"\n\"Cyprus\"\n\"Asia\"\n\"Western Asia\"\n1.371\n81.77\n35.0\n33.0\n0.913\n55720\n31.2\n60.71\n123.1\n99.41781\n\"ell|tur\"\n\"train\"\n81.77\n81.14838\n\n\n\"PAK\"\n\"Pakistan\"\n\"Asia\"\n\"Southern Asia\"\n255.22\n66.71\n30.0\n71.0\n0.544\n5717\n29.6\n45.49\n49.8\n61.92651\n\"eng|urd\"\n\"train\"\n66.71\n66.771474\n\n\n\n\n\n\nThis combined DataFrame makes it easy to compare predictions to actual values while also seeing all the original variables, which can help diagnose why certain predictions are more or less accurate.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "11_supervised.html#lasso-regression",
    "href": "11_supervised.html#lasso-regression",
    "title": "11  Supervised Learning",
    "section": "11.5 Lasso Regression",
    "text": "11.5 Lasso Regression\nLinear regression estimates coefficients by minimizing the sum of squared residuals on the training data. While this produces the best possible fit to the training data, it does not necessarily produce the best predictions on new data. When models have many predictors, or when predictors are highly correlated with each other, the estimated coefficients can become unstable — small changes in the training data lead to large changes in the coefficients. This instability often leads to poor predictions on new observations.\nRegularization addresses this problem by adding a penalty term to the optimization objective. Instead of simply minimizing squared residuals, we minimize squared residuals plus a penalty that discourages extreme coefficient values. This produces coefficients that are somewhat worse at fitting the training data but often better at predicting new data.\nThe lasso (Least Absolute Shrinkage and Selection Operator) is one of the most important regularization techniques. It adds a penalty proportional to the sum of the absolute values of the coefficients. If we denote the coefficients as \\(\\beta_1, \\beta_2, \\ldots, \\beta_k\\), the lasso objective function can be written as follows:\n\\[\n\\min_{\\alpha, \\beta} \\left[ \\sum_{i=1}^{n} (y_i - \\alpha - \\beta_1 x_{i1} - \\cdots - \\beta_k x_{ik})^2 + \\lambda \\sum_{j=1}^{k} |\\beta_j| \\right]\n\\]\nThe parameter \\(\\lambda\\) (lambda) controls the strength of the penalty. When \\(\\lambda = 0\\), the lasso is equivalent to ordinary linear regression. As \\(\\lambda\\) increases, the penalty term becomes more important, and coefficients are pushed toward zero. A remarkable property of the lasso is that it can push some coefficients exactly to zero, effectively removing those predictors from the model. This makes the lasso useful not only for improving predictions but also for variable selection — identifying which predictors are most important.\n\n\n\n\n\n\nWhy the Absolute Value?\n\n\n\n\n\nThe absolute value penalty is what gives the lasso its variable selection property. To understand why, imagine the optimization process searching for the best coefficients. When a coefficient is small, the absolute value penalty creates a constant “pull” toward zero, regardless of the coefficient’s current value. This pull can overcome the slight improvement in fit that a small coefficient provides, driving the coefficient all the way to zero. In contrast, other penalty forms (like the squared penalty used in ridge regression) create a pull that weakens as the coefficient approaches zero, so coefficients shrink but never quite reach zero.\n\n\n\nChoosing the right value of \\(\\lambda\\) is crucial. Too small a value provides little regularization and does not address overfitting; too large a value over-penalizes the coefficients and produces a model that underfits the data. Cross-validation is the standard approach for selecting \\(\\lambda\\). The training data is divided into several folds (subsets), and for each candidate value of \\(\\lambda\\), the model is trained on some folds and evaluated on the remaining fold. This process is repeated for all possible fold arrangements, and the \\(\\lambda\\) value that produces the best average performance is selected.\nThe DSSklearn class handles cross-validation automatically when using the elastic_net_cv method, which is a general framework that includes the lasso as a special case. Setting l1_ratio=1 specifies that we want the lasso penalty.\n\nmodel = (\n    country\n    .pipe(\n        DSSklearn.elastic_net_cv,\n        target=c.lexp,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        l1_ratio=1\n    )\n)\n\nThe resulting model object provides the same methods as before. The score method shows performance on training and testing sets.\n\nmodel.score()\n\n{'train': np.float64(2.961003671269824),\n 'test': np.float64(3.0813582323445163)}\n\n\nThe predict method returns predictions.\n\nmodel.predict()\n\n\nshape: (135, 3)\n\n\n\nindex_\ntarget_\nprediction_\n\n\nstr\nf64\nf64\n\n\n\n\n\"train\"\n70.43\n66.86029\n\n\n\"train\"\n76.18\n73.598074\n\n\n\"test\"\n82.84\n83.095582\n\n\n\"train\"\n79.83\n82.16778\n\n\n\"train\"\n78.51\n74.952659\n\n\n…\n…\n…\n\n\n\"train\"\n79.67\n77.061804\n\n\n\"test\"\n76.03\n77.588752\n\n\n\"train\"\n76.98\n72.834118\n\n\n\"train\"\n81.77\n81.054559\n\n\n\"train\"\n66.71\n67.098843\n\n\n\n\n\n\nThe coef method shows the estimated coefficients.\n\nmodel.coef()\n\n\nshape: (5, 2)\n\n\n\nname\nparam\n\n\nstr\nf64\n\n\n\n\n\"Intercept\"\n75.147766\n\n\n\"hdi\"\n5.487217\n\n\n\"happy\"\n0.5501\n\n\n\"gdp\"\n0.0\n\n\n\"cellphone\"\n-0.0\n\n\n\n\n\n\nNotice that some coefficients may be exactly zero or very close to zero, reflecting the lasso’s variable selection property. The model has determined that these predictors do not contribute enough to predictions to justify keeping them.\nTo compare the relative importance of predictors, it is helpful to examine coefficients on a standardized scale. The raw=True argument returns coefficients based on standardized (scaled) features, making the magnitudes comparable across predictors with different units.\n\nmodel.coef(raw=True)\n\n\nshape: (5, 2)\n\n\n\nname\nparam\n\n\nstr\nf64\n\n\n\n\n\"Intercept\"\n45.41005\n\n\n\"hdi\"\n35.826175\n\n\n\"happy\"\n0.048348\n\n\n\"gdp\"\n0.0\n\n\n\"cellphone\"\n-0.0\n\n\n\n\n\n\nThe alpha method returns the value of \\(\\lambda\\) (called alpha in scikit-learn) selected by cross-validation.\n\nmodel.alpha()\n\nnp.float64(0.24888744821240497)\n\n\nOnce we have identified the optimal penalty strength through cross-validation, we can refit the model with this specific value using elastic_net instead of elastic_net_cv. This is useful when we want more control over the fitting process.\n\n(\n    country\n    .pipe(\n        DSSklearn.elastic_net,\n        target=c.lexp,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        l1_ratio=1,\n        alpha=1\n    )\n    .coef(raw=True)\n    .filter(c.param != 0)\n)\n\n\nshape: (3, 2)\n\n\n\nname\nparam\n\n\nstr\nf64\n\n\n\n\n\"Intercept\"\n49.565524\n\n\n\"hdi\"\n33.071864\n\n\n\"happy\"\n0.011268\n\n\n\n\n\n\nThe filter method selects only predictors with nonzero coefficients, showing us which variables the lasso has retained in the model.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "11_supervised.html#ridge-regression-and-enet",
    "href": "11_supervised.html#ridge-regression-and-enet",
    "title": "11  Supervised Learning",
    "section": "11.6 Ridge Regression and ENet",
    "text": "11.6 Ridge Regression and ENet\nWhile the lasso uses an absolute value penalty, ridge regression uses a squared penalty on the coefficients. The ridge objective function is:\n\\[\n\\min_{\\alpha, \\beta} \\left[ \\sum_{i=1}^{n} (y_i - \\alpha - \\beta_1 x_{i1} - \\cdots - \\beta_k x_{ik})^2 + \\lambda \\sum_{j=1}^{k} \\beta_j^2 \\right]\n\\]\nThe squared penalty shrinks coefficients toward zero but, unlike the lasso, never shrinks them exactly to zero. This means ridge regression keeps all predictors in the model but with reduced influence. Ridge regression is particularly useful when predictors are highly correlated with each other, a situation where ordinary least squares and even the lasso can produce unstable estimates.\nThe elastic net combines both penalties, allowing the user to balance the lasso’s variable selection with ridge regression’s stability. The l1_ratio parameter controls this balance: a value of 1 gives the lasso (all absolute value penalty), a value of 0 gives ridge regression (all squared penalty), and intermediate values give a blend.\nTo fit a ridge regression model, we set l1_ratio=0:\n\nmodel = (\n    country\n    .pipe(\n        DSSklearn.elastic_net_cv,\n        target=c.lexp,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        l1_ratio=0,\n        alphas=[0.0, 0.01, 1, 10]\n    )\n)\n\nThe alphas parameter specifies the candidate penalty values to consider during cross-validation. The alpha method returns the selected value.\n\nmodel.alpha()\n\nnp.float64(0.01)\n\n\nTo fit an elastic net with a blend of both penalties, we specify an intermediate l1_ratio:\n\nmodel = (\n    country\n    .pipe(\n        DSSklearn.elastic_net_cv,\n        target=c.lexp,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        l1_ratio=0.2\n    )\n)\n\n\nmodel.alpha()\n\nnp.float64(0.030825660663502993)\n\n\nThe choice between lasso, ridge, and elastic net depends on the specific problem. If variable selection is important and you want an interpretable model with fewer predictors, the lasso is often preferred. If you have many correlated predictors and want to retain all of them with reduced influence, ridge regression is more appropriate. When uncertain, the elastic net provides a compromise that often works well in practice.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "11_supervised.html#gradient-boosting",
    "href": "11_supervised.html#gradient-boosting",
    "title": "11  Supervised Learning",
    "section": "11.7 Gradient Boosting",
    "text": "11.7 Gradient Boosting\nLinear models, even with regularization, assume that the relationship between predictors and response can be captured by a weighted sum. This assumption is violated when relationships are nonlinear or when interactions between predictors are important. In such cases, more flexible models may provide better predictions.\nGradient boosting is a powerful technique that builds predictions by combining many simple models, typically decision trees. The key insight is that while individual trees may be weak predictors, their combined predictions can be remarkably accurate. The “gradient” in gradient boosting refers to the optimization technique used to build successive trees: each new tree is trained to correct the errors made by the previous trees, gradually improving predictions.\nThe process works as follows. First, an initial prediction is made, typically the average of the response variable. Then, a small decision tree is fit to predict the residuals — the differences between actual values and current predictions. This tree’s predictions are added to the current predictions (multiplied by a small learning rate to prevent overcorrection). The process repeats, with each new tree focusing on the remaining errors, until a specified number of trees have been added or further improvement is minimal.\n\n\n\n\n\n\nDecision Trees\n\n\n\n\n\nA decision tree makes predictions by recursively splitting the data based on predictor values. At each step, the algorithm finds the predictor and split point that best separates observations with different response values. For regression, “best” typically means minimizing the squared error within each resulting group. The process continues until groups are small enough or further splits provide little improvement. To make a prediction, we follow the splits from the top of the tree down until we reach a final group (called a leaf), and the prediction is the average response value in that group.\n\n\n\nThe mathematical formulation of gradient boosting for regression is as follows. Let \\(F_0(x)\\) be the initial prediction (typically the mean of \\(y\\)). For iterations \\(m = 1, 2, \\ldots, M\\):\n\nCompute the residuals \\(r_i = y_i - F_{m-1}(x_i)\\) for each observation.\nFit a decision tree \\(h_m(x)\\) to predict these residuals.\nUpdate the model: \\(F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)\\)\n\nHere, \\(\\eta\\) is the learning rate, a small positive number that controls how much each tree contributes to the final prediction. Smaller learning rates require more trees but often produce better final results.\nThe DSSklearn class provides access to gradient boosting through the gradient_boosting_regressor method. Key parameters include the number of trees (n_estimators), the learning rate (learning_rate), and the maximum depth of each tree (max_depth).\n\nmodel = (\n    country\n    .pipe(\n        DSSklearn.gradient_boosting_regressor,\n        target=c.lexp,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        n_estimators=100,\n        learning_rate=0.1,\n        max_depth=3\n    )\n)\n\nThe model provides the same methods as before. The score method shows training and testing performance.\n\nmodel.score()\n\n{'train': np.float64(0.6290821129470985),\n 'test': np.float64(3.194865235423206)}\n\n\nThe predict method returns predictions for individual observations.\n\nmodel.predict()\n\n\nshape: (135, 3)\n\n\n\nindex_\ntarget_\nprediction_\n\n\nstr\nf64\nf64\n\n\n\n\n\"train\"\n70.43\n69.834624\n\n\n\"train\"\n76.18\n76.05566\n\n\n\"test\"\n82.84\n83.889788\n\n\n\"train\"\n79.83\n80.740508\n\n\n\"train\"\n78.51\n77.881171\n\n\n…\n…\n…\n\n\n\"train\"\n79.67\n78.583256\n\n\n\"test\"\n76.03\n76.073709\n\n\n\"train\"\n76.98\n76.1445\n\n\n\"train\"\n81.77\n82.213405\n\n\n\"train\"\n66.71\n66.656872\n\n\n\n\n\n\nBecause gradient boosting does not produce simple linear coefficients, we cannot interpret the model in the same way as linear regression. Instead, the importance method provides a measure of how much each predictor contributes to reducing prediction error across all trees.\n\nmodel.importance()\n\n\nshape: (4, 2)\n\n\n\nname\nimportance\n\n\nstr\nf64\n\n\n\n\n\"hdi\"\n0.861063\n\n\n\"gdp\"\n0.059134\n\n\n\"cellphone\"\n0.028945\n\n\n\"happy\"\n0.050858\n\n\n\n\n\n\nHigher importance values indicate predictors that are more useful for making predictions. This gives us insight into which variables matter most, even though we cannot point to a single coefficient with a straightforward interpretation.\n\n\n\n\n\n\nTuning Gradient Boosting\n\n\n\n\n\nGradient boosting has several parameters that affect its performance. The number of trees (n_estimators) and learning rate (learning_rate) work together: more trees with a smaller learning rate generally produce better results but require more computation. The maximum depth (max_depth) controls the complexity of individual trees; deeper trees can capture more complex patterns but are more prone to overfitting. In practice, choosing good parameter values often requires experimentation, though reasonable defaults (100-500 trees, learning rate of 0.05-0.1, maximum depth of 3-6) work well for many problems.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "11_supervised.html#classification",
    "href": "11_supervised.html#classification",
    "title": "11  Supervised Learning",
    "section": "11.8 Classification",
    "text": "11.8 Classification\nThe techniques we have discussed so far address regression problems, where the goal is to predict a numeric response. Classification extends these ideas to categorical responses, where we want to assign observations to discrete classes or categories. Many real-world prediction problems are classification tasks: detecting spam emails, diagnosing diseases, predicting customer churn, or identifying objects in images.\nThe fundamental challenge in classification is that we cannot directly minimize squared error when the response is categorical. Instead, we typically model the probability that an observation belongs to each class and then assign the observation to the most likely class. Different classification methods approach this probability modeling in different ways.\nWe introduced logistic regression in Chapter 10 as a method for modeling binary outcomes. In the predictive context, logistic regression serves as a classification method. Given predictor values, we compute the probability that an observation belongs to each class and predict the class with the highest probability.\nFor classification with more than two classes (called multinomial or multiclass classification), logistic regression extends naturally. Instead of modeling a single probability, we model the probability of each class simultaneously, with the probabilities constrained to sum to one.\nThe DSSklearn class provides logistic regression through the .logistic_regression_cv method, which automatically selects the regularization strength through cross-validation. For classification problems, we also typically want to stratify the train-test split, ensuring that each class is proportionally represented in both sets. This is especially important when some classes are rare.\n\nmodel = (\n    country\n    .pipe(\n        DSSklearn.logistic_regression_cv,\n        target=c.region,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        stratify=c.region,\n        l1_ratios=[1],\n        solver=\"saga\"\n    )\n)\n\nThis code fits a multinomial logistic regression predicting the region of a country based on development indicators. The l1_ratios=[1] specifies lasso regularization, which can help identify the most important predictors for distinguishing between regions. The solver=\"saga\" specifies the optimization algorithm, which is required for lasso regularization with multiple classes.\nThe score method returns the accuracy (proportion of correct predictions) for training and testing sets rather than RMSE.\n\nmodel.score()\n\n{'train': 0.648936170212766, 'test': 0.6585365853658537}\n\n\nThe predict method returns the actual and predicted classes for each observation.\n\nmodel.predict()\n\n\nshape: (135, 3)\n\n\n\nindex_\ntarget_\nprediction_\n\n\nstr\nstr\nstr\n\n\n\n\n\"test\"\n\"Africa\"\n\"Africa\"\n\n\n\"test\"\n\"Americas\"\n\"Asia\"\n\n\n\"test\"\n\"Europe\"\n\"Europe\"\n\n\n\"test\"\n\"Americas\"\n\"Europe\"\n\n\n\"train\"\n\"Asia\"\n\"Asia\"\n\n\n…\n…\n…\n\n\n\"train\"\n\"Europe\"\n\"Asia\"\n\n\n\"train\"\n\"Asia\"\n\"Europe\"\n\n\n\"test\"\n\"Americas\"\n\"Americas\"\n\n\n\"train\"\n\"Asia\"\n\"Europe\"\n\n\n\"test\"\n\"Asia\"\n\"Africa\"\n\n\n\n\n\n\nWhile overall accuracy provides a simple summary, it does not reveal which classes are easy or difficult to predict. A confusion matrix shows the detailed breakdown of predictions versus actual classes. Each row represents the actual class, and each column represents the predicted class. The diagonal elements show correct predictions, while off-diagonal elements show misclassifications.\n\nmodel.confusion_matrix()\n\n\n\n\n\n\n\n\nThe confusion matrix helps diagnose model performance. If certain classes are frequently confused with each other, this might suggest that those classes are genuinely similar according to the available predictors, or that additional predictors are needed to distinguish them.\nRather than just predicting the most likely class, classification models can provide the probability of each class. These probabilities are useful for understanding model confidence and for applications where the costs of different types of errors vary.\n\nmodel.predict_proba()\n\n\nshape: (135, 9)\n\n\n\nindex_\ntarget_\nprediction_\nprob_pred_\nAfrica\nAmericas\nAsia\nEurope\nOceania\n\n\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"test\"\n\"Africa\"\n\"Africa\"\n0.75986\n0.75986\n0.05542\n0.166566\n0.013723\n0.004431\n\n\n\"test\"\n\"Americas\"\n\"Asia\"\n0.342031\n0.267804\n0.270032\n0.342031\n0.106997\n0.013137\n\n\n\"test\"\n\"Europe\"\n\"Europe\"\n0.636847\n0.009729\n0.234223\n0.11038\n0.636847\n0.008821\n\n\n\"test\"\n\"Americas\"\n\"Europe\"\n0.757316\n0.00967\n0.033028\n0.192805\n0.757316\n0.007181\n\n\n\"train\"\n\"Asia\"\n\"Asia\"\n0.54861\n0.182788\n0.038869\n0.54861\n0.212955\n0.016777\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"train\"\n\"Europe\"\n\"Asia\"\n0.442427\n0.102469\n0.169017\n0.442427\n0.270176\n0.01591\n\n\n\"train\"\n\"Asia\"\n\"Europe\"\n0.40051\n0.092074\n0.156949\n0.334111\n0.40051\n0.016356\n\n\n\"test\"\n\"Americas\"\n\"Americas\"\n0.398266\n0.3073\n0.398266\n0.200638\n0.082739\n0.011057\n\n\n\"train\"\n\"Asia\"\n\"Europe\"\n0.67407\n0.021437\n0.071547\n0.221417\n0.67407\n0.01153\n\n\n\"test\"\n\"Asia\"\n\"Africa\"\n0.724343\n0.724343\n0.035139\n0.21958\n0.016154\n0.004783\n\n\n\n\n\n\nThe output includes the predicted class, the maximum probability (the confidence in that prediction), and the probability assigned to each possible class. Observations with high maximum probability are those where the model is most confident; observations with probabilities spread across multiple classes are more uncertain.\nFor multinomial classification, there are separate coefficients for each class. Each set of coefficients describes how the predictors relate to the probability of that particular class relative to the others.\n\nmodel.coef()\n\n\nshape: (5, 6)\n\n\n\nname\nAfrica\nAmericas\nAsia\nEurope\nOceania\n\n\nstr\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Intercept\"\n0.408139\n0.099446\n1.175126\n0.484408\n-2.167119\n\n\n\"hdi\"\n-1.786844\n0.0\n0.0\n0.758631\n0.0\n\n\n\"gdp\"\n0.0\n-0.770222\n0.0\n0.500775\n0.0\n\n\n\"cellphone\"\n0.0\n0.0\n-0.40783\n0.0\n0.0\n\n\n\"happy\"\n-0.079028\n1.038953\n-0.038708\n0.0\n0.0\n\n\n\n\n\n\nInterpreting these coefficients is more complex than in binary logistic regression. A positive coefficient for a predictor in a particular class means that higher values of that predictor are associated with higher probability of that class, all else being equal. However, because the probabilities must sum to one, the effects are relative rather than absolute.\nGradient boosting can also be applied to classification problems. The algorithm adapts to predict class probabilities rather than numeric values, using a suitable loss function for classification.\n\nmodel = (\n    country\n    .pipe(\n        DSSklearn.gradient_boosting_classifier,\n        target=c.region,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        n_estimators=100,\n        learning_rate=0.1,\n        max_depth=3\n    )\n)\n\n\nmodel.score()\n\n{'train': 1.0, 'test': 0.5853658536585366}\n\n\nGradient boosting classifiers can capture complex, nonlinear relationships between predictors and class membership. However, they can also overfit, especially with small datasets or many trees. A common sign of overfitting is a large gap between training accuracy (often very high) and testing accuracy.\n\nmodel.predict_proba()\n\n\nshape: (135, 9)\n\n\n\nindex_\ntarget_\nprediction_\nprob_pred_\nAfrica\nAmericas\nAsia\nEurope\nOceania\n\n\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"train\"\n\"Africa\"\n\"Africa\"\n0.99756\n0.99756\n0.000015\n0.002401\n0.000024\n4.8954e-8\n\n\n\"train\"\n\"Americas\"\n\"Americas\"\n0.989641\n0.000369\n0.989641\n0.009535\n0.000453\n0.000001\n\n\n\"test\"\n\"Europe\"\n\"Europe\"\n0.852998\n0.000567\n0.004214\n0.142184\n0.852998\n0.000037\n\n\n\"train\"\n\"Americas\"\n\"Americas\"\n0.987551\n0.000278\n0.987551\n0.002829\n0.00934\n0.000002\n\n\n\"train\"\n\"Asia\"\n\"Asia\"\n0.989476\n0.005208\n0.002332\n0.989476\n0.002978\n0.000006\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"train\"\n\"Europe\"\n\"Europe\"\n0.990756\n0.00121\n0.000478\n0.007554\n0.990756\n0.000002\n\n\n\"test\"\n\"Asia\"\n\"Americas\"\n0.923263\n0.002322\n0.923263\n0.023678\n0.050729\n0.000008\n\n\n\"train\"\n\"Americas\"\n\"Americas\"\n0.997877\n0.000023\n0.997877\n0.002094\n0.000006\n7.9017e-8\n\n\n\"train\"\n\"Asia\"\n\"Asia\"\n0.98239\n0.000105\n0.001211\n0.98239\n0.016294\n3.5747e-7\n\n\n\"train\"\n\"Asia\"\n\"Asia\"\n0.980853\n0.014223\n0.001893\n0.980853\n0.003024\n0.000006\n\n\n\n\n\n\nNotice that gradient boosting often produces very confident predictions — probabilities close to 0 or 1 — even when accuracy is moderate. This overconfidence can be problematic in applications where calibrated probability estimates are important. The learning rate and number of trees can be adjusted to reduce overfitting, though this may also reduce accuracy.\n\n\n\n\n\n\nChoosing Between Methods\n\n\n\n\n\nThe choice between logistic regression and gradient boosting (or other methods) depends on the specific problem. Logistic regression is simpler, more interpretable, and often works well when relationships between predictors and class membership are approximately linear. Gradient boosting is more flexible and can capture complex patterns, but is harder to interpret and more prone to overfitting. In practice, it is common to try multiple methods and compare their performance on testing data.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "11_supervised.html#conclusion",
    "href": "11_supervised.html#conclusion",
    "title": "11  Supervised Learning",
    "section": "11.9 Conclusion",
    "text": "11.9 Conclusion\nThis chapter introduced the fundamental concepts of supervised learning, the branch of machine learning focused on making predictions from data. We began by distinguishing predictive modeling from statistical inference: while both involve modeling relationships in data, inference emphasizes understanding and testing those relationships, while prediction emphasizes accurate forecasting for new observations.\nThe central challenge in predictive modeling is overfitting — fitting the training data so closely that the model fails to generalize to new data. We addressed this challenge through the train-test split, which evaluates models on data not used in training, and through regularization techniques like the lasso and ridge regression, which penalize complex models to improve generalization.\nWe explored several modeling approaches. Linear regression, familiar from the previous chapter, provides a baseline for regression problems. Regularization methods improve upon basic linear regression by preventing overfitting and, in the case of the lasso, performing variable selection. Gradient boosting offers a powerful alternative that can capture nonlinear relationships, at the cost of interpretability.\nFor classification problems, we extended logistic regression to multiple classes and applied similar regularization techniques. Gradient boosting classifiers provide a flexible alternative, though they require careful tuning to avoid overconfidence in predictions.\nThroughout, we emphasized the importance of proper evaluation. Training error measures how well a model fits known data; testing error measures how well it predicts new data. Models should be selected and tuned based on testing performance, not training performance. This principle — never trust a model’s performance on the same data used to build it — is perhaps the most important lesson of supervised learning.\nThe techniques introduced here form the foundation for more advanced methods we will explore in subsequent chapters. Deep learning extends these ideas with more flexible model architectures, while careful feature engineering and model selection can further improve predictions. Regardless of the specific method used, the core principles remain the same: learn patterns from labeled data, evaluate on held-out data, and guard against overfitting.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "12_unsupervised.html",
    "href": "12_unsupervised.html",
    "title": "12  Unsupervised Learning",
    "section": "",
    "text": "12.1 Setup\nLoad all of the modules and datasets needed for the chapter.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\ncountry = pl.read_csv(\"data/countries.csv\")",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "12_unsupervised.html#introduction",
    "href": "12_unsupervised.html#introduction",
    "title": "12  Unsupervised Learning",
    "section": "12.2 Introduction",
    "text": "12.2 Introduction\nIn the previous chapter, we explored supervised learning, where we used labeled data to train models that predict outcomes for new observations. Every training example had both input features and a known target value, and our goal was to learn the relationship between them. In this chapter, we turn to a fundamentally different task: extracting structure from data when no target variable exists. This is the domain of unsupervised learning.\nUnsupervised learning addresses two primary questions. First, can we reduce the complexity of high-dimensional data while preserving its essential structure? A dataset with dozens or hundreds of variables is difficult to visualize and may contain redundant information. Dimensionality reduction techniques compress such data into a smaller number of dimensions that capture the most important patterns. Second, can we discover natural groupings in data without being told what those groups should be? Clustering algorithms identify observations that are similar to each other and different from observations in other groups, revealing structure that might not be apparent from examining individual variables.\nThese techniques are valuable in their own right for exploratory analysis and visualization, but they also serve as preprocessing steps for supervised learning. Reducing dimensions before fitting a predictive model can improve computational efficiency, reduce overfitting, and sometimes even improve predictive accuracy by eliminating noise. Similarly, cluster labels can become features in a supervised model or help identify distinct subpopulations that might benefit from separate modeling approaches.\nThe methods in this chapter differ from those in previous chapters in an important way: there is no obvious measure of success. In supervised learning, we can evaluate a model by comparing its predictions to known outcomes on held-out data. In unsupervised learning, we lack this external benchmark. Whether a dimensionality reduction captures the “right” structure or a clustering produces “meaningful” groups depends on the goals of the analysis and often requires domain expertise to assess. This subjectivity does not make these methods less useful—it simply means we must be thoughtful about interpreting and validating their results.\nWe organize this chapter around the two main tasks of unsupervised learning. We begin with dimensionality reduction, covering principal component analysis (PCA) as a foundational linear method, then introduce UMAP and t-SNE as nonlinear alternatives better suited for visualization. We then turn to clustering, examining k-means as a classic approach and DBSCAN as a density-based alternative. We conclude by showing how unsupervised methods can be combined with the supervised techniques from the previous chapter.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "12_unsupervised.html#principal-components-pca",
    "href": "12_unsupervised.html#principal-components-pca",
    "title": "12  Unsupervised Learning",
    "section": "12.3 Principal Components (PCA)",
    "text": "12.3 Principal Components (PCA)\nPrincipal component analysis, commonly known as PCA, is the most widely used technique for dimensionality reduction. The core idea is to find new variables, called principal components, that are linear combinations of the original variables and capture as much of the variation in the data as possible. The first principal component is the direction along which the data vary the most. The second principal component is the direction of maximum remaining variation, subject to being perpendicular (orthogonal) to the first. Subsequent components follow the same pattern, each capturing the maximum variation while remaining perpendicular to all previous components.\nTo understand this more precisely, suppose we have \\(n\\) observations and \\(p\\) variables. For each observation \\(i\\), we have measurements \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\). The first principal component is a weighted combination of these variables:\n\\[\n\\text{PC}_1 = w_{11} x_1 + w_{12} x_2 + \\cdots + w_{1p} x_p\n\\]\nThe weights \\(w_{11}, w_{12}, \\ldots, w_{1p}\\) are chosen to maximize the variance of \\(\\text{PC}_1\\) across all observations, subject to the constraint that the sum of the squared weights equals one (this normalization prevents us from making the variance arbitrarily large by simply increasing the weights). The second principal component takes the same form:\n\\[\n\\text{PC}_2 = w_{21} x_1 + w_{22} x_2 + \\cdots + w_{2p} x_p\n\\]\nHere the weights are chosen to maximize variance subject to two constraints: the squared weights sum to one, and \\(\\text{PC}_2\\) is uncorrelated with \\(\\text{PC}_1\\). This process continues until we have \\(p\\) principal components, though in practice we typically keep only the first few.\nThe power of PCA lies in the fact that, for many datasets, the first few components capture the vast majority of the total variation. If two or three components explain 90% of the variance, we can visualize the data in two or three dimensions without losing much information. Even when visualization is not the goal, reducing from hundreds of variables to tens can dramatically speed up subsequent analyses while preserving the signal in the data.\n\n\n\n\n\n\nStandardization\n\n\n\n\n\nBefore applying PCA, it is standard practice to center each variable (subtract its mean) and by dividing by the standard deviation. Centering ensures the principal components pass through the center of the data. Standardization ensures that variables measured on different scales contribute equally to the analysis. Without standardization, a variable measured in thousands (like GDP) would dominate one measured in single digits (like a happiness index) simply because of the units chosen. The DSSklearn class handles standardization automatically, but this is worth keeping in mind when interpreting results or applying PCA in other contexts.\n\n\n\nLet’s apply PCA to our country dataset. We select four variables—the Human Development Index (hdi), GDP per capita (gdp), cellphone subscriptions per 100 people (cellphone), and a happiness score (happy)—and reduce them to two dimensions. The DSSklearn.pca method takes a list of feature columns, and the number of components to compute.\n\n(\n    country\n    .pipe(\n        DSSklearn.pca,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        n_components=2\n    )\n    .predict()\n\n)\n\n\nshape: (135, 2)\n\n\n\ndr0\ndr1\n\n\nf64\nf64\n\n\n\n\n-1.758354\n0.147961\n\n\n-0.394464\n0.548632\n\n\n2.891235\n0.426905\n\n\n1.921109\n-1.266114\n\n\n-1.176426\n0.257248\n\n\n…\n…\n\n\n-0.038424\n0.1643\n\n\n0.785001\n0.317731\n\n\n0.243163\n1.053777\n\n\n1.615747\n-0.082606\n\n\n-2.132016\n-0.170759\n\n\n\n\n\n\nThe output is a DataFrame with two columns, dr0 and dr1, representing the first and second principal components for each country. These new variables are uncorrelated with each other and together capture the maximum possible variance from the original four variables in just two dimensions.\nOften we want to retain the original data alongside the principal components for further analysis or visualization. The full=True option accomplishes this by appending the components to the original DataFrame rather than returning only the reduced representation.\n\n(\n    country\n    .pipe(\n        DSSklearn.pca,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy]\n    )\n    .predict(full=True)\n\n)\n\n\nshape: (135, 19)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\ndr0\ndr1\ndr2\ndr3\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"SEN\"\n\"Senegal\"\n\"Africa\"\n\"Western Africa\"\n18.932\n70.43\n14.366667\n-14.283333\n0.53\n4871\n38.1\n50.93\n66.0\n54.93987\n\"pbp|fra|wol\"\n-1.758354\n0.147961\n0.404841\n-0.572315\n\n\n\"VEN\"\n\"Venezuela, Bolivarian Republic…\n\"Americas\"\n\"South America\"\n28.517\n76.18\n8.0\n-67.0\n0.709\n8899\n44.8\n57.65\n96.8\n95.66913\n\"spa|vsl\"\n-0.394464\n0.548632\n0.453292\n0.009272\n\n\n\"FIN\"\n\"Finland\"\n\"Europe\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n\"fin|swe\"\n2.891235\n0.426905\n0.482956\n-0.214911\n\n\n\"USA\"\n\"United States of America\"\n\"Americas\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n\"eng\"\n1.921109\n-1.266114\n-0.272289\n0.027414\n\n\n\"LKA\"\n\"Sri Lanka\"\n\"Asia\"\n\"Southern Asia\"\n23.229\n78.51\n7.0\n81.0\n0.776\n14380\n39.3\n36.02\n83.1\n90.77437\n\"sin|sin|tam|tam\"\n-1.176426\n0.257248\n-1.22158\n0.696395\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"ALB\"\n\"Albania\"\n\"Europe\"\n\"Southern Europe\"\n2.772\n79.67\n41.0\n20.0\n0.81\n20362\n30.8\n54.45\n91.9\n98.5473\n\"sqi\"\n-0.038424\n0.1643\n-0.033963\n0.448928\n\n\n\"MYS\"\n\"Malaysia\"\n\"Asia\"\n\"South-eastern Asia\"\n35.978\n76.03\n3.7805111\n102.314362\n0.819\n35990\n46.2\n58.68\n118.2\n95.69194\n\"msa\"\n0.785001\n0.317731\n-0.153479\n-0.012839\n\n\n\"SLV\"\n\"El Salvador\"\n\"Americas\"\n\"Central America\"\n6.366\n76.98\n13.668889\n-88.866111\n0.678\n12221\n38.3\n64.82\n126.9\n86.19786\n\"spa\"\n0.243163\n1.053777\n0.761397\n-0.536137\n\n\n\"CYP\"\n\"Cyprus\"\n\"Asia\"\n\"Western Asia\"\n1.371\n81.77\n35.0\n33.0\n0.913\n55720\n31.2\n60.71\n123.1\n99.41781\n\"ell|tur\"\n1.615747\n-0.082606\n-0.440613\n0.104972\n\n\n\"PAK\"\n\"Pakistan\"\n\"Asia\"\n\"Southern Asia\"\n255.22\n66.71\n30.0\n71.0\n0.544\n5717\n29.6\n45.49\n49.8\n61.92651\n\"eng|urd\"\n-2.132016\n-0.170759\n0.082109\n-0.313767\n\n\n\n\n\n\nWith the components added to our data, we can create a visualization that projects all 188 countries into two dimensions while preserving information about their region and identity. This type of plot reveals structure that would be impossible to see by examining the four original variables separately.\n\n(\n    country\n    .pipe(\n        DSSklearn.pca,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy]\n    )\n    .predict(full=True)\n    .pipe(ggplot, aes(\"dr0\", \"dr1\"))\n    + geom_point(aes(color=\"region\"))\n    + geom_text(aes(color=\"region\", label=\"iso\"), nudge_y=0.05, size=5)\n)\n\n\n\n\n\n\n\n\nThe visualization shows countries positioned according to their scores on the first two principal components. Countries that are similar across the four original variables appear close together in this reduced space. We can see regional patterns emerging: European countries cluster together, as do Sub-Saharan African countries, reflecting shared characteristics in development, wealth, technology adoption, and well-being. The horizontal axis (first principal component) appears to capture an overall development gradient, while the vertical axis (second principal component) picks up variation not explained by this primary dimension.\nFor certain applications, particularly when feeding the reduced data into another model, it can be convenient to store all components in a single array column rather than separate columns. The array=True option provides this format.\n\n(\n    country\n    .pipe(\n        DSSklearn.pca,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy]\n    )\n    .predict(array=True)\n\n)\n\n\nshape: (135, 1)\n\n\n\ndr\n\n\narray[f64, 4]\n\n\n\n\n[-1.758354, 0.147961, … -0.572315]\n\n\n[-0.394464, 0.548632, … 0.009272]\n\n\n[2.891235, 0.426905, … -0.214911]\n\n\n[1.921109, -1.266114, … 0.027414]\n\n\n[-1.176426, 0.257248, … 0.696395]\n\n\n…\n\n\n[-0.038424, 0.1643, … 0.448928]\n\n\n[0.785001, 0.317731, … -0.012839]\n\n\n[0.243163, 1.053777, … -0.536137]\n\n\n[1.615747, -0.082606, … 0.104972]\n\n\n[-2.132016, -0.170759, … -0.313767]",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "12_unsupervised.html#umap",
    "href": "12_unsupervised.html#umap",
    "title": "12  Unsupervised Learning",
    "section": "12.4 UMAP",
    "text": "12.4 UMAP\nWhile PCA is powerful and widely applicable, it is fundamentally a linear method. The principal components are linear combinations of the original variables, which means PCA can only capture linear relationships in the data. When the underlying structure is more complex—when similar observations lie along curved surfaces or intricate manifolds in high-dimensional space—linear methods may fail to preserve important relationships.\nUniform Manifold Approximation and Projection, or UMAP, is a nonlinear dimensionality reduction technique designed to preserve both local and global structure in the data. The method works by first constructing a graph that represents the high-dimensional relationships between points, then finding a low-dimensional representation that preserves these relationships as faithfully as possible.\nThe intuition behind UMAP is that data often lie on or near a lower-dimensional manifold embedded in the high-dimensional space. Imagine a sheet of paper (a two-dimensional surface) crumpled and placed in a three-dimensional room. Points that are close together on the paper remain close when the paper is crumpled, even though they might appear far apart if we only measured straight-line distance in three dimensions. UMAP attempts to “uncrumple” such structures, finding the lower-dimensional representation that best preserves the local neighborhood relationships.\nThe mathematical details of UMAP involve concepts from topology and manifold learning that go beyond the scope of this text. In practice, what matters is understanding how the method behaves and how its parameters affect the results.\nThe DSSklearn.umap method provides access to UMAP with the same interface we used for PCA.\n\n(\n    country\n    .pipe(\n        DSSklearn.umap,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy]\n    )\n    .predict(full=True)\n    .pipe(ggplot, aes(\"dr0\", \"dr1\"))\n    + geom_point(aes(color=\"region\"))\n    + geom_text(aes(color=\"region\", label=\"iso\"), nudge_y=0.05, size=5)\n)\n\n\n\n\n\n\n\n\nComparing this visualization to the PCA result, we see both similarities and differences. Regional clusters remain visible, but the overall layout has changed. UMAP often produces tighter, more separated clusters than PCA because it prioritizes preserving local neighborhood structure. Points that were close in high-dimensional space remain close in the UMAP embedding, even if this means distorting larger-scale distances.\nTwo parameters are particularly important for controlling UMAP’s behavior. The n_neighbors parameter determines how many neighboring points UMAP considers when constructing the high-dimensional graph. Small values cause UMAP to focus on very local structure, potentially fragmenting the data into many small clusters. Large values emphasize global structure, producing smoother embeddings that may lose fine-grained detail. The default is typically 15.\nThe min_dist parameter controls how tightly UMAP packs points together in the low-dimensional representation. Small values allow points to cluster very closely, which can be useful for identifying tight groupings but may result in overlapping points that are hard to distinguish visually. Larger values spread points out more evenly. The default is 0.1.\nLet’s see how changing these parameters affects the embedding.\n\n(\n    country\n    .pipe(\n        DSSklearn.umap,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        n_neighbors=4,\n        min_dist=0.5\n    )\n    .predict(full=True)\n    .pipe(ggplot, aes(\"dr0\", \"dr1\"))\n    + geom_point(aes(color=\"region\"))\n    + geom_text(aes(color=\"region\", label=\"iso\"), nudge_y=0.05, size=5)\n)\n\n\n\n\n\n\n\n\nWith n_neighbors=4, UMAP focuses on very local relationships, which can reveal fine structure but may fragment larger patterns. The increased min_dist=0.5 spreads points apart, making individual countries easier to distinguish but potentially obscuring cluster boundaries. There is no universally correct choice of parameters—the best settings depend on the specific dataset and analytical goals.\n\n\n\n\n\n\nReproducibility\n\n\n\n\n\nUnlike PCA, which produces the same result every time given the same input, UMAP involves random initialization and stochastic optimization. Running UMAP twice on the same data may produce different embeddings. For reproducible results, you can set a random seed before fitting. The broad structure should remain similar across runs, but exact positions may vary.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "12_unsupervised.html#t-sne",
    "href": "12_unsupervised.html#t-sne",
    "title": "12  Unsupervised Learning",
    "section": "12.5 t-SNE",
    "text": "12.5 t-SNE\nt-distributed Stochastic Neighbor Embedding, or t-SNE, is another nonlinear dimensionality reduction technique that has become popular for visualizing high-dimensional data. Like UMAP, t-SNE aims to preserve local neighborhood structure, placing similar points close together in the low-dimensional embedding. The method works by converting high-dimensional distances into probabilities, then finding a low-dimensional configuration where the probability distribution matches as closely as possible.\nThe name comes from the use of the t-distribution (which we encountered in Chapter 10 for hypothesis testing) to model distances in the low-dimensional space. This choice has a specific benefit: the heavy tails of the t-distribution allow moderately distant points in high dimensions to be placed farther apart in low dimensions without incurring a large penalty. This helps t-SNE create clearer separation between clusters.\nIn mathematical terms, t-SNE first computes, for each pair of points, a probability that reflects their similarity in high-dimensional space. Points that are close together have high probability; points far apart have low probability. It then does the same in the low-dimensional embedding and adjusts the positions of points to minimize the difference between these two probability distributions. The objective function being minimized is the Kullback-Leibler divergence, a measure of how one probability distribution differs from another.\nThe interface for t-SNE matches what we have seen for PCA and UMAP.\n\n(\n    country\n    .pipe(\n        DSSklearn.tsne,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy]\n    )\n    .predict(full=True)\n    .pipe(ggplot, aes(\"dr0\", \"dr1\"))\n    + geom_point(aes(color=\"region\"))\n    + geom_text(aes(color=\"region\", label=\"iso\"), nudge_y=0.05, size=5)\n)\n\n\n\n\n\n\n\n\nThe t-SNE embedding often produces visually striking plots with clear cluster separation. However, interpreting these plots requires caution. Unlike PCA, distances in t-SNE plots are not directly meaningful—two clusters that appear far apart are not necessarily more different than two clusters that appear close together. The method is optimized for preserving local neighborhoods, not global distances.\nThe most important parameter for t-SNE is perplexity, which can be thought of as a smooth measure of how many neighbors each point considers. The perplexity is related to the effective number of nearest neighbors and typically ranges from 5 to 50. Lower values focus on very local structure, while higher values incorporate more global information. The perplexity must be smaller than the number of observations in the dataset.\n\n(\n    country\n    .pipe(\n        DSSklearn.tsne,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        perplexity=10\n    )\n    .predict(full=True)\n    .pipe(ggplot, aes(\"dr0\", \"dr1\"))\n    + geom_point(aes(color=\"region\"))\n    + geom_text(aes(color=\"region\", label=\"iso\"), nudge_y=0.05, size=5)\n)\n\n\n\n\n\n\n\n\nWith a lower perplexity of 10, the embedding focuses on smaller local neighborhoods. This can reveal finer structure within clusters but may break apart groups that a higher perplexity would keep together. As with UMAP, there is no single correct parameter choice—experimentation and domain knowledge guide the selection.\n\n\n\n\n\n\nChoosing between methods\n\n\n\n\n\nPCA, UMAP, and t-SNE each have their strengths. PCA is fast, deterministic, and interpretable—the components are linear combinations of original variables with known weights. It works well when the data have roughly linear structure. UMAP and t-SNE excel at revealing complex, nonlinear structure and often produce more visually appealing plots for exploratory analysis. Between them, UMAP tends to preserve more global structure and scales better to large datasets, while t-SNE often produces tighter, more separated clusters. For serious analysis, it is often valuable to try multiple methods and compare their results.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "12_unsupervised.html#kmeans",
    "href": "12_unsupervised.html#kmeans",
    "title": "12  Unsupervised Learning",
    "section": "12.6 KMeans",
    "text": "12.6 KMeans\nHaving covered dimensionality reduction, we now turn to clustering: the task of grouping observations so that those within the same group are more similar to each other than to those in other groups. Unlike supervised classification, where we learn to assign observations to predefined categories, clustering discovers the categories themselves from the structure of the data.\nK-means is perhaps the most widely used clustering algorithm. The method partitions observations into exactly \\(k\\) clusters, where \\(k\\) is specified by the user. Each cluster is represented by its centroid—the mean of all points assigned to that cluster. The algorithm works by iteratively assigning each observation to the nearest centroid, then updating the centroids based on the new assignments, until the assignments stabilize.\nMore formally, suppose we want to partition \\(n\\) observations into \\(k\\) clusters. Let \\(\\mu_1, \\mu_2, \\ldots, \\mu_k\\) denote the centroids of these clusters. The k-means algorithm seeks to minimize the total within-cluster sum of squares:\n\\[\n\\sum_{j=1}^{k} \\sum_{i \\in C_j} \\left( x_{i1} - \\mu_{j1} \\right)^2 + \\left( x_{i2} - \\mu_{j2} \\right)^2 + \\cdots + \\left( x_{ip} - \\mu_{jp} \\right)^2\n\\]\nHere \\(C_j\\) denotes the set of observations assigned to cluster \\(j\\), and the inner sum measures how spread out the observations are around their cluster’s centroid. The algorithm minimizes this objective by alternating between two steps: assign each observation to the cluster with the nearest centroid, and update each centroid to be the mean of its assigned observations. This process repeats until no observations change their cluster assignment.\nThe DSSklearn.kmeans method provides access to k-means. The n_clusters parameter specifies the number of clusters to create.\n\n(\n    country\n    .pipe(\n        DSSklearn.kmeans,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        n_clusters=5\n    )\n    .predict()\n)\n\n\nshape: (135, 2)\n\n\n\nlabel_\ndist_\n\n\ni64\nf64\n\n\n\n\n1\n0.898742\n\n\n3\n0.432613\n\n\n2\n1.437611\n\n\n4\n0.736308\n\n\n1\n1.804269\n\n\n…\n…\n\n\n3\n0.430951\n\n\n2\n0.82221\n\n\n3\n1.229334\n\n\n2\n0.640985\n\n\n1\n0.282839\n\n\n\n\n\n\nThe output includes two columns: label_, an integer indicating which cluster each observation belongs to, and dist_, the distance from each observation to its assigned cluster centroid. Observations with larger distances are farther from the center of their cluster and might be considered less typical members of that group.\nTo visualize the clustering results, we can combine k-means with dimensionality reduction. First we cluster the data in the original four-dimensional space, then we project the data to two dimensions using PCA for visualization. This approach lets k-means use all available information for clustering while still allowing us to see the results.\n\n(\n    country\n    .pipe(\n        DSSklearn.kmeans,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        n_clusters=5\n    )\n    .predict(full=True)\n    .pipe(\n        DSSklearn.pca,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy]\n    )\n    .predict(full=True)\n    .pipe(ggplot, aes(\"dr0\", \"dr1\"))\n    + geom_point(aes(color=\"factor(label_)\"))\n)\n\n\n\n\n\n\n\n\nThe plot shows countries colored by their k-means cluster assignment, positioned according to their PCA coordinates. We can see that k-means has partitioned the countries into five groups that correspond roughly to different levels of development and well-being. The clusters are reasonably compact in this projection, though some overlap is visible—this is expected when projecting from four dimensions to two.\n\n\n\n\n\n\nChoosing k\n\n\n\n\n\nA fundamental challenge with k-means is selecting the number of clusters. The algorithm requires us to specify \\(k\\) in advance, but the “correct” number of clusters is often unknown. Several heuristics can help guide this choice. The elbow method plots the total within-cluster sum of squares against different values of \\(k\\) and looks for a point where adding more clusters yields diminishing returns. The silhouette method measures how similar each observation is to its own cluster compared to other clusters. Ultimately, domain knowledge often plays the most important role—the number of clusters should make substantive sense for the problem at hand.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "12_unsupervised.html#dbscan",
    "href": "12_unsupervised.html#dbscan",
    "title": "12  Unsupervised Learning",
    "section": "12.7 DBSCAN",
    "text": "12.7 DBSCAN\nK-means has several limitations. It requires specifying the number of clusters in advance, assumes clusters are roughly spherical (since it uses distance to centroids), and assigns every observation to exactly one cluster. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) addresses these limitations through a fundamentally different approach based on the density of points in the feature space.\nThe core idea of DBSCAN is that clusters are dense regions of points separated by sparser regions. The algorithm identifies two types of points: core points that have at least a minimum number of neighbors within a specified distance, and border points that fall within the neighborhood of a core point but don’t have enough neighbors themselves to be core points. Points that are neither core nor border points are classified as noise and not assigned to any cluster.\nTwo parameters control DBSCAN’s behavior. The eps parameter (epsilon) specifies the maximum distance between two points for them to be considered neighbors. The min_samples parameter specifies how many neighbors a point needs within distance eps to be considered a core point. Clusters form by connecting core points that are neighbors of each other, along with any border points in their neighborhoods.\nMore precisely, a point \\(x_i\\) is a core point if at least min_samples points (including itself) lie within distance eps of \\(x_i\\). Two core points belong to the same cluster if they are within distance eps of each other, or if there is a chain of core points connecting them where each consecutive pair is within distance eps. Border points are assigned to the cluster of the nearest core point within their eps neighborhood.\nLet’s apply DBSCAN to the country data with default parameters.\n\n(\n    country\n    .pipe(\n        DSSklearn.dbscan,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy]\n    )\n    .predict()\n)\n\n\nshape: (135, 2)\n\n\n\nlabel_\ndist_\n\n\ni64\nf64\n\n\n\n\n-1\nNaN\n\n\n0\n0.500632\n\n\n-1\nNaN\n\n\n-1\nNaN\n\n\n-1\nNaN\n\n\n…\n…\n\n\n0\n0.446973\n\n\n3\n0.765656\n\n\n-1\nNaN\n\n\n3\n0.497674\n\n\n1\n0.305086\n\n\n\n\n\n\nThe output shows cluster labels for each country. A label of -1 indicates that the observation was classified as noise—it doesn’t belong to any cluster. This is a key difference from k-means, which forces every observation into a cluster.\nWe can visualize the DBSCAN results using the same approach we used for k-means: cluster in the original space, then project to two dimensions for plotting.\n\n(\n    country\n    .pipe(\n        DSSklearn.dbscan,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n    )\n    .predict(full=True)\n    .pipe(\n        DSSklearn.pca,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy]\n    )\n    .predict(full=True)\n    .pipe(ggplot, aes(\"dr0\", \"dr1\"))\n    + geom_point(aes(color=\"factor(label_)\"))\n)\n\n\n\n\n\n\n\n\nWith default parameters, DBSCAN identifies distinct clusters while leaving some points unassigned. The noise points (label -1) often correspond to countries that don’t fit neatly into any dense group—they may have unusual combinations of characteristics that make them outliers.\nThe eps parameter is the most important tuning parameter for DBSCAN. Smaller values of eps require points to be closer together to be considered neighbors, resulting in more and smaller clusters. Larger values allow more distant points to be grouped together, producing fewer and larger clusters. Finding the right value requires experimentation and understanding of the data’s scale.\n\n(\n    country\n    .pipe(\n        DSSklearn.dbscan,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        eps=1\n    )\n    .predict(full=True)\n    .pipe(\n        DSSklearn.pca,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy]\n    )\n    .predict(full=True)\n    .pipe(ggplot, aes(\"dr0\", \"dr1\"))\n    + geom_point(aes(color=\"factor(label_)\"))\n)\n\n\n\n\n\n\n\n\nWith eps=1, the neighborhood radius is much larger, and DBSCAN groups almost all countries into a single cluster. The few remaining points are either in their own small cluster or classified as noise. This illustrates how sensitive DBSCAN can be to parameter choices—the structure it finds depends heavily on what we consider “close enough” to be neighbors.\n\n\n\n\n\n\nK-means versus DBSCAN\n\n\n\n\n\nThese two clustering algorithms make different assumptions and excel in different situations. K-means works well when clusters are roughly spherical and similar in size, when you have a good sense of how many clusters to expect, and when every observation should belong to some cluster. DBSCAN works well when clusters have irregular shapes, when the number of clusters is unknown, and when some observations may genuinely be outliers that don’t belong to any group. In practice, trying both methods and comparing their results often provides useful insights about the structure of the data.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "12_unsupervised.html#unsupervised-supervised",
    "href": "12_unsupervised.html#unsupervised-supervised",
    "title": "12  Unsupervised Learning",
    "section": "12.8 Unsupervised + Supervised",
    "text": "12.8 Unsupervised + Supervised\nThe unsupervised methods we have covered—dimensionality reduction and clustering—are valuable not only for exploration and visualization but also as preprocessing steps for supervised learning. Reducing the dimensionality of features can improve the performance and interpretability of predictive models, while cluster labels can serve as new features that capture structure not easily represented by the original variables.\nTo illustrate this combination, let’s use PCA to reduce our four country indicators to two principal components, then use these components as features in a regression model predicting life expectancy.\n\nmodel = (\n    country\n    .pipe(\n        DSSklearn.pca,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        n_components=2\n    )\n    .predict(full=True, array=True)\n    .pipe(\n        DSSklearn.elastic_net_cv,\n        target=c.lexp,\n        features=[c.dr],\n        l1_ratio=1\n    )\n)\n\n\nmodel.coef()\n\n\nshape: (3, 2)\n\n\n\nname\nparam\n\n\nstr\nf64\n\n\n\n\n\"Intercept\"\n75.147766\n\n\n\"col0\"\n5.659064\n\n\n\"col1\"\n-0.507537\n\n\n\n\n\n\nThe model uses the two principal components (stored in the array column dr) to predict life expectancy. The coefficients tell us how changes in each principal component relate to changes in life expectancy. Since the first principal component typically captures overall development, we would expect it to have a strong positive relationship with life expectancy.\nWe can apply the same approach to classification tasks. Here we use the principal components to predict which region a country belongs to.\n\nmodel = (\n    country\n    .pipe(\n        DSSklearn.pca,\n        features=[c.hdi, c.gdp, c.cellphone, c.happy],\n        n_components=2\n    )\n    .predict(full=True, array=True)\n    .pipe(\n        DSSklearn.logistic_regression_cv,\n        target=c.region,\n        features=[c.dr],\n        l1_ratios=[1],\n        solver=\"saga\"\n    )\n)\n\n\nmodel.coef()\n\n\nshape: (3, 6)\n\n\n\nname\nAfrica\nAmericas\nAsia\nEurope\nOceania\n\n\nstr\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Intercept\"\n-0.478412\n0.792121\n1.467526\n0.53761\n-2.318845\n\n\n\"col0\"\n-3.469932\n0.0\n-0.681216\n1.329494\n1.502739\n\n\n\"col1\"\n1.129954\n0.439896\n-0.03993\n-0.040427\n-0.365391\n\n\n\n\n\n\nThe coefficient table now shows, for each region, how the two principal components contribute to the probability of a country belonging to that region. The multinomial logistic regression (introduced in Chapter 11) estimates these relationships jointly across all regions.\n\nmodel.predict(full=True)\n\n\nshape: (135, 19)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\nlang\ndr\nindex_\ntarget_\nprediction_\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nstr\narray[f64, 2]\nstr\nstr\nstr\n\n\n\n\n\"SEN\"\n\"Senegal\"\n\"Africa\"\n\"Western Africa\"\n18.932\n70.43\n14.366667\n-14.283333\n0.53\n4871\n38.1\n50.93\n66.0\n54.93987\n\"pbp|fra|wol\"\n[-1.758354, 0.147961]\n\"train\"\n\"Africa\"\n\"Africa\"\n\n\n\"VEN\"\n\"Venezuela, Bolivarian Republic…\n\"Americas\"\n\"South America\"\n28.517\n76.18\n8.0\n-67.0\n0.709\n8899\n44.8\n57.65\n96.8\n95.66913\n\"spa|vsl\"\n[-0.394464, 0.548632]\n\"train\"\n\"Americas\"\n\"Asia\"\n\n\n\"FIN\"\n\"Finland\"\n\"Europe\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n\"fin|swe\"\n[2.891235, 0.426905]\n\"test\"\n\"Europe\"\n\"Europe\"\n\n\n\"USA\"\n\"United States of America\"\n\"Americas\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n\"eng\"\n[1.921109, -1.266114]\n\"train\"\n\"Americas\"\n\"Europe\"\n\n\n\"LKA\"\n\"Sri Lanka\"\n\"Asia\"\n\"Southern Asia\"\n23.229\n78.51\n7.0\n81.0\n0.776\n14380\n39.3\n36.02\n83.1\n90.77437\n\"sin|sin|tam|tam\"\n[-1.176426, 0.257248]\n\"train\"\n\"Asia\"\n\"Africa\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"ALB\"\n\"Albania\"\n\"Europe\"\n\"Southern Europe\"\n2.772\n79.67\n41.0\n20.0\n0.81\n20362\n30.8\n54.45\n91.9\n98.5473\n\"sqi\"\n[-0.038424, 0.1643]\n\"train\"\n\"Europe\"\n\"Asia\"\n\n\n\"MYS\"\n\"Malaysia\"\n\"Asia\"\n\"South-eastern Asia\"\n35.978\n76.03\n3.7805111\n102.314362\n0.819\n35990\n46.2\n58.68\n118.2\n95.69194\n\"msa\"\n[0.785001, 0.317731]\n\"test\"\n\"Asia\"\n\"Europe\"\n\n\n\"SLV\"\n\"El Salvador\"\n\"Americas\"\n\"Central America\"\n6.366\n76.98\n13.668889\n-88.866111\n0.678\n12221\n38.3\n64.82\n126.9\n86.19786\n\"spa\"\n[0.243163, 1.053777]\n\"train\"\n\"Americas\"\n\"Americas\"\n\n\n\"CYP\"\n\"Cyprus\"\n\"Asia\"\n\"Western Asia\"\n1.371\n81.77\n35.0\n33.0\n0.913\n55720\n31.2\n60.71\n123.1\n99.41781\n\"ell|tur\"\n[1.615747, -0.082606]\n\"train\"\n\"Asia\"\n\"Europe\"\n\n\n\"PAK\"\n\"Pakistan\"\n\"Asia\"\n\"Southern Asia\"\n255.22\n66.71\n30.0\n71.0\n0.544\n5717\n29.6\n45.49\n49.8\n61.92651\n\"eng|urd\"\n[-2.132016, -0.170759]\n\"train\"\n\"Asia\"\n\"Africa\"\n\n\n\n\n\n\nThe predictions show the most likely region for each country based solely on its principal component scores. This demonstrates a complete pipeline: we start with four economic and social indicators, compress them to two dimensions that capture the most important variation, and then use those dimensions to predict a categorical outcome. The dimensionality reduction simplifies the modeling problem while (ideally) retaining the information most relevant for prediction.\n\n\n\n\n\n\nWhen to reduce dimensions\n\n\n\n\n\nDimensionality reduction before supervised learning is not always beneficial. When the original features are few and interpretable, using them directly may produce more useful models. When features are many and correlated, or when visualization of the feature space would aid interpretation, dimensionality reduction can help. The choice depends on the specific problem, and comparing models with and without reduction is often worthwhile.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "12_unsupervised.html#conclusion",
    "href": "12_unsupervised.html#conclusion",
    "title": "12  Unsupervised Learning",
    "section": "12.9 Conclusion",
    "text": "12.9 Conclusion\nThis chapter introduced the core methods of unsupervised learning: techniques for finding structure in data without the guidance of labeled outcomes. We covered two main tasks—dimensionality reduction and clustering—each with multiple algorithmic approaches suited to different situations.\nFor dimensionality reduction, we examined PCA as the foundational linear method that finds directions of maximum variance, and UMAP and t-SNE as nonlinear alternatives that preserve local neighborhood structure for visualization. These methods compress high-dimensional data into a form that can be plotted, interpreted, and used as input to other analyses.\nFor clustering, we contrasted k-means, which partitions data into a specified number of spherical clusters, with DBSCAN, which discovers clusters of arbitrary shape based on density and can identify outliers. Both methods reveal groupings in data that may correspond to meaningful categories in the domain of application.\nThroughout, we emphasized that unsupervised learning lacks the clear evaluation criteria of supervised learning. There is no ground truth against which to measure success. Whether a dimensionality reduction captures meaningful structure or a clustering produces useful groups depends on the goals of the analysis and requires human judgment to assess. This makes unsupervised methods particularly valuable for exploration—they can reveal patterns and generate hypotheses that might not be apparent from examining variables individually.\nFinally, we demonstrated that unsupervised and supervised methods can work together. Dimensionality reduction can preprocess features before fitting a predictive model, and cluster labels can become features in their own right. The methods in this chapter expand the toolkit available for understanding data, whether that understanding is the ultimate goal or a stepping stone toward prediction.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "13_deeplearning.html",
    "href": "13_deeplearning.html",
    "title": "13  Deep Learning",
    "section": "",
    "text": "13.1 Setup\nLoad all of the modules and datasets needed for the chapter. We load two submodules of the torch module to build neural network models. The mnist dataset contains a sample of 1,000 images of handwritten digits from the well-known MNIST collection. Full details are available in Chapter 22.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nmnist = pl.read_csv(\"data/mnist_1000.csv\")",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "13_deeplearning.html#introduction",
    "href": "13_deeplearning.html#introduction",
    "title": "13  Deep Learning",
    "section": "13.2 Introduction",
    "text": "13.2 Introduction\nThe main idea behind deep learning is that instead of building a model, such as linear regression, that goes from the inputs to the predictions, we should generate intermediate results that summarize the important information in the inputs. The summarized information can then be used to produce the final predictions. The idea can be expanded to many layers of intermediate representations. This is where the term “deep” come from, indicating that there is a large distance between the things we put into the model and the things that come out of it. The earliest deep models drew from biological analogies to neurons in the brain, which is why the models also became known as neural networks. These ideas have been incredibly powerful, perhaps the single most important idea in computer science in the last 50 years, and have been behind almost all of the growth in machine learning and AI over the past two decades, including the rise of generative text models such as ChatGPT.\nThe power of deep learning is most apparent in application domains where the input data is only connected to the things we want to know about the data in a complex and/or abstract way. This is true, for example, with text analysis where individual letters only have meaning when put together in specific patterns and orders. The same goes for the air pressure measurements that are recorded in sound data and the individual pixels that represent images. A sequence of sound pressures gets interpreted as a specific song only through a complex relationship that is hard to explain but easy for our ears and brains to decode. The inner representations within a deep learning model work to transform the raw data into things that more closely have a connection to the things they represent.\nTraining deep learning models requires more hands-on background knowledge than kinds of models we saw in Chapters 13 and 14. While there is a lot of theory of computational depth in linear and logistic regression models, once experts have coded the algorithms to learn from data we can (mostly) apply them without understanding the mechanics of how the models determine the final weights. This is not so with deep learning because there are far too many different ways that different sets of intermediate results could produce the same predictions. Choosing “good” representations is the key to making the model work well on data that it has not been trained on. This is something that is attainable by anyone reading this book (it’s not magic), but requires some practice and time.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "13_deeplearning.html#mnist-data",
    "href": "13_deeplearning.html#mnist-data",
    "title": "13  Deep Learning",
    "section": "13.3 MNIST Data",
    "text": "13.3 MNIST Data\nTo understand the true power of deep learning we need to work with a dataset where there is a disconnect between the information contained in the object and the raw format of the data that we are given. This, for example, is the case with text, image, video, and sound data. There is a wide gap between the data stored on one hand in the individual pixels of an image, characters in a text, or sound pressure at a specific millisecond and the semantic meaning represented by these objects when processed by the human perceptive system.\nIn order to better understand how this works, we will work in this chapter with a subset of the MNIST (Modified National Institute of Standards and Technology database) hand-written digits dataset. This is one of the earliest popular image processing datasets. It consists of small, standardized images of hand-written digits from 0 to 9. The goal is to build a supervised model that recognizes which digit is written based on the image. All of the image datasets in this text use a format where we have metadata about the images in one table. This table includes a filepath column that contains the location of each of the image files. In this case, each image is a greyscale file of a square image that is 28 pixels tall and wide. Here is the dataset:\n\nmnist\n\n\nshape: (1_000, 3)\n\n\n\nlabel\nfilepath\nindex\n\n\ni64\nstr\nstr\n\n\n\n\n3\n\"media/mnist_1000/00000.png\"\n\"test\"\n\n\n9\n\"media/mnist_1000/00001.png\"\n\"test\"\n\n\n9\n\"media/mnist_1000/00002.png\"\n\"train\"\n\n\n8\n\"media/mnist_1000/00003.png\"\n\"test\"\n\n\n8\n\"media/mnist_1000/00004.png\"\n\"train\"\n\n\n…\n…\n…\n\n\n4\n\"media/mnist_1000/00995.png\"\n\"train\"\n\n\n4\n\"media/mnist_1000/00996.png\"\n\"train\"\n\n\n6\n\"media/mnist_1000/00997.png\"\n\"test\"\n\n\n4\n\"media/mnist_1000/00998.png\"\n\"train\"\n\n\n7\n\"media/mnist_1000/00999.png\"\n\"test\"\n\n\n\n\n\n\nWe have a helper function called DSImage.plot_image_grid that allows us to see a collection of the images from within Python along with their labels. Here is an example of what the data looks like for 30 randomly selected images.\n\nDSImage.plot_image_grid(mnist.sample(30))\n\n\n\n\n\n\n\n\nIn order to load the data into Python, we have another helper function called DSTorch.load_image that takes the dataset and some parameters and generates objects corresponding to the features and responses of the data. We have both the data split into a training and testing set as well as the complete original data. The latter is useful if we later want to add information back into the mnist table.\n\nX, X_train, X_test, y, y_train, y_test, cn = DSTorch.load_image(\n    mnist, scale=True, flatten=True\n)\n\nNotice that we set the option flatten to True above. This takes the pixel data from the images and converts them to a single vector that has no implicit encoding of the shape of the image. In this case, this means that the 28-by-28 grid of pixel intensities has been converted into a single string of \\(28 * 28 = 784\\) values. We can see the shape of the X object below.\n\nX.shape\n\ntorch.Size([1000, 784])\n\n\nAll of the objects returned by DSTorch.load_image other than cn (the category names) are special objects called a Tensor created by the torch library. These are the formats that we will need to run deep-learning models in Python. The category names are returned because the y Tensor needs to be integer codes rather than the names of the labels. In the special case of MNIST these are the same (the digits are already integer codes). In nearly every other application this will be different and having the original names will be very helpful.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "13_deeplearning.html#dense-neural-networks",
    "href": "13_deeplearning.html#dense-neural-networks",
    "title": "13  Deep Learning",
    "section": "13.4 Dense Neural Networks",
    "text": "13.4 Dense Neural Networks\nLet’s say we have a dataset with \\(n\\) features that we want to use to predict an output. We’ve seen how to do linear regression with a linear combination of the parameters. We can modify this slightly by creating an intermediate value \\(h\\) that is a linear combination in exactly the same way and then produce the prediction for the output \\(y\\) as a simple linear combination of \\(h\\). Mathematically, we can write this as:\n\\[\n\\begin{aligned}\nh &= \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\cdots + \\beta_n \\cdot x_n \\\\\ny &= \\gamma_0 + \\gamma_1 \\cdot h\n\\end{aligned}\n\\]\nThere are a few things to take note of here. First of all, the model as written is not fundamentally any different than linear regression. A linear combination of linear combinations is just another linear combination. We simply have more ways of getting to the same output. The second thing is that the value of \\(h\\) is not something we actually observe in the data. It is only an intermediate value that helps us go from the input values \\(x_j\\) to the output value \\(y\\). We use the letter \\(h\\) here because this value relates to a hidden state of the model in the sense that no data we observe is directly related to it.\nIn order to do something that is not possible with normal regressions, we need to add something non-linear (not just adding and multiplying constants) into the model. The typical way to do this is to add a pre-determined function that gets applied to the result before creating the value \\(h\\). This function is called an activation function. It is often written with the symbol \\(\\sigma(\\cdot)\\) (sigma) because the earliest models used a sigmoid function for this. We can modify our equation as follows.\n\\[\n\\begin{aligned}\nh &= \\sigma\\left(\\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\cdots + \\beta_n \\cdot x_n\\right) \\\\\ny &= \\gamma_0 + \\gamma_1 \\cdot h\n\\end{aligned}\n\\]\nMost models now use simpler activation functions than the sigmoid. The most common choice is a rectified linear unit (ReLU). While the name sounds complicated, the function is actually very simple. It is equal to the identity function for positive inputs while mapping all negative values to zero. Mathematically, we can write this as:\n\\[\n\\text{ReLU}(z) = \\max(0, z)\n\\]\nThe key features of an activation function are that it must be differentiable at almost all points and it must be non-linear. The ReLU satisfies both requirements while being computationally efficient to evaluate.\nWe can visually think about the mathematical formulation we now have as a network diagram such as the one in Figure Figure 13.1. For every observation, we have a number associated with each of the inputs in the left. Then, if we know all of the slopes and intercepts defined by the model, we can compute the hidden state \\(h\\) and the output value \\(y\\).\n\n\n\n\n\n\nFigure 13.1: A simple neural network with a single hidden state.\n\n\n\nWith the activation function we can now generate ways of producing estimates \\(y\\) from our input data that are not strictly the same as those from linear regression. However, we are still very constrained in the kinds of models we can build, and the overall complexity of what we can represent is essentially the same. In order to produce more complex models we need to include a larger set of hidden states, all of which are individual linear combinations of the inputs followed by the activation function. Then, the output \\(y\\) is a linear combination of all of these outputs. Below is a mathematical description of this model. Figure Figure 13.2 shows the corresponding visualization of adding more hidden states into our model.\n\\[\n\\begin{aligned}\nh_1 &= \\sigma\\left(\\beta_{1,0} + \\beta_{1,1} \\cdot x_1 + \\beta_{1,2} \\cdot x_2 + \\cdots + \\beta_{1,n} \\cdot x_n\\right) \\\\\n&\\,\\,\\vdots \\\\\nh_m &= \\sigma\\left(\\beta_{m,0} + \\beta_{m,1} \\cdot x_1 + \\beta_{m,2} \\cdot x_2 + \\cdots + \\beta_{m,n} \\cdot x_n\\right) \\\\\ny &= \\gamma_0 + \\gamma_1 \\cdot h_1 + \\cdots + \\gamma_m \\cdot h_m\n\\end{aligned}\n\\]\n\n\n\n\n\n\nFigure 13.2: A neural network with a single layer of \\(m\\) hidden states.\n\n\n\nThe model above is an example of the basic building blocks of a dense, shallow neural network. A very-well known result about neural networks shows that given enough hidden states, any well-behaved real-valued function of \\(n\\) dimensions can be approximated by such as model [1]. This property makes neural networks—even simple, shallow ones—a type of estimator known as a universal approximator.\nIn order to see the real power of neural networks, we need to expand the model not just by the number of hidden states, but also by the number of hidden layers themselves. In other words, we can create a collection of hidden intermediate values that are combinations for the hidden values we created in the first layer and then use these second-order hidden layers to construct the output. We won’t attempt to write this out mathematically because the notation is already getting complicated. But, we can visualize this fairly straightforwardly as shown in Figure Figure 13.3.\n\n\n\n\n\n\nFigure 13.3: A dense neural network with two hidden layers Here the second layer has four hidden states, but in general can have as many as needed.\n\n\n\nWe can continue in the same way by increasing the number of layers and adjusting the number of hidden states in each. As we increase the number of layers (the exact cut-off is debated; certainly after 10) we arrive at a deep neural network. It turns out that while even a one-hidden layer model can approximate any function, it is much quicker to approximate a function increasing the number of layers rather than the number of states within a layer. At the same time, it becomes more and more difficult to learn how to make the model do so, particularly when we are using large complex training data. In the next section we will investigate how we actually train these models using data.\n\n\n\n\n\n\nInterpreting parameters\n\n\n\n\n\nUnlike linear regression, where each coefficient directly measures the effect of a one-unit change in a predictor on the response, the parameters in a neural network have no straightforward interpretation. The weights connecting one hidden layer to another describe relationships between intermediate representations that have no direct connection to either the original inputs or the final output. Combined with the non-linear activation functions applied at each layer, it becomes essentially impossible to understand what any individual parameter does in a network of even modest size. This is why neural networks are sometimes called black-box models. We can observe what goes in and what comes out, but the internal workings remain opaque. Various research efforts have attempted to interpret neural network representations, with some success in specific domains like image classification, but this remains an active and challenging area of study.\n\n\n\nBefore we continue, it will be useful to modify our visual concept of a neural network beyond the complex web of circles and arrows that we have used so far. Instead of thinking about each hidden state as a separate entity, it will be much more scalable to think about each layer as a concrete unit. The typical way that you will see neural networks described in papers and in code is through the format shown in Figure Figure 13.4. This abstraction explains all of the information that we need to know other than the size of the input and the number of hidden states in each linear unit. Once we specify those, the model is completely described by this relatively simple diagram.\n\n\n\n\n\n\nFigure 13.4: An abstraction of how a neural network is constructed.\n\n\n\nOne thing that become clear with the abstract form of the neural network is that the process of going from the last hidden layer to the output is structurally the same as creating any of the internal hidden layers. In other words, these are all just linear units that consist of a linear combination of all the output from the previous states. The only difference is that the final Linear unit for a regression problem would always have only a single output that corresponds with the value we want to predict.\nAlmost all of the deep learning tasks we will look at involve classification. The fact that the output is structurally no different from the other layers indicates a way to using this to easily extend to classification. We can change the last layer to output one prediction for each category rather than only a single number. Then, we can apply a special activation function called as Softmax that turns a collection of number into a valid set of probabilities (all non-negative numbers that sum to one). Figure Figure 13.5 illustrates how this can be seen as one additional layer on the neural network.\n\n\n\n\n\n\nFigure 13.5: Classification variation of a neural network with a final softmax layer.\n\n\n\nWe will continue to see throughout this chapter that deep neural networks can be extended by adding new layer types and combinations to work with increasingly large and complex datasets.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "13_deeplearning.html#stochastic-gradient-descent",
    "href": "13_deeplearning.html#stochastic-gradient-descent",
    "title": "13  Deep Learning",
    "section": "13.5 Stochastic Gradient Descent",
    "text": "13.5 Stochastic Gradient Descent\nWhen we introduced linear regression in Chapter 10 it was mentioned that the slopes and intercepts in the model were fit using ordinary least squares (OLS). In other words, we find the parameters that minimize the sum of squared differences between the model predictions and the observed data. There is a well known formula for solving OLS and we didn’t go any more into the computational details. We let statsmodels compute the values for us and moved on to the next step. When discussing penalized regression and gradient boosted trees in Chapter 11 we mentioned a few more details but still avoiding discussing too many computational details about the process. There are excellent algorithms already coded into sklearn that are able to find the optimal solutions in a straightforward way with no manual intervention. Neural networks are computationally much more complicated and will require some understanding of the way they are trained with data.\nWe will motivate the technique used in training neural networks with an easy to visualize one-dimensional example of trying to minimize a relatively smooth function. Of course, the ideal way to find the minimum of a function is to compute its derivative and determine where this derivative is equal to zero. In many cases, though, it is not possible to compute the derivative for all of the points and even when we can it may not be possible to figure out at what points that function is equal to zero. An alternative is to do something iterative: we start at an initial guess, compute measurements about the function in a small neighborhood of this point, and then use this information to find a new point that should be closer to the optimal value. We will look at a few different approaches of this type using a single function where our initial guess is near -2.\nTo start, we will compute the first and second derivative of the function at our initial starting point. This allows us to construct a quadratic approximation of the function. A quadratic function will always “turn around” somewhere. In other words, there will be an easy to compute minimum for the approximation of this approximation of the function. A straightforward approach is, therefore, to jump to the minimum of this approximation and then start the process all over again. We can do this until the process converges to a point, which it does in our example very quickly.\n\n\n\n\n\n\nFigure 13.6: A second-order method for finding the minimum value of a function.\n\n\n\nThe technique above is called a second-order method because it uses the second derivative of the function to compute the minimum value. These techniques are very fast and for functions with unique minima have strong convergence results. Second-order methods are used throughout statistics and the sciences as the go-to method when there are no analytic solutions available, as is often the case when we go beyond the most basic techniques. Unfortunately, these are not feasible for training neural networks. The higher-dimensional analog of a second-derivative is a Hessian matrix, which in \\(d\\) dimensions requires \\(d^2\\) parameters. Modern neural networks often have dozens of billions of parameters. Storing a single Hessian matrix would take hundreds of Exabytes, or approximately 1 million hours of HD video. Clearly this is not a practical approach for such large models even if we had enough data to reliably compute the Hessian matrix, which we do not.\nAn alternative is to use a first-order method that is based only on the first derivative of the function at a given point. The multidimensional equivalent is the gradient. It requires only \\(d\\) parameters in \\(d\\) dimensions (one partial derivative for each dimension) and is therefore reasonable to both compute and store. The difficulty is that the derivative approximates a function by a line. This will tell us which direction to go in to find the minimum. By way of the slope, it may possibly also give some idea of how fast it is descending. However, since a line never turns around we have no idea how far to go in the direction of the derivative. The standard solution is to pick a positive value represented by \\(\\eta\\) (eta) called the learning rate. We then update our guess of the optimal value by using the following formula (the negative sign because we are moving in the opposite direction of the derivative when minimizing the function):\n\\[\nx \\rightarrow x - \\eta \\times \\frac{d}{dx}(x)\n\\]\nThis technique is called gradient descent. The visualization below shows an example of gradient descent with a relatively small value of \\(\\eta\\). The algorithm eventually finds the minimum, but only after taking a large number of small steps.\n\n\n\n\n\n\nFigure 13.7: Gradient descent with a small learning rate.\n\n\n\nIn the example below we use a larger value for \\(\\eta\\). The first two steps are approach the minimum much quicker, but the increased learning rate eventually causes the estimates to bounce back and forth on other side of the minimum value and it never seems to completely converge to the optimal point (at least during the cycle of the simulation).\n\n\n\n\n\n\nFigure 13.8: Gradient descent with a large learning rate.\n\n\n\nThe two examples above illustrate how the choice of the learning rate greatly affects the ability of even a simple one-dimensional example to converge to a minimum value. Too low and we do not move fast enough; too large and we bounce around the minimum value and never converge. These do illustrate, though, two ways of trying to improve the performance of gradient descent. First of all, we can slowly decrease the learning rate over time. Bigger steps are great to make quick progress to decent values and then smaller steps can be used to refine the best parameters in a local neighborhood. Secondly, we can introduce a momentum term that keeps track of how the derivatives change over time. When, as in the first case, they continue to increase in the same direction we can pick up the pace of the steps. In the second case, the derivatives keep shifting directions, indicating that we should take very small steps to center into the minimum value. We can describe this by setting a momentum factor \\(\\mu\\), keeping track of a velocity value \\(v\\) and updating as follows:\n\\[\n\\begin{aligned}\nv &\\rightarrow \\mu \\times v - \\eta \\times \\frac{d}{dx}(x) \\\\\nx &\\rightarrow x + v \\\\\n\\end{aligned}\n\\]\nA visualization of this is given below, where we see that the convergence happens quicker than in the low learning rate case but does not overshoot the target as in the high learning rate case.\n\n\n\n\n\n\nFigure 13.9: Gradient descent with a momentum term.\n\n\n\nNote that all of the methods introduced here, including the powerful second-order methods, have the possibility of getting stuck in a local minimum, such as the one in our example around \\(x=1\\). For example, below is a visualization in which our momentum term is turned too high and we ultimately get into the second minima.\n\n\n\n\n\n\nFigure 13.10: Optimization\n\n\n\nWhile the models we saw in previous chapters have convex functions that define their optimal values, the complex nature of the parameters in neural networks causes them to have a very large number of local optima. This means that we never find the true minimum. Rather, the goal is simply to find a value of the parameters that produces reasonably predictive results.\nWe’ve talked a lot in the abstract about optimization here. How does this actually get put into practice with neural networks? As with OLS or logistic regression, we can train a neural network with training data to find the parameters (the slopes and intercepts in all of the hidden layers) that attempt to minimize the RMSE or classification error rate. This is done using a modified version of the methods introduced above called stochastic gradient descent (SGD). The only core difference is that rather than using all of the (often very large) data to determine the derivatives at each step, we instead take a small step using data from only a small set of the observations in the training data. We do this over and over again until we have used every data point once, called an epoch, and then start over again.\n\n\n\n\n\n\nBackpropagation\n\n\n\n\n\nComputing the gradient in a neural network requires determining how changes in each parameter affect the final loss function. This is complicated because parameters in early layers influence the output only indirectly, through many subsequent layers. The solution is an algorithm called backpropagation, which applies the chain rule from calculus systematically through the network. Starting from the output layer and working backward toward the input, backpropagation computes how much each parameter contributed to the prediction error. For a given batch of training examples, we first run the forward pass to compute predictions, then run the backward pass to compute gradients, and finally update the parameters using those gradients. Modern deep learning libraries like PyTorch handle backpropagation automatically, building a computational graph during the forward pass that records all operations, then traversing this graph in reverse to compute gradients efficiently.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "13_deeplearning.html#dense-nn-in-pytorch",
    "href": "13_deeplearning.html#dense-nn-in-pytorch",
    "title": "13  Deep Learning",
    "section": "13.6 Dense NN in PyTorch",
    "text": "13.6 Dense NN in PyTorch\nWe are now ready to implement a dense neural network in Python using PyTorch. PyTorch is one of the two dominant deep learning libraries (the other being TensorFlow). It provides the building blocks for constructing neural networks along with efficient implementations of stochastic gradient descent and backpropagation.\nIn PyTorch, we define a neural network by creating a class that inherits from nn.Module. The class must define two methods: __init__, which sets up the layers of the network, and forward, which describes how data flows through those layers. The code below defines a dense neural network for classifying our MNIST digits.\n\nclass DenseNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(784, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nThe architecture defined above uses nn.Sequential to chain together the layers in order. The first nn.Linear(784, 128) creates a fully connected layer that takes our 784-dimensional input (the flattened 28×28 pixel image) and produces 128 hidden values. Each of the 784 inputs is connected to each of the 128 outputs through a learned weight, plus a bias term for each output. This layer alone contains \\(784 \\times 128 + 128 = 100{,}480\\) trainable parameters. The ReLU activation function is applied after this linear transformation to introduce non-linearity.\nThe second linear layer reduces from 128 hidden values down to 64, again followed by a ReLU activation. The final layer maps from 64 values to 10 outputs, one for each digit class (0 through 9). Notice that there is no activation function after the final layer. This is because we will use a loss function during training that internally applies the softmax transformation and computes the cross-entropy loss, which is the standard approach for multi-class classification problems.\nWith the network architecture defined, we next create an instance of the model and an optimizer. The optimizer implements the gradient descent update rule. Here we use the Adam optimizer, which implements a version of stochastic gradient descent with an adaptive learning rate. We set the initial learning rate to 0.001, a good starting point for this particular model.\n\nmodel = DenseNet()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nTraining the model involves repeatedly showing it batches of training examples and updating the parameters to reduce the prediction error. The DSTorch.train method handles this process. It takes the model, optimizer, training data, and several hyperparameters: the number of epochs (complete passes through the training data) and the batch size (number of examples processed before each parameter update).\n\nDSTorch.train(\n    model, optimizer, X_train, y_train, num_epochs=20, batch_size=64\n)\n\nEpoch 1/20, Loss: 2.4077\nEpoch 2/20, Loss: 1.9432\nEpoch 3/20, Loss: 1.3106\nEpoch 4/20, Loss: 0.8377\nEpoch 5/20, Loss: 0.5983\nEpoch 6/20, Loss: 0.4519\nEpoch 7/20, Loss: 0.3727\nEpoch 8/20, Loss: 0.3168\nEpoch 9/20, Loss: 0.2759\nEpoch 10/20, Loss: 0.2401\nEpoch 11/20, Loss: 0.2076\nEpoch 12/20, Loss: 0.1782\nEpoch 13/20, Loss: 0.1456\nEpoch 14/20, Loss: 0.1240\nEpoch 15/20, Loss: 0.1107\nEpoch 16/20, Loss: 0.0945\nEpoch 17/20, Loss: 0.0798\nEpoch 18/20, Loss: 0.0707\nEpoch 19/20, Loss: 0.0635\nEpoch 20/20, Loss: 0.0626\n\n\nThe output shows the training loss decreasing over epochs, which indicates that the model is learning to fit the training data. However, the loss on training data alone does not tell us how well the model will perform on new, unseen examples. To evaluate generalization performance, we use the DSTorch.score_image method, which computes predictions and compares them to the true labels.\n\nDSTorch.score_image(model, X_train, y_train, cn)\n\n0.9959999918937683\n\n\n\nDSTorch.score_image(model, X_test, y_test, cn)\n\n0.8360000252723694\n\n\nThe training accuracy tells us how well the model fits the data it was trained on, while the test accuracy measures how well it generalizes to new examples. A large gap between these two numbers would indicate overfitting, where the model has memorized the training examples rather than learning patterns that transfer to new data. With the architecture and hyperparameters used here, we achieve strong performance on both sets, demonstrating that a dense neural network can effectively learn to recognize handwritten digits.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "13_deeplearning.html#conclusion",
    "href": "13_deeplearning.html#conclusion",
    "title": "13  Deep Learning",
    "section": "13.7 Conclusion",
    "text": "13.7 Conclusion\nThis chapter introduced the fundamental concepts and architectures of deep learning. We began with the building blocks of neural networks: layers of linear combinations followed by non-linear activation functions. We saw how stacking these layers creates models capable of learning complex patterns that would be impossible to capture with traditional linear methods.\nThe training process for neural networks relies on stochastic gradient descent, which iteratively updates model parameters to reduce prediction error on batches of training examples. The learning rate and momentum are critical hyperparameters that control the speed and stability of this optimization process. Backpropagation provides an efficient algorithm for computing the gradients needed at each step. In this chapter, we implemented a dense (fully connected) network treats each input feature independently, making it a straightforward extension of the regression models from earlier chapters.\nThe MNIST digit classification task served as our running example throughout the chapter. While this dataset is small and simple by modern standards, it effectively illustrates the key concepts: how to structure a neural network, how to train it with gradient descent, and how to evaluate its performance on held-out test data. The same principles apply to much larger and more complex tasks, though the architectures grow accordingly.\nDeep learning has transformed many areas of artificial intelligence and data science over the past decade. Image recognition, speech processing, natural language understanding, and game playing have all been revolutionized by neural network approaches. As you continue to explore this field, you will encounter many variations and extensions of the basic ideas introduced here: different layer types, regularization techniques to prevent overfitting, normalization methods to stabilize training, and attention mechanisms that allow models to focus on relevant parts of the input. The foundation laid in this chapter provides the conceptual framework for understanding these more advanced techniques.\n\n\n\n\n[1] Barron, A (1993 ). Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory. 39 930–45",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "14_cnnwordvec.html",
    "href": "14_cnnwordvec.html",
    "title": "14  CNNs and Word2Vec",
    "section": "",
    "text": "14.1 Setup\nLoad all of the modules and datasets needed for the chapter. We also load several parts of the torch module for building deep learning models and the Word2Vec model from gensim.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom gensim.models import Word2Vec\n\nmnist = pl.read_csv(\"data/mnist_1000.csv\")\nimdb5k = pl.read_parquet(\"data/imdb5k_pca.parquet\")",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CNNs and Word2Vec</span>"
    ]
  },
  {
    "objectID": "14_cnnwordvec.html#cnns-theory",
    "href": "14_cnnwordvec.html#cnns-theory",
    "title": "14  CNNs and Word2Vec",
    "section": "14.2 CNNs: Theory",
    "text": "14.2 CNNs: Theory\nThe dense neural network we built in Chapter 13 treats each pixel as an independent feature. While this works reasonably well for small images like MNIST digits, it ignores a fundamental property of images: spatial structure. Pixels that are close together tend to be related. An edge in an image is defined by the relationship between neighboring pixels, not by any single pixel in isolation. Convolutional neural networks (CNNs) are designed to exploit this spatial structure by using a different type of layer that processes local regions of the input.\nThe core building block of a CNN is the convolutional layer. Instead of connecting every input to every output as in a dense layer, a convolutional layer applies a small filter (also called a kernel) that slides across the image. The filter is a small grid of weights, typically 3×3 or 5×5 pixels. At each position, the filter computes a weighted sum of the pixels it covers, producing a single output value. Figure Figure 14.1 illustrates this process for a 3×3 kernel applied to a small portion of an image.\n\n\n\n\n\n\nFigure 14.1: A convolutional kernel slides across an image, computing a weighted sum at each position.\n\n\n\nThe power of convolution comes from weight sharing: the same filter weights are used at every position in the image. This dramatically reduces the number of parameters compared to a dense layer. A 3×3 filter has only 9 weights (plus a bias term), regardless of the image size. Moreover, because the same filter is applied everywhere, it can detect the same pattern wherever it appears in the image. A filter that detects vertical edges will find them whether they appear in the top-left corner or the bottom-right.\nBy sliding the filter across the entire image, we produce an output that has the same spatial structure as the input but with different values at each position. Figure Figure 14.2 illustrates how the kernel would get applied to another set of pixels in the image.\n\n\n\n\n\n\nFigure 14.2: Multiple convolutional filters produce multiple output channels.\n\n\n\nSo far we have considered an input image that has only a height and a width and a single kernel. We can expand both the input and the output in a third dimension.\nWhen working with color images, though, we have a third dimension corresponding to the red, green, and blue color channels. When we have a kernel, it will use all channels from the pixels in question. So, a 3×3 kernel for a color image would need \\(3×3×3+1=28\\) parameters. In practice, a convolutional layer also applies multiple filters simultaneously, each producing its own output. These outputs are stacked together to form a multi-channel result. If we apply 16 different filters to an image, we get 16 different feature maps, each highlighting different patterns in the input. A second convolution could be applied afterwards the works the same way, combining all of the filters within a spatial region. Figure Figure 14.3 illustrates how multiple filters produce multiple output channels from a color image.\n\n\n\n\n\n\nFigure 14.3: Convolutional layers with multiple channels.\n\n\n\nA second key component of CNNs is the pooling layer, which reduces the spatial dimensions of the feature maps. The most common type is max pooling, which divides the input into non-overlapping rectangular regions and outputs the maximum value from each region. A 2×2 max pooling operation reduces each dimension by half, so a 28×28 feature map becomes 14×14. Pooling serves two purposes: it reduces the computational burden for subsequent layers, and it provides a degree of translation invariance, meaning that small shifts in the input do not dramatically change the output. Figure Figure 14.4 shows how max pooling works on a small example.\n\n\n\n\n\n\nFigure 14.4: Max pooling takes the maximum value from each region.\n\n\n\nA typical CNN architecture alternates between convolutional layers (followed by ReLU activations) and pooling layers. The convolutional layers detect increasingly complex features: early layers might detect edges and simple textures, while deeper layers combine these into more abstract representations like shapes or object parts. The pooling layers progressively reduce the spatial dimensions. After several such blocks, the feature maps are flattened into a vector and passed through one or more dense layers, which produce the final classification output. This combination of local feature detection through convolution and global reasoning through dense layers has proven remarkably effective across a wide range of image analysis tasks.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CNNs and Word2Vec</span>"
    ]
  },
  {
    "objectID": "14_cnnwordvec.html#cnns-application",
    "href": "14_cnnwordvec.html#cnns-application",
    "title": "14  CNNs and Word2Vec",
    "section": "14.3 CNNs: Application",
    "text": "14.3 CNNs: Application\nTo apply a CNN to our MNIST data, we first need to reload the image data without flattening. Convolutional layers expect inputs with spatial structure: height, width, and channels. For our grayscale digits, each image is 28 pixels tall, 28 pixels wide, and has 1 channel.\n\nX, X_train, X_test, y, y_train, y_test, cn = DSTorch.load_image(\n    mnist, scale=True\n)\nX.shape\n\ntorch.Size([1000, 1, 28, 28])\n\n\nThe shape shows that we have 1,000 images, each with 1 channel and dimensions 28×28. This four-dimensional structure (batch size, channels, height, width) is the standard format for image data in PyTorch.\nWe now define a CNN architecture using the same class-based approach as before. The model consists of two main parts: a feature extraction section with convolutional and pooling layers, and a classification section with dense layers.\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n          \n            nn.Flatten(),\n            nn.Linear(32 * 7 * 7, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nThe feature extraction section begins with nn.Conv2d(1, 16, kernel_size=3, padding=1). This creates a convolutional layer that takes 1 input channel (grayscale) and produces 16 output channels using 3×3 filters. The padding=1 argument adds a border of zeros around the input so that the output has the same spatial dimensions as the input. After the ReLU activation, nn.MaxPool2d(2) applies 2×2 max pooling, which halves both spatial dimensions from 28×28 to 14×14. The second convolutional block follows the same pattern: a 3×3 convolution that increases from 16 to 32 channels, a ReLU activation, and another 2×2 max pooling that reduces dimensions to 7×7. After these two blocks, each image is represented by a 32×7×7 tensor containing 1,568 values.\nThe classifier section first flattens this tensor into a vector of length \\(32 \\times 7 \\times 7 = 1{,}568\\). A dense layer reduces this to 128 hidden units, followed by ReLU and a final layer that produces 10 outputs for our 10 digit classes.\nWe create the model and optimizer, using a smaller learning rate than we did for the dense network. CNNs often benefit from lower learning rates because the shared weights across spatial positions can lead to larger effective gradients.\n\nmodel = SimpleCNN()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\nDSTorch.train(\n    model, optimizer, X_train, y_train, num_epochs=20, batch_size=64\n)\n\nEpoch 1/20, Loss: 2.4154\nEpoch 2/20, Loss: 1.8451\nEpoch 3/20, Loss: 1.1140\nEpoch 4/20, Loss: 0.7491\nEpoch 5/20, Loss: 0.5468\nEpoch 6/20, Loss: 0.4680\nEpoch 7/20, Loss: 0.3937\nEpoch 8/20, Loss: 0.3192\nEpoch 9/20, Loss: 0.2690\nEpoch 10/20, Loss: 0.2131\nEpoch 11/20, Loss: 0.2152\nEpoch 12/20, Loss: 0.1721\nEpoch 13/20, Loss: 0.1518\nEpoch 14/20, Loss: 0.1331\nEpoch 15/20, Loss: 0.1066\nEpoch 16/20, Loss: 0.0951\nEpoch 17/20, Loss: 0.0680\nEpoch 18/20, Loss: 0.0717\nEpoch 19/20, Loss: 0.0650\nEpoch 20/20, Loss: 0.0592\n\n\nAfter training, we evaluate the model on both the training and test sets.\n\nDSTorch.score_image(model, X_train, y_train, cn)\n\n0.9946666955947876\n\n\n\nDSTorch.score_image(model, X_test, y_test, cn)\n\n0.9039999842643738\n\n\nThe CNN achieves strong performance on the digit classification task. To better understand where the model struggles, we can examine the images it misclassifies. The code below filters for incorrect predictions and displays them with their predicted labels.\n\n(\n    mnist\n    .with_columns(\n        DSTorch.predict(model, X, y, cn)\n    )\n    .filter(c.target_ != c.prediction_)\n    .pipe(DSImage.plot_image_grid, label_name=\"prediction_\")\n)\n\n\n\n\n\n\n\n\nLooking at the misclassified examples reveals that many are genuinely ambiguous even to human readers. Some digits are written in unusual styles, have stray marks, or could reasonably be interpreted as multiple different digits. This is a common finding in classification tasks: the examples that remain difficult for the model after training are often the ones that would challenge human annotators as well.\n\n\n\n\n\n\nHyperparameter choices\n\n\n\n\n\nThe architecture and hyperparameters used above are not the result of exhaustive optimization. The number of filters (16 and 32), the filter size (3×3), the number of dense units (128), the learning rate (0.001), and the momentum (0.9) are all choices that could be tuned. In practice, these values often come from a combination of established conventions in the field and experimentation on the specific dataset. For MNIST, which is a relatively simple dataset by modern standards, many different configurations will work well. For more challenging tasks, careful hyperparameter tuning becomes increasingly important and can be the difference between a model that barely works and one that achieves state-of-the-art results.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CNNs and Word2Vec</span>"
    ]
  },
  {
    "objectID": "14_cnnwordvec.html#word-embeddings",
    "href": "14_cnnwordvec.html#word-embeddings",
    "title": "14  CNNs and Word2Vec",
    "section": "14.4 Word Embeddings",
    "text": "14.4 Word Embeddings\nThe neural network architectures we have explored so far take numerical inputs: pixel intensities for images or measured quantities for tabular data. Text data presents a different challenge. Words are discrete symbols with no inherent numerical representation. The word “cat” is not naturally closer to “dog” than it is to “democracy” in any obvious numerical sense. Before we can apply neural networks to text, we need a way to represent words as vectors of numbers.\nThe simplest approach is called one-hot encoding. If our vocabulary contains \\(V\\) distinct words, we represent each word as a vector of length \\(V\\) with a single 1 in the position corresponding to that word and 0s everywhere else. For example, if our vocabulary is {“cat”, “dog”, “house”}, we might represent “cat” as \\([1, 0, 0]\\), “dog” as \\([0, 1, 0]\\), and “house” as \\([0, 0, 1]\\). This approach has two serious problems. First, the vectors are extremely long for realistic vocabularies (tens of thousands of words), making computation expensive. Second, all words are equally distant from all other words. The representation contains no information about semantic relationships.\nWord embeddings solve both problems by learning dense, low-dimensional vectors that capture semantic meaning. Instead of a sparse vector with thousands of dimensions, each word is represented by a dense vector with perhaps 100 to 300 dimensions. More importantly, words with similar meanings end up with similar vectors. The word “cat” will be closer to “dog” than to “democracy” because both are animals that people keep as pets.\nThe key insight behind learning word embeddings is that words appearing in similar contexts tend to have similar meanings. This is known as the distributional hypothesis. If we see the sentences “The cat sat on the mat” and “The dog sat on the mat,” we learn something about the relationship between “cat” and “dog”: they can appear in the same contexts. Word embedding algorithms exploit this insight by training neural networks to predict words from their contexts (or vice versa), and the learned internal representations become the word vectors.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CNNs and Word2Vec</span>"
    ]
  },
  {
    "objectID": "14_cnnwordvec.html#movie-reviews-data",
    "href": "14_cnnwordvec.html#movie-reviews-data",
    "title": "14  CNNs and Word2Vec",
    "section": "14.5 Movie Reviews Data",
    "text": "14.5 Movie Reviews Data\nTo illustrate word embeddings, we will work with a collection of movie reviews. Each review is a short text expressing an opinion about a film. We load the data and examine its structure.\n\nimdb5k\n\n\nshape: (5_000, 5)\n\n\n\nid\nlabel\ntext\nindex\ne5\n\n\nstr\nstr\nstr\nstr\nlist[f64]\n\n\n\n\n\"doc0001\"\n\"negative\"\n\"In my opinion, this movie is n…\n\"test\"\n[-0.253192, 0.099743, … -0.000806]\n\n\n\"doc0002\"\n\"positive\"\n\"Loved today's show!!! It was a…\n\"test\"\n[0.280815, -0.064508, … -0.0273]\n\n\n\"doc0003\"\n\"negative\"\n\"Nothing about this movie is an…\n\"test\"\n[-0.179638, 0.171097, … 0.007573]\n\n\n\"doc0004\"\n\"positive\"\n\"Even though this was a disaste…\n\"train\"\n[0.300824, 0.027666, … -0.028764]\n\n\n\"doc0005\"\n\"positive\"\n\"I cannot believe I enjoyed thi…\n\"test\"\n[0.319154, 0.035425, … 0.001316]\n\n\n…\n…\n…\n…\n…\n\n\n\"doc4996\"\n\"positive\"\n\"\"Americans Next Top Model\" is …\n\"test\"\n[0.305853, 0.026326, … 0.006194]\n\n\n\"doc4997\"\n\"negative\"\n\"It's very sad that Lucian Pint…\n\"train\"\n[0.217778, 0.283147, … -0.028361]\n\n\n\"doc4998\"\n\"positive\"\n\"Ruth Gordon at her best. This …\n\"train\"\n[0.373751, 0.076117, … -0.032517]\n\n\n\"doc4999\"\n\"negative\"\n\"I actually saw the movie befor…\n\"test\"\n[-0.073533, 0.10723, … 0.019535]\n\n\n\"doc5000\"\n\"positive\"\n\"I've Seen The Beginning Of The…\n\"test\"\n[0.147752, -0.095977, … -0.02008]\n\n\n\n\n\n\nEach row contains a review text and a sentiment label indicating whether the review is positive or negative. For learning embeddings, we focus on the text itself. The first step is to convert each review into a list of words, a process called tokenization. We use a simple approach that converts text to lowercase and splits on whitespace and punctuation.\n\nimdb5k = (\n    imdb5k\n    .with_columns(\n        tokens = c.text.str.to_lowercase().str.extract_all(r\"[a-z]+\")\n    )\n)\nimdb5k\n\n\nshape: (5_000, 6)\n\n\n\nid\nlabel\ntext\nindex\ne5\ntokens\n\n\nstr\nstr\nstr\nstr\nlist[f64]\nlist[str]\n\n\n\n\n\"doc0001\"\n\"negative\"\n\"In my opinion, this movie is n…\n\"test\"\n[-0.253192, 0.099743, … -0.000806]\n[\"in\", \"my\", … \"movie\"]\n\n\n\"doc0002\"\n\"positive\"\n\"Loved today's show!!! It was a…\n\"test\"\n[0.280815, -0.064508, … -0.0273]\n[\"loved\", \"today\", … \"disappointed\"]\n\n\n\"doc0003\"\n\"negative\"\n\"Nothing about this movie is an…\n\"test\"\n[-0.179638, 0.171097, … 0.007573]\n[\"nothing\", \"about\", … \"zero\"]\n\n\n\"doc0004\"\n\"positive\"\n\"Even though this was a disaste…\n\"train\"\n[0.300824, 0.027666, … -0.028764]\n[\"even\", \"though\", … \"movie\"]\n\n\n\"doc0005\"\n\"positive\"\n\"I cannot believe I enjoyed thi…\n\"test\"\n[0.319154, 0.035425, … 0.001316]\n[\"i\", \"cannot\", … \"an\"]\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"doc4996\"\n\"positive\"\n\"\"Americans Next Top Model\" is …\n\"test\"\n[0.305853, 0.026326, … 0.006194]\n[\"americans\", \"next\", … \"next\"]\n\n\n\"doc4997\"\n\"negative\"\n\"It's very sad that Lucian Pint…\n\"train\"\n[0.217778, 0.283147, … -0.028361]\n[\"it\", \"s\", … \"of\"]\n\n\n\"doc4998\"\n\"positive\"\n\"Ruth Gordon at her best. This …\n\"train\"\n[0.373751, 0.076117, … -0.032517]\n[\"ruth\", \"gordon\", … \"episode\"]\n\n\n\"doc4999\"\n\"negative\"\n\"I actually saw the movie befor…\n\"test\"\n[-0.073533, 0.10723, … 0.019535]\n[\"i\", \"actually\", … \"money\"]\n\n\n\"doc5000\"\n\"positive\"\n\"I've Seen The Beginning Of The…\n\"test\"\n[0.147752, -0.095977, … -0.02008]\n[\"i\", \"ve\", … \"film\"]\n\n\n\n\n\n\nThe tokenized text is now a list of words for each review.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CNNs and Word2Vec</span>"
    ]
  },
  {
    "objectID": "14_cnnwordvec.html#word2vec",
    "href": "14_cnnwordvec.html#word2vec",
    "title": "14  CNNs and Word2Vec",
    "section": "14.6 Word2Vec",
    "text": "14.6 Word2Vec\nThe most influential word embedding algorithm is Word2Vec, introduced by researchers at Google in 2013. Word2Vec comes in two variants: Skip-gram, which predicts context words given a target word, and CBOW (Continuous Bag of Words), which predicts a target word given its context. Both variants produce high-quality embeddings, but Skip-gram often works better for smaller datasets and rare words.\nThe Word2Vec model is itself a specific kind of neural network. We can represent the model in a digram format as shown in Figure Figure 14.5.\n\n\n\n\n\n\nFigure 14.5: Visualization of the Skip-gram version of Word2Vec\n\n\n\nTraining this model in PyTorch is certainly possible, but a bit complex because the embeddings on the left-side of the equation (the one that takes “jumped” as an input in the exam) need to be the same as the embeddings used to compare to the surrounding words on the right-side of the equation. Rather than implementing this logic ourselves, we use the gensim library to build Word2Vec embeddings using our movie reviews. The model takes several hyperparameters: vector_size controls the dimensionality of the embeddings, window specifies how many words on each side of the target word to consider as context, min_count filters out rare words that appear fewer than this many times, and sg=1 selects the Skip-gram variant.\n\nmodel = Word2Vec(\n    sentences=imdb5k[\"tokens\"].to_list(),\n    vector_size=100,\n    window=2,\n    min_count=5,\n    sg=1,\n    epochs=20\n)\n\nException ignored in: 'gensim.models.word2vec_inner.our_dot_float'\nException ignored in: 'gensim.models.word2vec_inner.our_dot_float'\nException ignored in: 'gensim.models.word2vec_inner.our_dot_float'\nException ignored in: 'gensim.models.word2vec_inner.our_dot_float'\nException ignored in: 'gensim.models.word2vec_inner.our_dot_float'\nException ignored in: 'gensim.models.word2vec_inner.our_dot_float'\nException ignored in: 'gensim.models.word2vec_inner.our_dot_float'\nException ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n\n\nThe trained model contains a vocabulary of words and their corresponding embedding vectors. We can examine the size of the learned vocabulary and retrieve the embedding for any word.\n\nlen(model.wv)\n\n12479\n\n\nEach word is now represented as a vector of 100 numbers. We can convert this into a DataFrame using the following code:\n\nembed = pl.DataFrame({\n    \"word\": list(set(model.wv.key_to_index)),\n    \"embedding\": [model.wv[w].tolist() for w in set(model.wv.key_to_index)],\n})\nembed\n\n\nshape: (12_479, 2)\n\n\n\nword\nembedding\n\n\nstr\nlist[f64]\n\n\n\n\n\"gasp\"\n[0.202926, 0.172699, … -0.03288]\n\n\n\"nicolas\"\n[0.149095, 0.029603, … 0.053545]\n\n\n\"relatively\"\n[0.373473, -0.480511, … 0.456371]\n\n\n\"poignancy\"\n[0.157766, 0.116407, … -0.11928]\n\n\n\"magazines\"\n[0.111295, 0.278877, … -0.162705]\n\n\n…\n…\n\n\n\"worships\"\n[-0.450613, 0.160482, … -0.010699]\n\n\n\"al\"\n[0.130421, 0.170884, … 0.778572]\n\n\n\"severely\"\n[0.269815, 0.136924, … -0.175788]\n\n\n\"damned\"\n[-0.107711, -0.054413, … 0.127976]\n\n\n\"biko\"\n[-0.102705, 0.376713, … 0.036604]\n\n\n\n\n\n\nAs with the output from the dimensionality reduction algorithms in Chapter 12, the individual numbers have no intrinsic meaning. What matters are the relationships between the embeddings for different words.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CNNs and Word2Vec</span>"
    ]
  },
  {
    "objectID": "14_cnnwordvec.html#semantic-relationships",
    "href": "14_cnnwordvec.html#semantic-relationships",
    "title": "14  CNNs and Word2Vec",
    "section": "14.7 Semantic Relationships",
    "text": "14.7 Semantic Relationships\nThe power of word embeddings lies in their ability to capture semantic relationships. Words with similar meanings should have similar vectors, which we can measure using cosine similarity. The gensim model provides a convenient method to find the words most similar to a given word.\n\nmodel.wv.most_similar(\"good\", topn=10)\n\n[('decent', 0.6656251549720764),\n ('bad', 0.6361536979675293),\n ('great', 0.6094589829444885),\n ('cool', 0.5628355145454407),\n ('goofy', 0.5593497157096863),\n ('ok', 0.5590813755989075),\n ('reasonable', 0.5571255683898926),\n ('lousy', 0.554226279258728),\n ('promising', 0.5480849742889404),\n ('stellar', 0.5436846613883972)]\n\n\nThe results show words that appear in similar contexts to “good” in our movie review corpus. Words like “great,” “bad,” and “excellent” appear because they are used in similar ways when expressing opinions about films.\nWe can also explore relationships for other types of individual words. Perhaps the most interesting thing, however, is looking at the relationships between multiple words all at once. The 100-dimensional embedding space is difficult to visualize directly. We can use the dimensionality reduction algorithms from Chapter 12 to reduce their dimenionality to make them possible to plot.\n\nterms = pl.DataFrame({\n    \"word\": [\"good\", \"great\", \"excellent\", \"bad\", \"terrible\", \"awful\",\n             \"movie\", \"film\", \"actor\", \"actress\", \"director\",\n             \"love\", \"hate\", \"boring\", \"exciting\", \"funny\", \"scary\"]\n})\n\n(\n    terms\n    .join(embed, on=\"word\", how=\"inner\")\n    .pipe(\n        DSSklearn.umap,\n        features=[c.embedding],\n        n_neighbors=4\n    )\n    .predict(full=True)\n    .pipe(ggplot, aes(\"dr0\", \"dr1\"))\n    + geom_text(aes(label=\"word\"))\n    + theme_minimal()\n)\n\n\n\n\n\n\n\n\nThe visualization shows that semantically related words cluster together in the embedding space. Positive sentiment words like “good,” “great,” and “excellent” appear near each other, as do negative words like “bad,” “terrible,” and “awful.” Words related to film roles like “actor,” “actress,” and “director” form another cluster.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CNNs and Word2Vec</span>"
    ]
  },
  {
    "objectID": "14_cnnwordvec.html#cnns-for-text",
    "href": "14_cnnwordvec.html#cnns-for-text",
    "title": "14  CNNs and Word2Vec",
    "section": "14.8 CNNs for Text",
    "text": "14.8 CNNs for Text\nThe word embeddings we learned in the previous section give us a way to represent individual words as dense vectors that capture semantic meaning. Now we turn to the question of how to use these representations for text classification. Given a movie review, can we predict whether it expresses a positive or negative sentiment?\nOne approach would be to use the averaged document vectors we computed at the end of the previous section as features for a traditional classifier. This works reasonably well but throws away information about word order. The sentence “not good” has a very different meaning from “good, not” but averaging the word vectors for both would produce similar results since they contain the same words. To capture patterns that depend on word order and local context, we turn to convolutional neural networks.\nRecall that in image classification, a 2D convolutional layer slides a small filter across the image, computing a weighted sum at each position. The filter detects local patterns like edges or textures regardless of where they appear in the image. The same idea applies to text, but in one dimension rather than two.\nWhen we represent a document as a sequence of word embeddings, we get a two-dimensional array: one dimension for position in the sequence (which word) and one for the embedding features (the 100 values representing each word). A 1D convolutional filter slides along the sequence dimension, looking at several consecutive words at a time. A filter of width 3 examines three adjacent word vectors and computes a weighted combination of all their features.\n\n\n\n\n\n\nFigure 14.6: A 1D convolutional layer applied to text embeddings.\n\n\n\nConsider what such a filter might learn. A filter could activate strongly when it sees the pattern “not good” or “not bad” — sequences where a negation word precedes a sentiment word. Another filter might detect emphatic phrases like “really great” or “absolutely terrible.” Each filter learns to recognize a different local pattern, and the collection of filters together captures many aspects of how sentiment is expressed in text.\nAfter the convolutional layer processes the sequence, we have a new sequence of values — one output per filter for each position where the filter was applied. To produce a fixed-length representation regardless of document length, we apply global max pooling: for each filter, we take the maximum value it produced anywhere in the document. This captures whether a particular pattern appeared at all, regardless of where. The resulting vector has one value per filter and can be passed through dense layers to produce the final classification.\nTo build a text CNN in PyTorch, we need to prepare our data in a specific format. Each document must be converted to a sequence of integer indices that refer to positions in our vocabulary. We also need to handle the fact that documents have different lengths by padding shorter documents and truncating longer ones to a fixed maximum length. representing negative.\n\nX, X_train, X_test, y, y_train, y_test, cn = DSTorch.load_text(\n    df=imdb5k,\n    model=model,\n    tokens_expr=c.tokens,\n    label_expr=c.label\n)\n\nA key component of our text CNN is the embedding layer, which converts word indices into embedding vectors. PyTorch provides nn.Embedding for this purpose. We can initialize this layer with the pre-trained Word2Vec embeddings we learned earlier, giving the model a head start with meaningful word representations.\n\nembedding_dim = model.vector_size\nembedding_matrix = torch.tensor(model.wv.vectors, dtype=torch.float32)\n\nWe now define the complete CNN architecture for text classification. The model consists of an embedding layer initialized with our Word2Vec vectors, a 1D convolutional layer that detects local patterns, global max pooling to aggregate across the sequence, and dense layers to produce the final classification.\n\nclass TextCNN(nn.Module):\n    def __init__(self, embedding_matrix, num_filters=100, filter_size=3, num_classes=2):\n        super().__init__()\n        vocab_size, embedding_dim = embedding_matrix.shape\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.embedding.weight = nn.Parameter(embedding_matrix)\n        self.embedding.weight.requires_grad = False\n        \n        self.conv = nn.Conv1d(\n            in_channels=embedding_dim,\n            out_channels=num_filters,\n            kernel_size=filter_size,\n            padding=filter_size // 2\n        )\n        self.relu = nn.ReLU()\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(num_filters, 64),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(64, num_classes)\n        )\n    \n    def forward(self, x):\n        embedded = self.embedding(x)\n        embedded = embedded.permute(0, 2, 1)\n        conv_out = self.relu(self.conv(embedded))\n        pooled = conv_out.max(dim=2)[0]\n        \n        output = self.classifier(pooled)\n        return output\n\nThe architecture deserves careful examination. The embedding layer converts each word index into its 100-dimensional vector. The permute operation rearranges the dimensions so that the embedding features become the “channels” that the 1D convolution operates over, and the sequence positions become the spatial dimension the filter slides along.\nThe convolutional layer nn.Conv1d applies num_filters different filters, each of width filter_size. With filter_size=3, each filter looks at three consecutive words. The padding ensures the output sequence has the same length as the input. After the ReLU activation, we take the maximum value across all positions for each filter using max(dim=2). This global max pooling produces a fixed-length vector regardless of input length.\nThe classifier head consists of two dense layers with dropout for regularization. Dropout randomly sets a fraction of the values to zero during training, which helps prevent overfitting by forcing the network to learn redundant representations.\n\n\n\n\n\n\nFreezing embeddings\n\n\n\n\n\nIn the model above, we set requires_grad = False for the embedding weights. This freezes the pre-trained Word2Vec vectors, preventing them from being modified during training. This is appropriate when we have limited training data and want to preserve the semantic relationships learned from the larger corpus. With more training data, we could allow the embeddings to be fine-tuned by setting requires_grad = True, which might improve performance by adapting the representations to the specific task.\n\n\n\nWe create an instance of the model and set up the optimizer and loss function. For binary classification, we use cross-entropy loss, which is the standard choice for classification problems.\n\nmodel_cnn = TextCNN(embedding_matrix, num_filters=100, filter_size=3)\noptimizer = optim.Adam(model_cnn.parameters(), lr=0.001)\n\nThe training loop iterates over the training data in batches, computing predictions, calculating the loss, and updating the model parameters through backpropagation.\n\nDSTorch.train(model_cnn, optimizer, X_train, y_train)\n\nEpoch 1/18, Loss: 0.6695\nEpoch 2/18, Loss: 0.4729\nEpoch 3/18, Loss: 0.3519\nEpoch 4/18, Loss: 0.2573\nEpoch 5/18, Loss: 0.1715\nEpoch 6/18, Loss: 0.1084\nEpoch 7/18, Loss: 0.0532\nEpoch 8/18, Loss: 0.0265\nEpoch 9/18, Loss: 0.0141\nEpoch 10/18, Loss: 0.0122\nEpoch 11/18, Loss: 0.0079\nEpoch 12/18, Loss: 0.0060\nEpoch 13/18, Loss: 0.0054\nEpoch 14/18, Loss: 0.0033\nEpoch 15/18, Loss: 0.0024\nEpoch 16/18, Loss: 0.0022\nEpoch 17/18, Loss: 0.0016\nEpoch 18/18, Loss: 0.0018\n\n\nAfter training, we evaluate the model on both the training and test sets to assess how well it has learned and how well it generalizes to new examples.\n\nDSTorch.score_text(model_cnn, X_train, y_train)\n\n1.0\n\n\n\nDSTorch.score_text(model_cnn, X_test, y_test)\n\n0.8600000143051147\n\n\nThe model learns to classify sentiment from the movie reviews by detecting local patterns in the word sequences. The convolutional filters learn to recognize phrases and word combinations that are indicative of positive or negative sentiment.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CNNs and Word2Vec</span>"
    ]
  },
  {
    "objectID": "14_cnnwordvec.html#pre-trained-embeddings",
    "href": "14_cnnwordvec.html#pre-trained-embeddings",
    "title": "14  CNNs and Word2Vec",
    "section": "14.9 Pre-trained embeddings",
    "text": "14.9 Pre-trained embeddings\nTraining word embeddings on a small corpus like our movie reviews has limitations. Rare words may not have good representations, and the learned relationships reflect only the patterns in this specific dataset. In practice, it is common to use pre-trained embeddings learned from massive text corpora containing billions of words. Popular options include Word2Vec trained on Google News, GloVe trained on Wikipedia and web text, and fastText trained on Common Crawl. These pre-trained embeddings capture general semantic relationships and can be fine-tuned or used directly for downstream tasks. The gensim library provides tools for loading many pre-trained embedding models.\nThe gensim library includes a downloader that provides access to several pre-trained models. We can list the available models and load them directly.\n\nimport gensim.downloader as api\n\nprint(list(api.info()['models'].keys()))\n\n['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n\n\nThe list includes models of varying sizes and training corpora. For our purposes, we will use glove-wiki-gigaword-100, which provides 100-dimensional GloVe embeddings trained on Wikipedia and Gigaword news text. This model strikes a balance between quality and computational efficiency.\n\npretrained = api.load(\"glove-wiki-gigaword-100\")\n\nThe loaded model works similarly to our Word2Vec model. We can query it for word vectors and find similar words. Let’s compare the most similar words to “good” in the pre-trained model versus our corpus-specific model.\n\npretrained.most_similar(\"good\", topn=10)\n\n[('better', 0.893191397190094),\n ('sure', 0.8314563035964966),\n ('really', 0.8297762274742126),\n ('kind', 0.8288268446922302),\n ('very', 0.8260800242424011),\n ('we', 0.8234356045722961),\n ('way', 0.8215397596359253),\n ('think', 0.8205099105834961),\n ('thing', 0.8171301484107971),\n (\"'re\", 0.8141681551933289)]\n\n\nThe results differ from our corpus-trained model because the pre-trained embeddings reflect patterns from general English text rather than movie reviews specifically. Notice that words related to quality and evaluation appear prominently. The pre-trained model has a much larger vocabulary, covering words that might appear rarely or not at all in our movie reviews.\n\nlen(pretrained)\n\n400000\n\n\nTo use pre-trained embeddings with our text CNN, we need to create a new embedding matrix that maps our corpus vocabulary to the pre-trained vectors. Words that exist in the pre-trained model get their learned vectors; words that do not exist get random vectors initialized in the same range.\n\nvocab = list(model.wv.key_to_index.keys())\nembedding_dim = pretrained.vector_size\n\npretrained_matrix = np.zeros((len(vocab), embedding_dim))\nfound_count = 0\n\nfor i, word in enumerate(vocab):\n    if word in pretrained:\n        pretrained_matrix[i] = pretrained[word]\n        found_count += 1\n    else:\n        pretrained_matrix[i] = np.random.uniform(-0.25, 0.25, embedding_dim)\n\nprint(f\"Found {found_count} of {len(vocab)} words in pre-trained embeddings\")\npretrained_matrix = torch.tensor(pretrained_matrix, dtype=torch.float32)\n\nFound 12430 of 12479 words in pre-trained embeddings\n\n\nMost words in our vocabulary have corresponding pre-trained vectors because they are common English words. The few missing words are typically misspellings, informal variants, or domain-specific terms.\nWe can now create a new text CNN using the pre-trained embedding matrix instead of our corpus-trained embeddings. The architecture remains identical; only the initial word vectors differ.\n\nmodel_pretrained = TextCNN(pretrained_matrix, num_filters=100, filter_size=3)\noptimizer_pretrained = optim.Adam(model_pretrained.parameters(), lr=0.001)\n\n\nDSTorch.train(model_pretrained, optimizer_pretrained, X_train, y_train)\n\nEpoch 1/18, Loss: 0.6772\nEpoch 2/18, Loss: 0.5099\nEpoch 3/18, Loss: 0.3867\nEpoch 4/18, Loss: 0.3089\nEpoch 5/18, Loss: 0.2293\nEpoch 6/18, Loss: 0.1356\nEpoch 7/18, Loss: 0.0735\nEpoch 8/18, Loss: 0.0417\nEpoch 9/18, Loss: 0.0273\nEpoch 10/18, Loss: 0.0192\nEpoch 11/18, Loss: 0.0100\nEpoch 12/18, Loss: 0.0084\nEpoch 13/18, Loss: 0.0071\nEpoch 14/18, Loss: 0.0043\nEpoch 15/18, Loss: 0.0034\nEpoch 16/18, Loss: 0.0025\nEpoch 17/18, Loss: 0.0020\nEpoch 18/18, Loss: 0.0022\n\n\n\nDSTorch.score_text(model_pretrained, X_train, y_train)\nDSTorch.score_text(model_pretrained, X_test, y_test)\n\n0.8180000185966492\n\n\nThe model with pre-trained embeddings often achieves comparable or better performance, particularly when the training corpus is small. The pre-trained embeddings provide a better starting point because they encode semantic relationships learned from billions of words of text.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CNNs and Word2Vec</span>"
    ]
  },
  {
    "objectID": "14_cnnwordvec.html#conclusion",
    "href": "14_cnnwordvec.html#conclusion",
    "title": "14  CNNs and Word2Vec",
    "section": "14.10 Conclusion",
    "text": "14.10 Conclusion\nThis chapter introduced two powerful ideas that extend deep learning beyond the dense feedforward networks of Chapter 13. Convolutional neural networks exploit spatial structure in data by using local filters that detect patterns regardless of their position. For images, 2D convolutions slide across height and width to find edges, textures, and shapes. For text, 1D convolutions slide across sequences of words to find phrases and patterns that indicate meaning.\nWord embeddings transform the discrete symbols of language into continuous vectors that capture semantic relationships. Words that appear in similar contexts end up with similar vectors, allowing neural networks to generalize across related words. Training embeddings on task-specific corpora produces representations tailored to the domain, while pre-trained embeddings from large general corpora provide a strong starting point that transfers well to many tasks.\nThe combination of word embeddings and convolutional networks creates a complete pipeline for text classification. The embedding layer converts words to vectors, convolutions detect local patterns like negation or emphasis, and pooling aggregates these detections into a fixed-length representation for classification. This approach captures information about word order that simpler bag-of-words methods discard.\nThe techniques in this chapter represent important steps in the development of modern deep learning, but they are not the final word. Recurrent neural networks process sequences one element at a time while maintaining memory of what came before, making them particularly suited to variable-length text. Transformer architectures use attention mechanisms to relate all positions in a sequence simultaneously, achieving remarkable performance on language tasks. Pre-trained language models like BERT and GPT extend the idea of pre-trained embeddings to entire contextual representations, where the same word gets different vectors depending on its surrounding context. These more recent approaches have largely superseded CNNs for text classification in research settings, though CNNs remain useful for their simplicity and efficiency. The foundations laid in this chapter, particularly the ideas of local pattern detection and learned representations, remain central to understanding these more advanced methods.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>CNNs and Word2Vec</span>"
    ]
  },
  {
    "objectID": "15_transferlearn.html",
    "href": "15_transferlearn.html",
    "title": "15  Transfer Learning",
    "section": "",
    "text": "15.1 Setup\nLoad all of the modules and datasets needed for the chapter. We load the openai module to make requests to the OpenAI API in order to work with state-of-the-art transformer models.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nfrom openai import OpenAI\n\nbirds = pl.read_parquet(\"data/birds10.parquet\")",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Transfer Learning</span>"
    ]
  },
  {
    "objectID": "15_transferlearn.html#transfer-learning",
    "href": "15_transferlearn.html#transfer-learning",
    "title": "15  Transfer Learning",
    "section": "15.2 Transfer Learning",
    "text": "15.2 Transfer Learning\nIn the previous chapter, we saw how to train neural networks from scratch to recognize patterns in image data. While this approach works well for standardized datasets like MNIST, it has significant limitations when applied to real-world problems. Training a deep neural network requires enormous amounts of labeled data, substantial computational resources, and considerable expertise in tuning hyperparameters. For many practical applications, we simply do not have access to millions of labeled training examples, nor do we have weeks of GPU time to dedicate to training.\nTransfer learning offers an elegant solution to this problem. The core insight is that the internal representations learned by a neural network trained on one task often capture general features that are useful for many other tasks. Consider a model trained to classify millions of images into thousands of categories. The early layers of such a model learn to detect basic visual features like edges, textures, and shapes. The middle layers combine these into more complex patterns like eyes, wheels, or leaves. Only the final layers specialize to the specific categories in the original training data. If we want to build a classifier for a new set of categories, we can reuse all but the final layers, benefiting from the rich visual representations the model has already learned.\nThis approach has become the dominant paradigm in modern machine learning. Rather than training models from scratch, practitioners typically start with a pre-trained model and adapt it to their specific task. This dramatically reduces the amount of data and computation required, often by orders of magnitude. A model that would require millions of training examples when built from scratch might achieve excellent performance with just hundreds or thousands of examples when using transfer learning. The technique is so powerful that it accounts for the vast majority of deep learning applications in production today.\nIn this chapter, we will explore several forms of transfer learning. We begin with the traditional approach of extracting embeddings from a pre-trained image model and using them as features for a downstream classifier. We then examine zero-shot learning, a more recent development that allows classification without any task-specific training. Finally, we apply these same ideas to text data and briefly discuss how to integrate external AI services through APIs.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Transfer Learning</span>"
    ]
  },
  {
    "objectID": "15_transferlearn.html#image-embedding",
    "href": "15_transferlearn.html#image-embedding",
    "title": "15  Transfer Learning",
    "section": "15.3 Image Embedding",
    "text": "15.3 Image Embedding\nFor this chapter we will use a new dataset of images of different bird species. The predictive model that we want to build should learn how to take an image of a bird and identify which of the ten species in our data the bird comes from. This is a more challenging task than MNIST digit recognition for several reasons: the images are larger and more complex, the subjects appear at different scales and orientations, and the differences between species can be subtle.\n\nbirds\n\n\nshape: (1_555, 5)\n\n\n\nlabel\nfilepath\nindex\nvit\nsiglip\n\n\nstr\nstr\nstr\nlist[f64]\nlist[f64]\n\n\n\n\n\"canary\"\n\"media/birds10/00000.png\"\n\"test\"\n[0.017947, -0.0305, … 0.003526]\n[-0.00611, -0.042975, … -0.031687]\n\n\n\"canary\"\n\"media/birds10/00001.png\"\n\"train\"\n[0.024804, -0.045255, … -0.007233]\n[-0.033767, -0.011978, … -0.020352]\n\n\n\"canary\"\n\"media/birds10/00002.png\"\n\"train\"\n[0.050587, -0.024486, … 0.029895]\n[-0.033664, -0.008117, … -0.01725]\n\n\n\"canary\"\n\"media/birds10/00003.png\"\n\"train\"\n[0.047036, -0.038993, … -0.008446]\n[-0.010029, -0.018192, … -0.009869]\n\n\n\"canary\"\n\"media/birds10/00004.png\"\n\"train\"\n[0.036349, -0.02734, … -0.018185]\n[-0.027327, 0.003568, … -0.033407]\n\n\n…\n…\n…\n…\n…\n\n\n\"swallow\"\n\"media/birds10/01550.png\"\n\"train\"\n[-0.022461, -0.025098, … -0.061945]\n[-0.022029, -0.008476, … -0.003879]\n\n\n\"swallow\"\n\"media/birds10/01551.png\"\n\"train\"\n[-0.000212, -0.003448, … -0.058042]\n[-0.02476, -0.016369, … 0.003875]\n\n\n\"swallow\"\n\"media/birds10/01552.png\"\n\"train\"\n[-0.012531, -0.006788, … -0.047077]\n[-0.022153, 0.00765, … -0.011152]\n\n\n\"swallow\"\n\"media/birds10/01553.png\"\n\"test\"\n[-0.007587, -0.053535, … -0.046395]\n[-0.005022, 0.00878, … -0.017564]\n\n\n\"swallow\"\n\"media/birds10/01554.png\"\n\"train\"\n[-0.01325, -0.032453, … -0.050751]\n[-0.015117, -0.00685, … -0.029756]\n\n\n\n\n\n\nBefore we get started with the model, let’s see one example of each of the classes from this dataset. Examining the data visually helps us understand what the model needs to learn and gives us intuition about which categories might be easy or difficult to distinguish.\n\nDSImage.plot_image_grid(birds.group_by('label').first(), ncol=5)\n\n\n\n\n\n\n\n\nIf we were to approach this problem using the techniques from Chapter 13, we would need to design a neural network architecture, choose appropriate hyperparameters, and train the model on our relatively small dataset. This would likely require extensive experimentation and might still produce mediocre results due to the limited training data available. Instead, we will use transfer learning to leverage the knowledge encoded in a model that has already been trained on millions of images.\nHere, we will use the internal representations of a powerful, pre-built deep learning model as a jumping-off point for our own model. The final predictions from the existing model would not be very helpful because they predict categories that are not the same as ours. But, the internal representations of images are sufficiently generic and meaningful to be a good starting point for our models. This approach is called transfer learning. It accounts, in one form or another, for the vast majority of use-cases of deep learning because the best models require very large amounts of training data and compute time.\nPython is the de facto language for machine learning, deep learning, and AI research. It is one of the key reasons we are using it in this revised text. As a result, there are fantastic ready-to-use libraries to build, run, and load deep learning models. We have wrapped up one of these models in the function called ViTEmbedder (the code to create it uses pytorch and is given in full in the notebooks). It implements Google’s Vision Transformer (ViT) model, which was trained on a dataset of 14 million images with a total of 21,843 classes. We will use the values in the internal representation of the final model that come right before the predictions of the classes in the original. Here is all we need to load (and, if not already grabbed, download) the model:\n\nvit = ViTEmbedder()\n\nThe Vision Transformer model works by dividing an image into a grid of patches, treating each patch as a token similar to how language models treat words. These patches are then processed through multiple transformer layers that allow the model to learn relationships between different parts of the image. The result is a rich representation that captures both local details and global structure.\nCalling the model on an image path returns a sequence of 768 numbers. These numbers, called an embedding, form a compact representation of the image’s content. Two images that are visually similar will have embeddings that are close together in this 768-dimensional space, while dissimilar images will have embeddings that are far apart. We can see an example on the first few bird images:\n\n(\n    birds\n    .head(5)\n    .with_columns(\n        vit = c.filepath.map_elements(\n            vit, return_dtype=pl.List(pl.Float32)\n        )\n    )\n)\n\n\nshape: (5, 5)\n\n\n\nlabel\nfilepath\nindex\nvit\nsiglip\n\n\nstr\nstr\nstr\nlist[f32]\nlist[f64]\n\n\n\n\n\"canary\"\n\"media/birds10/00000.png\"\n\"test\"\n[0.017947, -0.0305, … 0.003526]\n[-0.00611, -0.042975, … -0.031687]\n\n\n\"canary\"\n\"media/birds10/00001.png\"\n\"train\"\n[0.024804, -0.045255, … -0.007233]\n[-0.033767, -0.011978, … -0.020352]\n\n\n\"canary\"\n\"media/birds10/00002.png\"\n\"train\"\n[0.050587, -0.024486, … 0.029895]\n[-0.033664, -0.008117, … -0.01725]\n\n\n\"canary\"\n\"media/birds10/00003.png\"\n\"train\"\n[0.047036, -0.038993, … -0.008446]\n[-0.010029, -0.018192, … -0.009869]\n\n\n\"canary\"\n\"media/birds10/00004.png\"\n\"train\"\n[0.036349, -0.02734, … -0.018185]\n[-0.027327, 0.003568, … -0.033407]\n\n\n\n\n\n\nComputing embeddings for all images in the dataset takes some time, so we have precomputed these values and stored them in a Parquet file. Parquet is a columnar storage format that is particularly well-suited for analytical workloads. Unlike CSV files, Parquet preserves data types exactly, handles nested structures like lists efficiently, and compresses data effectively. This makes it an excellent choice for storing embeddings, which are arrays of floating-point numbers that would be awkward to represent in CSV format. Reading the precomputed embeddings is straightforward:\n\nbirds = pl.read_parquet(\"data/birds10.parquet\")\nbirds\n\n\nshape: (1_555, 5)\n\n\n\nlabel\nfilepath\nindex\nvit\nsiglip\n\n\nstr\nstr\nstr\nlist[f64]\nlist[f64]\n\n\n\n\n\"canary\"\n\"media/birds10/00000.png\"\n\"test\"\n[0.017947, -0.0305, … 0.003526]\n[-0.00611, -0.042975, … -0.031687]\n\n\n\"canary\"\n\"media/birds10/00001.png\"\n\"train\"\n[0.024804, -0.045255, … -0.007233]\n[-0.033767, -0.011978, … -0.020352]\n\n\n\"canary\"\n\"media/birds10/00002.png\"\n\"train\"\n[0.050587, -0.024486, … 0.029895]\n[-0.033664, -0.008117, … -0.01725]\n\n\n\"canary\"\n\"media/birds10/00003.png\"\n\"train\"\n[0.047036, -0.038993, … -0.008446]\n[-0.010029, -0.018192, … -0.009869]\n\n\n\"canary\"\n\"media/birds10/00004.png\"\n\"train\"\n[0.036349, -0.02734, … -0.018185]\n[-0.027327, 0.003568, … -0.033407]\n\n\n…\n…\n…\n…\n…\n\n\n\"swallow\"\n\"media/birds10/01550.png\"\n\"train\"\n[-0.022461, -0.025098, … -0.061945]\n[-0.022029, -0.008476, … -0.003879]\n\n\n\"swallow\"\n\"media/birds10/01551.png\"\n\"train\"\n[-0.000212, -0.003448, … -0.058042]\n[-0.02476, -0.016369, … 0.003875]\n\n\n\"swallow\"\n\"media/birds10/01552.png\"\n\"train\"\n[-0.012531, -0.006788, … -0.047077]\n[-0.022153, 0.00765, … -0.011152]\n\n\n\"swallow\"\n\"media/birds10/01553.png\"\n\"test\"\n[-0.007587, -0.053535, … -0.046395]\n[-0.005022, 0.00878, … -0.017564]\n\n\n\"swallow\"\n\"media/birds10/01554.png\"\n\"train\"\n[-0.01325, -0.032453, … -0.050751]\n[-0.015117, -0.00685, … -0.029756]\n\n\n\n\n\n\nNotice that the dataset now includes a vit column containing the 768-dimensional embedding for each image. With these embeddings in hand, we can now train a classifier using the techniques from earlier chapters. The key insight is that we are not training a deep neural network from scratch. Instead, we are using logistic regression on the embedding features, which is computationally cheap and requires far less data than end-to-end deep learning.\n\nmodel = (\n    birds\n    .pipe(\n        DSSklearn.logistic_regression_cv,\n        target=c.label,\n        features=[c.vit],\n        stratify=c.label,\n        l1_ratios=[0],\n        solver=\"saga\"\n    )\n)\n\nThe results are impressive given how little effort we put into the modeling process. We simply extracted pre-trained embeddings and applied logistic regression with cross-validation to select the regularization strength.\n\nmodel.score()\n\n{'train': 1.0, 'test': 1.0}\n\n\nThe confusion matrix shows us which species are most often confused with each other. This can provide insight into the structure of the problem and potentially guide data collection if we wanted to improve performance further. In this case we see that the model does not make a single error on our dataset!\n\nmodel.confusion_matrix()\n\n\n\n\n\n\n\n\nTo better understand what the model has learned, we can examine the images that the model is most confident about. These are the examples where the predicted probability for the correct class is highest. Looking at these high-confidence predictions helps verify that the model is attending to meaningful features of the birds rather than artifacts of the images.\n\n(\n    birds\n    .with_columns(\n        model.predict_proba()\n    )\n    .filter(c.index == \"test\")\n    .sort(c.prob_pred_, descending=True)\n    .group_by(c.label)\n    .agg(\n        filepath = c.filepath.first()\n    )\n    .pipe(DSImage.plot_image_grid, ncol=5)\n)\n\n\n\n\n\n\n\n\nBeyond classification, embeddings can also be visualized to understand the structure of our data. Dimensionality reduction techniques like UMAP (Uniform Manifold Approximation and Projection) can project the 768-dimensional embeddings down to two dimensions while preserving the relative distances between points. This allows us to see how the different bird species cluster in the embedding space.\n\n(\n    birds\n    .pipe(\n        DSSklearn.umap,\n        features=[c.vit]\n    )\n    .predict(full=True)\n    .pipe(ggplot, aes(\"dr0\", \"dr1\"))\n    + geom_point(aes(color=\"label\"))\n)\n\n\n\n\n\n\n\n\nNotice that the UMAP projection does an incredibly good job of separating the categories. Images of the same species cluster tightly together, while different species form distinct groups. This visualization confirms that the ViT embeddings capture meaningful information about bird species, information that was never explicitly part of the original model’s training objective but emerged naturally from learning to classify images more generally.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Transfer Learning</span>"
    ]
  },
  {
    "objectID": "15_transferlearn.html#zero-shot-learning",
    "href": "15_transferlearn.html#zero-shot-learning",
    "title": "15  Transfer Learning",
    "section": "15.4 Zero-shot Learning",
    "text": "15.4 Zero-shot Learning\nLet’s see another way that we can extend transfer learning using an even newer class of deep learning models called zero-shot models. These work, at least conceptually, by allowing us to do classification on a new dataset with data that the original model may never have seen with no additional training data needed. The idea sounds almost magical: we can classify images into categories without ever showing the model any labeled examples of those categories.\nZero-shot learning became practical with the development of multimodal models that learn to connect images and text in a shared embedding space. These models are trained on massive datasets of image-caption pairs scraped from the internet. By learning to match images with their textual descriptions, the models develop an understanding of concepts that can be expressed in natural language. To classify a new image, we simply compare its embedding to the embeddings of text descriptions of each possible category.\nWe will use a model called SigLIP (Sigmoid Language-Image Pre-training), which represents the current state of the art in this type of approach. Like ViT, we have wrapped the model in a convenient interface that handles loading and embedding.\n\nsiglip = SigLIPEmbedder()\n\nThe SigLIP model can embed both images and text into the same space. Images that match a textual description will have embeddings that are close together, measured by the dot product of their embedding vectors. Let’s start with a simple example: we will create a text embedding for “a photo of a canary” and see which images in our dataset are most similar to this description.\n\nembed = (\n    pl.DataFrame({\"text\": [\"a photo of a canary\"]})\n    .with_columns(\n        siglip_txt = c.text.map_elements(\n            siglip.embed_text,\n            return_dtype=pl.List(pl.Float32)\n        )\n    )\n)\n\nNow we can compute the similarity between this text embedding and every image in our dataset. The dot product of two normalized vectors measures how aligned they are: values close to 1 indicate high similarity, while values close to 0 indicate the vectors are nearly orthogonal (unrelated).\n\n(\n    birds\n    .join(embed, how=\"cross\")\n    .with_columns(\n        sim_score = dot_product(c.siglip, c.siglip_txt)   \n    )\n    .sort(c.sim_score, descending=True)\n)\n\n\nshape: (1_555, 8)\n\n\n\nlabel\nfilepath\nindex\nvit\nsiglip\ntext\nsiglip_txt\nsim_score\n\n\nstr\nstr\nstr\nlist[f64]\nlist[f64]\nstr\nlist[f32]\nf64\n\n\n\n\n\"canary\"\n\"media/birds10/00092.png\"\n\"train\"\n[0.02978, -0.047476, … 0.033946]\n[-0.031486, -0.00417, … -0.034138]\n\"a photo of a canary\"\n[-0.013163, 0.00856, … -0.006386]\n0.174141\n\n\n\"canary\"\n\"media/birds10/00025.png\"\n\"test\"\n[0.034456, -0.036724, … 0.01517]\n[-0.012494, -0.006824, … -0.011134]\n\"a photo of a canary\"\n[-0.013163, 0.00856, … -0.006386]\n0.168272\n\n\n\"canary\"\n\"media/birds10/00119.png\"\n\"train\"\n[0.040854, -0.038557, … 0.013647]\n[-0.023419, -0.008008, … -0.032821]\n\"a photo of a canary\"\n[-0.013163, 0.00856, … -0.006386]\n0.166631\n\n\n\"canary\"\n\"media/birds10/00115.png\"\n\"test\"\n[0.020037, -0.035357, … 0.006308]\n[-0.018599, -0.002016, … -0.018126]\n\"a photo of a canary\"\n[-0.013163, 0.00856, … -0.006386]\n0.164925\n\n\n\"canary\"\n\"media/birds10/00122.png\"\n\"train\"\n[0.024512, -0.034228, … 0.011564]\n[-0.031184, -0.00085, … -0.041639]\n\"a photo of a canary\"\n[-0.013163, 0.00856, … -0.006386]\n0.16463\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"duck\"\n\"media/birds10/00393.png\"\n\"test\"\n[-0.016794, -0.018484, … 0.031061]\n[-0.03385, -0.008774, … -0.020661]\n\"a photo of a canary\"\n[-0.013163, 0.00856, … -0.006386]\n-0.059658\n\n\n\"duck\"\n\"media/birds10/00404.png\"\n\"test\"\n[-0.004223, 0.002329, … 0.016521]\n[-0.027971, 0.003032, … -0.035659]\n\"a photo of a canary\"\n[-0.013163, 0.00856, … -0.006386]\n-0.060996\n\n\n\"condor\"\n\"media/birds10/00272.png\"\n\"train\"\n[-0.046735, -0.034367, … -0.003293]\n[0.008176, 0.001546, … 0.007645]\n\"a photo of a canary\"\n[-0.013163, 0.00856, … -0.006386]\n-0.062812\n\n\n\"duck\"\n\"media/birds10/00423.png\"\n\"train\"\n[0.004472, -0.02733, … 0.018769]\n[-0.042206, 0.008924, … -0.033317]\n\"a photo of a canary\"\n[-0.013163, 0.00856, … -0.006386]\n-0.065547\n\n\n\"condor\"\n\"media/birds10/00308.png\"\n\"test\"\n[-0.057108, 0.000503, … -0.003844]\n[0.003084, 0.027618, … 0.002365]\n\"a photo of a canary\"\n[-0.013163, 0.00856, … -0.006386]\n-0.075263\n\n\n\n\n\n\nVisualizing the similarity scores by bird species shows that canary images indeed have the highest similarity to our “a photo of a canary” text prompt. This demonstrates that the model has learned a meaningful connection between the visual appearance of canaries and the word “canary” without us ever explicitly teaching it this connection.\n\n(\n    birds\n    .join(embed, how=\"cross\")\n    .with_columns(\n        sim_score = dot_product(c.siglip, c.siglip_txt)   \n    )\n    .pipe(ggplot, aes(\"sim_score\", \"label\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nTo perform zero-shot classification on the entire dataset, we need to create text embeddings for all of our bird species categories. We can then assign each image to the category whose text embedding it is most similar to. This process requires no training data at all; we are simply leveraging the knowledge the model acquired during its original pre-training.\n\nembed = (\n    birds\n    .select(text = c.label.unique())\n    .with_columns(\n        siglip_txt = c.text.map_elements(\n            siglip.embed_text, return_dtype=pl.List(pl.Float32)\n        )\n    )    \n)\n\nFor each image, we compute its similarity to every category label, then select the label with the highest similarity as the predicted class. The classification rate tells us how often this zero-shot approach correctly identifies the bird species.\n\n(\n    birds\n    .join(embed, how=\"cross\")\n    .with_columns(\n        sim_score = dot_product(c.siglip, c.siglip_txt)   \n    )\n    .sort(c.sim_score, descending=True)\n    .group_by(c.filepath)\n    .head(1)\n    .select(class_rate = (c.label == c.text).mean())\n)\n\n\nshape: (1, 1)\n\n\n\nclass_rate\n\n\nf64\n\n\n\n\n0.998714\n\n\n\n\n\n\nWhile the zero-shot accuracy makes a few errors (in comparison to the model above), it is remarkable that we can achieve any reasonable classification performance without a single labeled training example, let alone such a strong classification rate on the testing data. This approach is particularly valuable when labeled data is expensive or impossible to obtain, there are a very large number of categories, or when we need to quickly prototype a classification system for new categories.\nLet’s examine the images that the zero-shot classifier got wrong. Understanding these errors can provide insight into the model’s limitations and the ambiguity inherent in visual classification.\n\n(\n    birds\n    .join(embed, how=\"cross\")\n    .with_columns(\n        sim_score = dot_product(c.siglip, c.siglip_txt)   \n    )\n    .sort(c.sim_score, descending=True)\n    .group_by(c.filepath)\n    .head(1)\n    .filter(c.label != c.text)\n    .with_columns(\n        desc = pl.concat_str(\"label\", \"text\", separator=\" =&gt; \")\n    )\n    .pipe(DSImage.plot_image_grid, label_name=\"desc\", ncol=3)\n)\n\n\n\n\n\n\n\n\nThese two misclassifications involve birds in strange positions and that are cropped at the edges, cutting off import aspects that differentiate the species.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Transfer Learning</span>"
    ]
  },
  {
    "objectID": "15_transferlearn.html#text-embedding",
    "href": "15_transferlearn.html#text-embedding",
    "title": "15  Transfer Learning",
    "section": "15.5 Text Embedding",
    "text": "15.5 Text Embedding\nThe transfer learning approach we applied to images works equally well for text data. Just as image models learn to represent visual content in ways that transfer across tasks, language models learn to represent text in ways that capture semantic meaning. By extracting embeddings from a pre-trained language model, we can build text classifiers with minimal task-specific training.\nFor this section, we will use a dataset of movie reviews from IMDB. Each review is labeled as either positive or negative, and our goal is to predict the sentiment from the text content. This is the same task we approached in Chapter 13 using neural networks trained from scratch, which allows us to directly compare the transfer learning approach.\n\nimdb = pl.read_parquet(\"data/imdb5k_pca.parquet\")\nimdb\n\n\nshape: (5_000, 5)\n\n\n\nid\nlabel\ntext\nindex\ne5\n\n\nstr\nstr\nstr\nstr\nlist[f64]\n\n\n\n\n\"doc0001\"\n\"negative\"\n\"In my opinion, this movie is n…\n\"test\"\n[-0.253192, 0.099743, … -0.000806]\n\n\n\"doc0002\"\n\"positive\"\n\"Loved today's show!!! It was a…\n\"test\"\n[0.280815, -0.064508, … -0.0273]\n\n\n\"doc0003\"\n\"negative\"\n\"Nothing about this movie is an…\n\"test\"\n[-0.179638, 0.171097, … 0.007573]\n\n\n\"doc0004\"\n\"positive\"\n\"Even though this was a disaste…\n\"train\"\n[0.300824, 0.027666, … -0.028764]\n\n\n\"doc0005\"\n\"positive\"\n\"I cannot believe I enjoyed thi…\n\"test\"\n[0.319154, 0.035425, … 0.001316]\n\n\n…\n…\n…\n…\n…\n\n\n\"doc4996\"\n\"positive\"\n\"\"Americans Next Top Model\" is …\n\"test\"\n[0.305853, 0.026326, … 0.006194]\n\n\n\"doc4997\"\n\"negative\"\n\"It's very sad that Lucian Pint…\n\"train\"\n[0.217778, 0.283147, … -0.028361]\n\n\n\"doc4998\"\n\"positive\"\n\"Ruth Gordon at her best. This …\n\"train\"\n[0.373751, 0.076117, … -0.032517]\n\n\n\"doc4999\"\n\"negative\"\n\"I actually saw the movie befor…\n\"test\"\n[-0.073533, 0.10723, … 0.019535]\n\n\n\"doc5000\"\n\"positive\"\n\"I've Seen The Beginning Of The…\n\"test\"\n[0.147752, -0.095977, … -0.02008]\n\n\n\n\n\n\nThe dataset includes precomputed embeddings from the E5 model, a powerful text embedding model designed specifically to produce representations suitable for downstream tasks like classification and semantic search. Like the image embeddings we used earlier, these text embeddings capture the semantic content of each review in a fixed-dimensional vector that can be used as input to any classifier.\nTraining a logistic regression model on these embeddings follows exactly the same pattern we used for the bird classification task. The only difference is the source of the embeddings: images of birds versus reviews of movies.\n\nmodel = (\n    imdb\n    .pipe(\n        DSSklearn.logistic_regression_cv,\n        target=c.label,\n        features=[c.e5],\n        stratify=c.label,\n        l1_ratios=[0],\n        solver=\"saga\"\n    )\n)\n\nThe performance using transfer learning noticeably exceeds what we achieved in Chapter 13 with neural networks trained from scratch. This improvement comes from the E5 model’s pre-training on vast amounts of text data, which gives it a sophisticated understanding of language that would be impossible to learn from our relatively small training set.\n\nmodel.score()\n\n{'train': 0.9577142857142857, 'test': 0.9426666666666667}\n\n\n\nmodel.confusion_matrix()\n\n\n\n\n\n\n\n\nAs with the bird images, we can visualize the embedding space using UMAP to see how positive and negative reviews cluster. Well-separated clusters indicate that the embeddings capture the sentiment distinction effectively.\n\n(\n    imdb\n    .pipe(\n        DSSklearn.umap,\n        features=[c.e5]\n    )\n    .predict(full=True)\n    .pipe(ggplot, aes(\"dr0\", \"dr1\"))\n    + geom_point(aes(color=\"label\"))\n)\n\n\n\n\n\n\n\n\nThe clear separation between positive and negative reviews in this visualization explains why a simple logistic regression classifier performs so well: the embedding model has already done the hard work of mapping reviews into a space where sentiment is linearly separable.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Transfer Learning</span>"
    ]
  },
  {
    "objectID": "15_transferlearn.html#calling-an-api",
    "href": "15_transferlearn.html#calling-an-api",
    "title": "15  Transfer Learning",
    "section": "15.6 Calling an API",
    "text": "15.6 Calling an API\nThroughout this chapter, we have used pre-trained models that run locally on our computer. This gives us complete control over our data and allows us to process examples quickly once the models are loaded. However, the most powerful AI models available today are too large to run on typical hardware. These models, with billions or even trillions of parameters, require specialized infrastructure to operate and are typically accessed through web-based APIs.\nUsing an API to access AI capabilities represents another form of transfer learning. Instead of downloading model weights and running inference locally, we send our data to a remote server that processes it using state-of-the-art models and returns the results. This approach offers access to capabilities far beyond what we could run locally, at the cost of per-request pricing and the need to send data over the network.\nHere is an example of using OpenAI’s API to classify a movie review. The API accepts natural language instructions, so we can describe the classification task in plain English rather than training a model.\n\nclient = OpenAI()\n\nreview_text = imdb.select(\"text\").to_series()[0]\n\nresponse = client.responses.create(\n    model=\"gpt-5-mini-2025-08-07\",\n    input=(\n        \"Return only one word, either 'positive' or 'negative'\" +\n        \"for this review:\\n\\n\" +\n        review_text\n    )\n)\n\nlabel = response.output_text\nlabel\n\n\n\n'negative'\n\n\nThe simplicity of this approach is striking: we describe what we want in natural language, provide the input data, and receive a structured response. This zero-shot capability, combined with instruction-following, makes large language models incredibly versatile tools for data processing tasks that would traditionally require custom model development.\nHowever, API-based approaches have important tradeoffs to consider. Each request incurs a cost, which can add up quickly when processing large datasets. Latency is higher than local inference because data must travel over the network. Privacy concerns may prevent sending sensitive data to external services. And the behavior of API models can change without notice as providers update their systems. For production applications with stable requirements and sufficient training data, local models trained via transfer learning often provide a better balance of cost, reliability, and performance.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Transfer Learning</span>"
    ]
  },
  {
    "objectID": "15_transferlearn.html#conclusions",
    "href": "15_transferlearn.html#conclusions",
    "title": "15  Transfer Learning",
    "section": "15.7 Conclusions",
    "text": "15.7 Conclusions\nTransfer learning has fundamentally changed how we approach machine learning problems. Rather than treating each task as a fresh challenge requiring massive datasets and extensive training, we can build on the knowledge captured in pre-trained models to achieve strong performance with minimal effort. This chapter demonstrated three complementary approaches to leveraging pre-trained models.\nFirst, we used embeddings from vision and language models as features for traditional classifiers. This approach requires some labeled data but dramatically reduces the amount needed compared to training from scratch. The ViT embeddings for bird classification and E5 embeddings for sentiment analysis both yielded excellent results with simple logistic regression classifiers. The key insight is that modern embedding models transform raw inputs into representations where the concepts we care about are often linearly separable.\nSecond, we explored zero-shot classification with multimodal models like SigLIP. By learning to connect images and text in a shared embedding space, these models can classify images into arbitrary categories described in natural language without any task-specific training. While accuracy may be lower than fine-tuned approaches, zero-shot methods enable rapid prototyping and handle scenarios where labeled data is unavailable.\nThird, we briefly examined how external APIs provide access to capabilities beyond what we can run locally. Large language models accessible through APIs can perform classification and many other tasks given only natural language instructions. This represents the extreme end of transfer learning, where we leverage models trained on essentially all available text and images to perform specific tasks on demand.\nThe choice among these approaches depends on the specific requirements of each application. When labeled data is plentiful and consistent performance is critical, traditional embedding plus classifier approaches offer the best balance. When exploring new problems or working with limited labels, zero-shot methods provide a quick baseline. And when tasks require sophisticated language understanding or domain knowledge, API-based models may offer capabilities that local approaches cannot match.\nAs pre-trained models continue to improve, the bar for what can be accomplished with transfer learning keeps rising. Tasks that once required extensive custom development can now be solved by combining off-the-shelf embeddings with simple classifiers. This democratization of machine learning capabilities is one of the most significant developments in the field, making powerful AI accessible to practitioners who may not have the resources for large-scale model training.",
    "crumbs": [
      "Part III: Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Transfer Learning</span>"
    ]
  },
  {
    "objectID": "16_spatial_data.html",
    "href": "16_spatial_data.html",
    "title": "16  Spatial Data",
    "section": "",
    "text": "16.1 Setup\nLoad all of the modules and datasets needed for the chapter.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nmetro = pl.read_csv(\"data/acs_cbsa.csv\")",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "16_spatial_data.html#introduction",
    "href": "16_spatial_data.html#introduction",
    "title": "16  Spatial Data",
    "section": "16.2 Introduction",
    "text": "16.2 Introduction\nEvery piece of data we have worked with so far in this book has existed in an abstract space defined only by the relationships between variables. A country’s GDP, a patient’s blood pressure, or a product’s sales figures are all numbers that live in a purely numerical realm. But much of the world’s most valuable data is inherently tied to physical locations on Earth. Where did this crime occur? Which neighborhoods have the highest rates of asthma? How far is the nearest hospital from each school? These questions require a fundamentally different approach to data analysis, one that treats location not as just another variable but as a geometric object with shape, area, and spatial relationships to other objects.\nSpatial data analysis bridges the gap between traditional tabular data and the physical world. At its core, spatial data associates each observation with a geometry: a mathematical representation of a location or region on Earth’s surface. A geometry might be as simple as a single point (the location of a coffee shop) or as complex as a polygon with thousands of vertices (the boundary of a congressional district). The key insight is that once we have geometries, we can ask questions that would be impossible with ordinary data: Which points fall inside which polygons? Which polygons share a border? What is the nearest neighbor to each observation?\nIn this chapter, we introduce tools for working with spatial data in Python. Because the Polars library we have used throughout this book does not natively support spatial operations, we provide a wrapper class called DSGeo that allows us to leverage the powerful GeoPandas library while keeping our data in Polars DataFrames. This approach gives us the best of both worlds: the speed and ergonomics of Polars for standard data manipulation, combined with the mature spatial functionality of GeoPandas when we need it.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "16_spatial_data.html#spatial-points",
    "href": "16_spatial_data.html#spatial-points",
    "title": "16  Spatial Data",
    "section": "16.3 Spatial Points",
    "text": "16.3 Spatial Points\nThe simplest type of spatial data consists of points: individual locations specified by their coordinates. Points are used to represent discrete features like cities, sensor stations, retail locations, or any other phenomenon that can be meaningfully reduced to a single location. Each point is defined by two numbers: a longitude (the east-west position) and a latitude (the north-south position).\nWe begin with a dataset containing information about metropolitan statistical areas (MSAs) in the United States. Each row represents a single metro region, with columns for population, economic indicators, and geographic coordinates.\n\nmetro\n\n\nshape: (934, 13)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\n\n\n\n\n\"New York\"\n35620\n\"NE\"\n-74.101056\n40.76877\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\n\"Middle Atlantic\"\n\n\n\"Los Angeles\"\n31080\n\"W\"\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\n\"Pacific\"\n\n\n\"Chicago\"\n16980\n\"NC\"\n-87.95882\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\n\"East North Central\"\n\n\n\"Dallas\"\n19100\n\"S\"\n-96.970508\n32.84948\n7.54334\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\n\"West South Central\"\n\n\n\"Houston\"\n26420\n\"S\"\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\n\"West South Central\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Zapata\"\n49820\n\"S\"\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\n\"West South Central\"\n\n\n\"Ketchikan\"\n28540\n\"O\"\n-130.927217\n55.582484\n0.013939\n1.046556\n44.6\n77820\n67.5\n946\n31.2\n\"Pacific\"\n\n\n\"Craig\"\n18780\n\"W\"\n-108.207523\n40.618749\n0.01324\n1.077471\n44.5\n58583\n72.8\n485\n29.0\n\"Mountain\"\n\n\n\"Vernon\"\n46900\n\"S\"\n-99.240853\n34.080611\n0.012887\n5.086411\n41.5\n45262\n62.0\n503\n22.0\n\"West South Central\"\n\n\n\"Lamesa\"\n29500\n\"S\"\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\n\"West South Central\"\n\n\n\n\n\n\nBecause longitude and latitude are just numbers, we can create a basic visualization using plotnine without any special spatial tools. The longitude values go on the x-axis and latitude values on the y-axis.\n\n(\n    metro\n    .pipe(ggplot, aes(\"lon\", \"lat\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nThis scatter plot is functional but has several limitations. The most obvious is that it treats longitude and latitude as if they were ordinary Cartesian coordinates, when in fact they represent positions on a curved surface. The distortion is particularly noticeable at higher latitudes, where equal increments in longitude correspond to shorter physical distances. Additionally, we have no context for these points: no coastlines, state boundaries, or other geographic features to help orient the viewer.\nTo unlock more sophisticated spatial functionality, we need to convert our latitude and longitude columns into proper geometric objects. The DSGeo.from_latlon() method creates a new column called geometry that stores each point as a binary blob in a standardized format. This format encodes not just the coordinates but also metadata about the coordinate reference system.\n\nmetro = DSGeo.from_latlon(metro)\nmetro\n\n\nshape: (934, 14)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\ngeometry\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\nbinary\n\n\n\n\n\"New York\"\n35620\n\"NE\"\n-74.101056\n40.76877\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\n\"Middle Atlantic\"\nb\"\\x01\\x01\\x00\\x00\\x000\\xadY\\xb2w\\x86R\\xc0\\x13\\x04\\xd7\\x10gbD@\"\n\n\n\"Los Angeles\"\n31080\n\"W\"\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\n\"Pacific\"\nb\"\\x01\\x01\\x00\\x00\\x00\\x1d\\xadx\\xa7\\x84\\x89]\\xc0[\\xf9\\x8c|\\x15\\x1cA@\"\n\n\n\"Chicago\"\n16980\n\"NC\"\n-87.95882\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\n\"East North Central\"\nb\"\\x01\\x01\\x00\\x00\\x00\\x07\\x8boM]\\xfdU\\xc0F\\xce\\x0dn\\xad\\xd9D@\"\n\n\n\"Dallas\"\n19100\n\"S\"\n-96.970508\n32.84948\n7.54334\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\n\"West South Central\"\nb\"\\x01\\x01\\x00\\x00\\x00\\xaf\\xe3\\xc9\\xcc\\x1c&gt;X\\xc0\\x82]\\x16\\xc0\\xbbl@@\"\n\n\n\"Houston\"\n26420\n\"S\"\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\n\"West South Central\"\nb\"\\x01\\x01\\x00\\x00\\x00gd\\x01c\\xb3\\xd9W\\xc0\\x87\\xbbJH~\\xc9=@\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Zapata\"\n49820\n\"S\"\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\n\"West South Central\"\nb\"\\x01\\x01\\x00\\x00\\x00$l\\xfa=\\xc9\\xcaX\\xc0\\x15L[\\x93%\\x00;@\"\n\n\n\"Ketchikan\"\n28540\n\"O\"\n-130.927217\n55.582484\n0.013939\n1.046556\n44.6\n77820\n67.5\n946\n31.2\n\"Pacific\"\nb\"\\x01\\x01\\x00\\x00\\x00\\x99\\xe3\\xda\\xc3\\xab]`\\xc0\\xc4\\x90\\x8f\\xd8\\x8e\\xcaK@\"\n\n\n\"Craig\"\n18780\n\"W\"\n-108.207523\n40.618749\n0.01324\n1.077471\n44.5\n58583\n72.8\n485\n29.0\n\"Mountain\"\nb\"\\x01\\x01\\x00\\x00\\x00\\x92\\x88u\\x0dH\\x0d[\\xc0\\xe7Q\\x16+3OD@\"\n\n\n\"Vernon\"\n46900\n\"S\"\n-99.240853\n34.080611\n0.012887\n5.086411\n41.5\n45262\n62.0\n503\n22.0\n\"West South Central\"\nb\"\\x01\\x01\\x00\\x00\\x00\\xcf\\x9a\\x09!j\\xcfX\\xc0\\xd3z\\xccuQ\\x0aA@\"\n\n\n\"Lamesa\"\n29500\n\"S\"\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\n\"West South Central\"\nb\"\\x01\\x01\\x00\\x00\\x00oF\\xe3\\x15\\xa6|Y\\xc0R\\xc7~\\xdc\\x09_@@\"\n\n\n\n\n\n\nThe DataFrame now contains a geometry column that appears as a series of binary values. While these values are not human-readable, they can be manipulated by specialized spatial functions that understand their internal structure.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "16_spatial_data.html#coordinate-systems-crs",
    "href": "16_spatial_data.html#coordinate-systems-crs",
    "title": "16  Spatial Data",
    "section": "16.4 Coordinate Systems (CRS)",
    "text": "16.4 Coordinate Systems (CRS)\nOne of the most important concepts in spatial data analysis is the coordinate reference system (CRS), sometimes called a spatial reference system. A CRS defines how coordinates map to actual locations on Earth. This might seem straightforward, but Earth’s surface is curved while maps are flat. Any attempt to represent the globe on a two-dimensional surface requires making trade-offs: you can preserve shapes or areas or distances, but never all three simultaneously.\nThe most common CRS for storing geographic data is WGS 84 (World Geodetic System 1984), which uses latitude and longitude measured in degrees. This is the system used by GPS devices and most web mapping services. When we created our geometry column above, the coordinates were assumed to be in WGS 84. However, for visualization and certain calculations, we often want to project our data into a different CRS that is optimized for a particular region or purpose.\nEach CRS is identified by a numeric code in the EPSG (European Petroleum Survey Group) registry. For example:\n\nEPSG:4326 is WGS 84, the standard latitude/longitude system\nEPSG:5069 is NAD83 Conus Albers, an equal-area projection suitable for the contiguous United States\nEPSG:3857 is Web Mercator, used by Google Maps and other web mapping services\nEPSG:32618 is UTM Zone 18N, appropriate for the US East Coast\nEPSG:2154 is the official projection for Metropolitan France (including Corsica), mandated by French law\nEPSG:7791 is Italy’s modern national reference system and the most accurate for Italian data\n\nTo find the appropriate EPSG code for a specific region or purpose, you can search the EPSG.io website or consult the Spatial Reference database. These resources allow you to look up projections by name, region, or properties (such as whether they preserve area or shape).\nWhen we plot our metro data with a specific CRS, the coordinates are transformed to reflect the chosen projection. The Albers Equal Area projection (EPSG:5069) is designed to minimize distortion for the contiguous United States, making it a good choice for visualizing data across the country.\n\nDSGeo.plot(metro, crs=5069)\n\n\n\n\n\n\n\n\nNotice how the shape of the point cloud now more closely resembles the familiar outline of the continental United States. Alaska and Hawaii, if present in the data, would appear distorted under this projection because it is optimized for the lower 48 states.\n\n\n\n\n\n\nWhy projections matter\n\n\n\n\n\nThe choice of projection can have significant implications for analysis. Consider calculating the distance between two cities. In WGS 84 coordinates, the distance between two points depends on their latitude: one degree of longitude represents about 111 kilometers at the equator but only about 85 kilometers at 40° latitude (roughly the latitude of New York). If you compute distances using raw latitude/longitude values as if they were Cartesian coordinates, your results will be systematically wrong.\nSimilarly, if you want to calculate the area of a state or country, you need to use an equal-area projection. Using a projection that distorts areas (like Web Mercator) will give you incorrect results. This is why Greenland appears larger than Africa on many web maps, even though Africa is actually 14 times larger.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "16_spatial_data.html#interactive-visualization",
    "href": "16_spatial_data.html#interactive-visualization",
    "title": "16  Spatial Data",
    "section": "16.5 Interactive Visualization",
    "text": "16.5 Interactive Visualization\nStatic maps are useful for publications and reports, but interactive maps allow for exploration and discovery. The DSGeo.explore() function creates an interactive map powered by the Folium library, which in turn uses Leaflet.js. Users can zoom, pan, and click on features to see their attributes.\n\nDSGeo.explore(metro, tooltip=[c.name])\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThe tooltip parameter specifies which columns should be displayed when the user hovers over a point. Try zooming in on different regions and clicking on individual metro areas to see their names. The base map provides geographic context that was missing from our static scatter plot.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "16_spatial_data.html#spatial-polygons",
    "href": "16_spatial_data.html#spatial-polygons",
    "title": "16  Spatial Data",
    "section": "16.6 Spatial Polygons",
    "text": "16.6 Spatial Polygons\nWhile points are useful for representing discrete locations, many spatial phenomena are better represented as polygons: closed shapes that define the boundaries of regions. States, countries, zip codes, census tracts, and watersheds are all naturally represented as polygons. Each polygon is defined by a series of coordinate pairs that trace its boundary.\nSpatial data is commonly stored in specialized file formats that can encode complex geometries along with their attributes. The GeoJSON format is particularly popular for web applications because it is based on JSON (JavaScript Object Notation) and can be read by virtually any programming language. GeoJSON files have a standardized structure where each row in the equivalent table is called a feature, and all features are collected into a FeatureCollection. The official specification is maintained by the Internet Engineering Task Force as RFC 7946.\nOther common spatial formats include Shapefiles (a legacy format still widely used), GeoPackage (a modern SQLite-based format), and various database formats like PostGIS. Our DSGeo.read_file() function can read most of these formats.\n\nstate = DSGeo.read_file(\"data/acs_state.geojson\")\nstate\n\n\nshape: (50, 4)\n\n\n\nname\nabb\nfips\ngeometry\n\n\nstr\nstr\nstr\nbinary\n\n\n\n\n\"Maine\"\n\"ME\"\n\"23\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00C\\x00\\x00\\x00\\x93\\x15\\xa7Z\\x0b\\xadQ\\xc0\\x10&gt;yX\\xa8\\x87E@y\\x15\\x8cJ\\xea\\xb4Q\\xc0\\xb4\\xe3\\xa2ZD\\x90E@\\xda\\xe2p\\xe6W\\xb4\"…\n\n\n\"New Hampshire\"\n\"NH\"\n\"33\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00!\\x00\\x00\\x00\\xeb\\x1ag\\xd3\\x11\\xe0Q\\xc0a\\xc2hV\\xb6\\x81F@4\\x85\\xb2\\xf0\\xf5\\xd9Q\\xc0#\\x88f\\x9e\\\\x99F@`\\xc7G\\x8b3\\xd2\"…\n\n\n\"Delaware\"\n\"DE\"\n\"10\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x10\\x00\\x00\\x00ip[[x\\xf2R\\xc0\\x1a\\x1aO\\x04q\\xdcC@\\x1dr3\\xdc\\x80\\xe7R\\xc0X8I\\xf3\\xc7\\xeaC@\\xfe\\xf34`\\x90\\xda\"…\n\n\n\"South Carolina\"\n\"SC\"\n\"45\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x009\\x00\\x00\\x00y\\x89\\x94f\\xf3\\xc6T\\xc0\\x8e\\xb9\\x17\\x98\\x15\\x80A@\\x9cI\\xb7%r\\x9dT\\xc0s\\x81\\xcbc\\xcd\\x96A@\\xda`S\\xe7Q\\x8b\"…\n\n\n\"Nebraska\"\n\"NE\"\n\"31\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00.\\x00\\x00\\x00\\x18\\xe2\\xc9nf\\x03Z\\xc0;\\xefU+\\x13\\x80E@R\\xf3U\\xf2\\xb1\\xb2Y\\xc0\\xb9S:X\\xff\\x7fE@\\xb7\\x93\\x16.\\xabm\"…\n\n\n…\n…\n…\n…\n\n\n\"Florida\"\n\"FL\"\n\"12\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00t\\x00\\x00\\x00Z\\xd1\\x90\\xf1(@U\\xc0%\\xb2\\x0f\\xb2,\\x00?@\\x90rL\\x16\\xf7;U\\xc0\\x1du\\xacRz\\xe2&gt;@)7\\x19U\\x86:\"…\n\n\n\"Alaska\"\n\"AK\"\n\"02\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00k\\x02\\x00\\x009\\xaf\\xce1\\x20\\x04e\\xc0\\x16\\xa3\\xae\\xb5\\xf7iP@\\xa1\\x16-@[\\xffd\\xc0\\xc7\\x19\\xdf\\x17\\x97nP@k\\x90e\\xc1D\\xf1\"…\n\n\n\"New Jersey\"\n\"NJ\"\n\"34\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00$\\x00\\x00\\x00T\\xd9\\xe8\\x9c\\x9f\\xe0R\\xc0M.\\x00\\x8d\\xd2\\xd7C@\\xfe\\xf34`\\x90\\xdaR\\xc0-\\x85#H\\xa5\\xe6C@\\x8ft\\xb1i\\xa5\\xc9\"…\n\n\n\"North Dakota\"\n\"ND\"\n\"38\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x19\\x00\\x00\\x00X\\xabw\\xb8\\x1d\\x03Z\\xc0\\xab'\\xd6\\xa9\\xf2\\x7fH@\\x00\\xc9t\\xe8\\xf4\\x8dY\\xc01v\\xa4\\xfa\\xce\\x7fH@\\xfe\\x1e-\\xce\\x18H\"…\n\n\n\"Iowa\"\n\"IA\"\n\"19\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00&lt;\\x00\\x00\\x00O%;6\\x02\\x1dX\\xc0\\x7f\\x9f\\x8e\\xc7\\x0c\\xc0E@F\\xb5\\xa5\\x0e\\xf2\\xe0W\\xc0]\\xb5\\x89\\x93\\xfb\\xbfE@\\xee\\x10\\x8d\\xee\\x20~\"…\n\n\n\n\n\n\nThe geometry column now contains polygon geometries rather than points. Each polygon is defined by many coordinate pairs (sometimes thousands for states with complex coastlines), but they are all stored efficiently in the binary geometry format.\nUnlike point data, polygon data cannot be meaningfully visualized with a simple scatter plot. We need specialized functions that understand how to draw shapes from their vertex coordinates. The effect of projection is also much more noticeable with polygons than with points.\n\nDSGeo.plot(state, crs=5069)\n\n\n\n\n\n\n\n\nCompare this to what the same data would look like in unprojected coordinates. The familiar shapes of states would be distorted, particularly in the northern regions where lines of longitude converge. The Albers projection stretches and compresses the coordinates to create a more faithful representation of the true shapes and relative sizes of states.\nInteractive maps work with polygons just as they do with points. Users can hover over states to see their names and other attributes.\n\nDSGeo.explore(state, tooltip=[c.name])\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\n\nOther geometry types\n\n\n\n\n\nPoints and polygons are the most common geometry types, but GeoJSON and other spatial formats support several others:\n\nLineString: A sequence of connected points forming a path, useful for roads, rivers, or flight routes\nMultiPoint: A collection of points treated as a single feature\nMultiLineString: Multiple disconnected line segments treated as a single feature\nMultiPolygon: Multiple disconnected polygons treated as a single feature (useful for states with islands, like Hawaii)\nGeometryCollection: A heterogeneous collection containing any mix of the above types\n\nMost of the techniques in this chapter apply to all geometry types, though some operations (like calculating area) only make sense for polygons.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "16_spatial_data.html#polygon-metrics",
    "href": "16_spatial_data.html#polygon-metrics",
    "title": "16  Spatial Data",
    "section": "16.7 Polygon Metrics",
    "text": "16.7 Polygon Metrics\nOnce we have polygon geometries, we can compute various geometric properties. Two particularly useful properties are the centroid (the geometric center of a polygon) and the area (the amount of surface enclosed by the polygon’s boundary).\nThe centroid of a polygon is itself a point geometry. Adding centroids to our state data gives us a representative point location for each state, which can be useful for labeling maps or for spatial operations that require point geometries.\n\nstate = DSGeo.add_centroid(state)\nstate\n\n\nshape: (50, 6)\n\n\n\nname\nabb\nfips\ngeometry\nlon\nlat\n\n\nstr\nstr\nstr\nbinary\nf64\nf64\n\n\n\n\n\"Maine\"\n\"ME\"\n\"23\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00C\\x00\\x00\\x00\\x93\\x15\\xa7Z\\x0b\\xadQ\\xc0\\x10&gt;yX\\xa8\\x87E@y\\x15\\x8cJ\\xea\\xb4Q\\xc0\\xb4\\xe3\\xa2ZD\\x90E@\\xda\\xe2p\\xe6W\\xb4\"…\n-69.225019\n45.368586\n\n\n\"New Hampshire\"\n\"NH\"\n\"33\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00!\\x00\\x00\\x00\\xeb\\x1ag\\xd3\\x11\\xe0Q\\xc0a\\xc2hV\\xb6\\x81F@4\\x85\\xb2\\xf0\\xf5\\xd9Q\\xc0#\\x88f\\x9e\\\\x99F@`\\xc7G\\x8b3\\xd2\"…\n-71.579258\n43.683295\n\n\n\"Delaware\"\n\"DE\"\n\"10\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x10\\x00\\x00\\x00ip[[x\\xf2R\\xc0\\x1a\\x1aO\\x04q\\xdcC@\\x1dr3\\xdc\\x80\\xe7R\\xc0X8I\\xf3\\xc7\\xeaC@\\xfe\\xf34`\\x90\\xda\"…\n-75.501089\n38.988027\n\n\n\"South Carolina\"\n\"SC\"\n\"45\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x009\\x00\\x00\\x00y\\x89\\x94f\\xf3\\xc6T\\xc0\\x8e\\xb9\\x17\\x98\\x15\\x80A@\\x9cI\\xb7%r\\x9dT\\xc0s\\x81\\xcbc\\xcd\\x96A@\\xda`S\\xe7Q\\x8b\"…\n-80.892889\n33.904653\n\n\n\"Nebraska\"\n\"NE\"\n\"31\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00.\\x00\\x00\\x00\\x18\\xe2\\xc9nf\\x03Z\\xc0;\\xefU+\\x13\\x80E@R\\xf3U\\xf2\\xb1\\xb2Y\\xc0\\xb9S:X\\xff\\x7fE@\\xb7\\x93\\x16.\\xabm\"…\n-99.810823\n41.527493\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Florida\"\n\"FL\"\n\"12\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00t\\x00\\x00\\x00Z\\xd1\\x90\\xf1(@U\\xc0%\\xb2\\x0f\\xb2,\\x00?@\\x90rL\\x16\\xf7;U\\xc0\\x1du\\xacRz\\xe2&gt;@)7\\x19U\\x86:\"…\n-82.50424\n28.652783\n\n\n\"Alaska\"\n\"AK\"\n\"02\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00k\\x02\\x00\\x009\\xaf\\xce1\\x20\\x04e\\xc0\\x16\\xa3\\xae\\xb5\\xf7iP@\\xa1\\x16-@[\\xffd\\xc0\\xc7\\x19\\xdf\\x17\\x97nP@k\\x90e\\xc1D\\xf1\"…\n-152.675093\n64.489399\n\n\n\"New Jersey\"\n\"NJ\"\n\"34\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00$\\x00\\x00\\x00T\\xd9\\xe8\\x9c\\x9f\\xe0R\\xc0M.\\x00\\x8d\\xd2\\xd7C@\\xfe\\xf34`\\x90\\xdaR\\xc0-\\x85#H\\xa5\\xe6C@\\x8ft\\xb1i\\xa5\\xc9\"…\n-74.660909\n40.182638\n\n\n\"North Dakota\"\n\"ND\"\n\"38\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x19\\x00\\x00\\x00X\\xabw\\xb8\\x1d\\x03Z\\xc0\\xab'\\xd6\\xa9\\xf2\\x7fH@\\x00\\xc9t\\xe8\\xf4\\x8dY\\xc01v\\xa4\\xfa\\xce\\x7fH@\\xfe\\x1e-\\xce\\x18H\"…\n-100.467458\n47.446063\n\n\n\"Iowa\"\n\"IA\"\n\"19\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00&lt;\\x00\\x00\\x00O%;6\\x02\\x1dX\\xc0\\x7f\\x9f\\x8e\\xc7\\x0c\\xc0E@F\\xb5\\xa5\\x0e\\xf2\\xe0W\\xc0]\\xb5\\x89\\x93\\xfb\\xbfE@\\xee\\x10\\x8d\\xee\\x20~\"…\n-93.500512\n42.07356\n\n\n\n\n\n\nThe centroid column contains point geometries representing the center of each state. For roughly rectangular states like Colorado, the centroid falls near the geographic center. For states with irregular shapes or multiple disconnected parts, the centroid might fall outside the state’s actual boundary.\nCalculating area requires a projection that preserves area relationships. If we compute areas in unprojected WGS 84 coordinates, the results would be meaningless numbers that do not correspond to actual square kilometers or square miles. By specifying a CRS, we ensure that the calculated areas reflect true surface measurements.\n\nstate = DSGeo.add_area(state, crs=5069)\nstate.sort(c.area).select(c.name, c.area)\n\n\nshape: (50, 2)\n\n\n\nname\narea\n\n\nstr\nf64\n\n\n\n\n\"Rhode Island\"\n2765.092425\n\n\n\"Delaware\"\n5169.59288\n\n\n\"Connecticut\"\n12958.532624\n\n\n\"Hawaii\"\n15215.771452\n\n\n\"New Jersey\"\n20160.360977\n\n\n…\n…\n\n\n\"New Mexico\"\n314909.256017\n\n\n\"Montana\"\n380769.784499\n\n\n\"California\"\n409190.207104\n\n\n\"Texas\"\n687901.770044\n\n\n\"Alaska\"\n1.4557e6\n\n\n\n\n\n\nThe results show area in square meters (the default unit for most projected coordinate systems). Dividing by 1,000,000 would give square kilometers. As expected, Alaska is the largest state by far, followed by Texas and California.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "16_spatial_data.html#choropleth-maps",
    "href": "16_spatial_data.html#choropleth-maps",
    "title": "16  Spatial Data",
    "section": "16.8 Choropleth Maps",
    "text": "16.8 Choropleth Maps\nA choropleth map colors geographic regions according to the value of a variable. This type of visualization is extremely common for showing how quantities vary across space: election results by county, income levels by census tract, or disease rates by state. The column parameter tells the explore function which variable to use for coloring.\n\nDSGeo.explore(state, column=c.area, tooltip=[c.name])\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThe resulting map shows each state colored according to its area, with a legend indicating how colors correspond to values. Larger states appear darker (or whatever color scheme is in use), making spatial patterns immediately visible.\nChoropleth maps are powerful but can be misleading. Because larger regions occupy more visual space, they can dominate the viewer’s attention even if they contain fewer people or are otherwise less important. For this reason, choropleth maps are best suited for variables that are naturally area-based (like population density) rather than absolute counts (like total population).",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "16_spatial_data.html#spatial-joins",
    "href": "16_spatial_data.html#spatial-joins",
    "title": "16  Spatial Data",
    "section": "16.9 Spatial Joins",
    "text": "16.9 Spatial Joins\nIn Chapter 4, we learned how to join two tables based on a shared key column. Spatial joins extend this concept by joining tables based on their geometric relationships. Instead of matching rows where a key value is equal, we match rows where geometries intersect, touch, contain, or are within some distance of each other.\nThe most common spatial join matches points to the polygons that contain them. Given our metro areas (points) and states (polygons), we can determine which state contains each metro area by checking which state polygon each metro point falls inside.\n\nDSGeo.sjoin(metro, state)\n\n\nshape: (920, 21)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\nindex__right\nname__right\nabb\nfips\nlon__right\nlat__right\narea\ngeometry\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\ni64\nstr\nstr\nstr\nf64\nf64\nf64\nbinary\n\n\n\n\n\"New York\"\n35620\n\"NE\"\n-74.101056\n40.76877\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\n\"Middle Atlantic\"\n47\n\"New Jersey\"\n\"NJ\"\n\"34\"\n-74.660909\n40.182638\n20160.360977\nb\"\\x01\\x01\\x00\\x00\\x000\\xadY\\xb2w\\x86R\\xc0\\x13\\x04\\xd7\\x10gbD@\"\n\n\n\"Los Angeles\"\n31080\n\"W\"\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\n\"Pacific\"\n9\n\"California\"\n\"CA\"\n\"06\"\n-119.61258\n37.252557\n409190.207104\nb\"\\x01\\x01\\x00\\x00\\x00\\x1d\\xadx\\xa7\\x84\\x89]\\xc0[\\xf9\\x8c|\\x15\\x1cA@\"\n\n\n\"Chicago\"\n16980\n\"NC\"\n-87.95882\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\n\"East North Central\"\n28\n\"Illinois\"\n\"IL\"\n\"17\"\n-89.196324\n40.061609\n145928.761225\nb\"\\x01\\x01\\x00\\x00\\x00\\x07\\x8boM]\\xfdU\\xc0F\\xce\\x0dn\\xad\\xd9D@\"\n\n\n\"Dallas\"\n19100\n\"S\"\n-96.970508\n32.84948\n7.54334\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\n\"West South Central\"\n8\n\"Texas\"\n\"TX\"\n\"48\"\n-99.349969\n31.484882\n687901.770044\nb\"\\x01\\x01\\x00\\x00\\x00\\xaf\\xe3\\xc9\\xcc\\x1c&gt;X\\xc0\\x82]\\x16\\xc0\\xbbl@@\"\n\n\n\"Houston\"\n26420\n\"S\"\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\n\"West South Central\"\n8\n\"Texas\"\n\"TX\"\n\"48\"\n-99.349969\n31.484882\n687901.770044\nb\"\\x01\\x01\\x00\\x00\\x00gd\\x01c\\xb3\\xd9W\\xc0\\x87\\xbbJH~\\xc9=@\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Zapata\"\n49820\n\"S\"\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\n\"West South Central\"\n8\n\"Texas\"\n\"TX\"\n\"48\"\n-99.349969\n31.484882\n687901.770044\nb\"\\x01\\x01\\x00\\x00\\x00$l\\xfa=\\xc9\\xcaX\\xc0\\x15L[\\x93%\\x00;@\"\n\n\n\"Ketchikan\"\n28540\n\"O\"\n-130.927217\n55.582484\n0.013939\n1.046556\n44.6\n77820\n67.5\n946\n31.2\n\"Pacific\"\n46\n\"Alaska\"\n\"AK\"\n\"02\"\n-152.675093\n64.489399\n1.4557e6\nb\"\\x01\\x01\\x00\\x00\\x00\\x99\\xe3\\xda\\xc3\\xab]`\\xc0\\xc4\\x90\\x8f\\xd8\\x8e\\xcaK@\"\n\n\n\"Craig\"\n18780\n\"W\"\n-108.207523\n40.618749\n0.01324\n1.077471\n44.5\n58583\n72.8\n485\n29.0\n\"Mountain\"\n19\n\"Colorado\"\n\"CO\"\n\"08\"\n-105.54769\n38.997652\n269556.293547\nb\"\\x01\\x01\\x00\\x00\\x00\\x92\\x88u\\x0dH\\x0d[\\xc0\\xe7Q\\x16+3OD@\"\n\n\n\"Vernon\"\n46900\n\"S\"\n-99.240853\n34.080611\n0.012887\n5.086411\n41.5\n45262\n62.0\n503\n22.0\n\"West South Central\"\n8\n\"Texas\"\n\"TX\"\n\"48\"\n-99.349969\n31.484882\n687901.770044\nb\"\\x01\\x01\\x00\\x00\\x00\\xcf\\x9a\\x09!j\\xcfX\\xc0\\xd3z\\xccuQ\\x0aA@\"\n\n\n\"Lamesa\"\n29500\n\"S\"\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\n\"West South Central\"\n8\n\"Texas\"\n\"TX\"\n\"48\"\n-99.349969\n31.484882\n687901.770044\nb\"\\x01\\x01\\x00\\x00\\x00oF\\xe3\\x15\\xa6|Y\\xc0R\\xc7~\\xdc\\x09_@@\"\n\n\n\n\n\n\nThe result contains all columns from both DataFrames, with each metro area matched to the state it falls within. Metro areas that span state boundaries will be assigned to whichever state contains their center point (the coordinate we used when creating the point geometry). Notice that the geometry in the output is the point geometry from the first (left) DataFrame.\nWe can also reverse the order of the join to get state geometry in the result. When we put the states first, the output contains polygon geometries, with each state matched to the metro areas it contains.\n\nDSGeo.sjoin(state, metro)\n\n\nshape: (920, 21)\n\n\n\nname\nabb\nfips\nlon\nlat\narea\nindex__right\nname__right\ngeoid\nquad\nlon__right\nlat__right\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\ngeometry\n\n\nstr\nstr\nstr\nf64\nf64\nf64\ni64\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\nbinary\n\n\n\n\n\"Maine\"\n\"ME\"\n\"23\"\n-69.225019\n45.368586\n85262.523591\n104\n\"Portland\"\n38860\n\"NE\"\n-70.470321\n43.695543\n0.547792\n93.194683\n43.8\n77759\n76.8\n968\n28.9\n\"New England\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00C\\x00\\x00\\x00\\x93\\x15\\xa7Z\\x0b\\xadQ\\xc0\\x10&gt;yX\\xa8\\x87E@y\\x15\\x8cJ\\xea\\xb4Q\\xc0\\xb4\\xe3\\xa2ZD\\x90E@\\xda\\xe2p\\xe6W\\xb4\"…\n\n\n\"Maine\"\n\"ME\"\n\"23\"\n-69.225019\n45.368586\n85262.523591\n365\n\"Lewiston\"\n30340\n\"NE\"\n-70.206499\n44.16555\n0.110378\n85.911812\n43.4\n59287\n70.1\n674\n28.0\n\"New England\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00C\\x00\\x00\\x00\\x93\\x15\\xa7Z\\x0b\\xadQ\\xc0\\x10&gt;yX\\xa8\\x87E@y\\x15\\x8cJ\\xea\\xb4Q\\xc0\\xb4\\xe3\\xa2ZD\\x90E@\\xda\\xe2p\\xe6W\\xb4\"…\n\n\n\"Maine\"\n\"ME\"\n\"23\"\n-69.225019\n45.368586\n85262.523591\n337\n\"Augusta\"\n12300\n\"NE\"\n-69.767669\n44.408937\n0.123293\n50.14378\n44.9\n58097\n76.4\n663\n29.1\n\"New England\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00C\\x00\\x00\\x00\\x93\\x15\\xa7Z\\x0b\\xadQ\\xc0\\x10&gt;yX\\xa8\\x87E@y\\x15\\x8cJ\\xea\\xb4Q\\xc0\\xb4\\xe3\\xa2ZD\\x90E@\\xda\\xe2p\\xe6W\\xb4\"…\n\n\n\"Maine\"\n\"ME\"\n\"23\"\n-69.225019\n45.368586\n85262.523591\n287\n\"Bangor\"\n12620\n\"NE\"\n-68.650949\n45.397272\n0.152211\n16.55587\n42.7\n55125\n73.8\n720\n29.0\n\"New England\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00C\\x00\\x00\\x00\\x93\\x15\\xa7Z\\x0b\\xadQ\\xc0\\x10&gt;yX\\xa8\\x87E@y\\x15\\x8cJ\\xea\\xb4Q\\xc0\\xb4\\xe3\\xa2ZD\\x90E@\\xda\\xe2p\\xe6W\\xb4\"…\n\n\n\"New Hampshire\"\n\"NH\"\n\"33\"\n-71.579258\n43.683295\n24037.263031\n130\n\"Manchester\"\n31700\n\"NE\"\n-71.715762\n42.915227\n0.420504\n182.271376\n44.1\n86930\n70.8\n1009\n29.0\n\"New England\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00!\\x00\\x00\\x00\\xeb\\x1ag\\xd3\\x11\\xe0Q\\xc0a\\xc2hV\\xb6\\x81F@4\\x85\\xb2\\xf0\\xf5\\xd9Q\\xc0#\\x88f\\x9e\\\\x99F@`\\xc7G\\x8b3\\xd2\"…\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Iowa\"\n\"IA\"\n\"19\"\n-93.500512\n42.07356\n145800.681715\n900\n\"Storm Lake\"\n44740\n\"NC\"\n-95.151132\n42.735419\n0.020723\n13.813099\n41.9\n53645\n74.4\n555\n23.6\n\"West North Central\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00&lt;\\x00\\x00\\x00O%;6\\x02\\x1dX\\xc0\\x7f\\x9f\\x8e\\xc7\\x0c\\xc0E@F\\xb5\\xa5\\x0e\\xf2\\xe0W\\xc0]\\xb5\\x89\\x93\\xfb\\xbfE@\\xee\\x10\\x8d\\xee\\x20~\"…\n\n\n\"Iowa\"\n\"IA\"\n\"19\"\n-93.500512\n42.07356\n145800.681715\n923\n\"Spencer\"\n43980\n\"NC\"\n-95.150936\n43.082492\n0.01641\n11.083706\n46.4\n52307\n73.1\n571\n27.2\n\"West North Central\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00&lt;\\x00\\x00\\x00O%;6\\x02\\x1dX\\xc0\\x7f\\x9f\\x8e\\xc7\\x0c\\xc0E@F\\xb5\\xa5\\x0e\\xf2\\xe0W\\xc0]\\xb5\\x89\\x93\\xfb\\xbfE@\\xee\\x10\\x8d\\xee\\x20~\"…\n\n\n\"Iowa\"\n\"IA\"\n\"19\"\n-93.500512\n42.07356\n145800.681715\n596\n\"Mason City\"\n32380\n\"NC\"\n-93.26083\n43.203198\n0.050635\n20.045526\n43.9\n58753\n74.7\n576\n23.5\n\"West North Central\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00&lt;\\x00\\x00\\x00O%;6\\x02\\x1dX\\xc0\\x7f\\x9f\\x8e\\xc7\\x0c\\xc0E@F\\xb5\\xa5\\x0e\\xf2\\xe0W\\xc0]\\xb5\\x89\\x93\\xfb\\xbfE@\\xee\\x10\\x8d\\xee\\x20~\"…\n\n\n\"Iowa\"\n\"IA\"\n\"19\"\n-93.500512\n42.07356\n145800.681715\n917\n\"Spirit Lake\"\n44020\n\"NC\"\n-95.150806\n43.377979\n0.017536\n16.791337\n44.6\n65215\n83.1\n770\n30.0\n\"West North Central\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00&lt;\\x00\\x00\\x00O%;6\\x02\\x1dX\\xc0\\x7f\\x9f\\x8e\\xc7\\x0c\\xc0E@F\\xb5\\xa5\\x0e\\xf2\\xe0W\\xc0]\\xb5\\x89\\x93\\xfb\\xbfE@\\xee\\x10\\x8d\\xee\\x20~\"…\n\n\n\"Iowa\"\n\"IA\"\n\"19\"\n-93.500512\n42.07356\n145800.681715\n260\n\"Waterloo\"\n47940\n\"NC\"\n-92.471855\n42.536019\n0.168595\n43.066768\n39.2\n61632\n71.2\n625\n27.9\n\"West North Central\"\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00&lt;\\x00\\x00\\x00O%;6\\x02\\x1dX\\xc0\\x7f\\x9f\\x8e\\xc7\\x0c\\xc0E@F\\xb5\\xa5\\x0e\\xf2\\xe0W\\xc0]\\xb5\\x89\\x93\\xfb\\xbfE@\\xee\\x10\\x8d\\xee\\x20~\"…\n\n\n\n\n\n\nThis produces more rows than we have states because each state can contain multiple metro areas. The logic is analogous to a one-to-many relational join: the state geometry is duplicated for each metro area it contains.\nSpatial joins can use various predicates (spatial relationships) beyond simple containment. The predicate parameter specifies which relationship to test. For example, we can find which states share a border with which other states using the touches predicate.\n\nDSGeo.sjoin(state, state, predicate=\"touches\")\n\n\nshape: (214, 14)\n\n\n\nname\nabb\nfips\nlon\nlat\narea\nindex__right\nname__right\nabb__right\nfips__right\nlon__right\nlat__right\narea__right\ngeometry\n\n\nstr\nstr\nstr\nf64\nf64\nf64\ni64\nstr\nstr\nstr\nf64\nf64\nf64\nbinary\n\n\n\n\n\"Maine\"\n\"ME\"\n\"23\"\n-69.225019\n45.368586\n85262.523591\n1\n\"New Hampshire\"\n\"NH\"\n\"33\"\n-71.579258\n43.683295\n24037.263031\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00C\\x00\\x00\\x00\\x93\\x15\\xa7Z\\x0b\\xadQ\\xc0\\x10&gt;yX\\xa8\\x87E@y\\x15\\x8cJ\\xea\\xb4Q\\xc0\\xb4\\xe3\\xa2ZD\\x90E@\\xda\\xe2p\\xe6W\\xb4\"…\n\n\n\"New Hampshire\"\n\"NH\"\n\"33\"\n-71.579258\n43.683295\n24037.263031\n43\n\"Massachusetts\"\n\"MA\"\n\"25\"\n-71.844911\n42.278432\n20631.817699\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00!\\x00\\x00\\x00\\xeb\\x1ag\\xd3\\x11\\xe0Q\\xc0a\\xc2hV\\xb6\\x81F@4\\x85\\xb2\\xf0\\xf5\\xd9Q\\xc0#\\x88f\\x9e\\\\x99F@`\\xc7G\\x8b3\\xd2\"…\n\n\n\"New Hampshire\"\n\"NH\"\n\"33\"\n-71.579258\n43.683295\n24037.263031\n29\n\"Vermont\"\n\"VT\"\n\"50\"\n-72.66144\n44.078034\n24839.475007\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00!\\x00\\x00\\x00\\xeb\\x1ag\\xd3\\x11\\xe0Q\\xc0a\\xc2hV\\xb6\\x81F@4\\x85\\xb2\\xf0\\xf5\\xd9Q\\xc0#\\x88f\\x9e\\\\x99F@`\\xc7G\\x8b3\\xd2\"…\n\n\n\"New Hampshire\"\n\"NH\"\n\"33\"\n-71.579258\n43.683295\n24037.263031\n0\n\"Maine\"\n\"ME\"\n\"23\"\n-69.225019\n45.368586\n85262.523591\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00!\\x00\\x00\\x00\\xeb\\x1ag\\xd3\\x11\\xe0Q\\xc0a\\xc2hV\\xb6\\x81F@4\\x85\\xb2\\xf0\\xf5\\xd9Q\\xc0#\\x88f\\x9e\\\\x99F@`\\xc7G\\x8b3\\xd2\"…\n\n\n\"Delaware\"\n\"DE\"\n\"10\"\n-75.501089\n38.988027\n5169.59288\n33\n\"Maryland\"\n\"MD\"\n\"24\"\n-76.773391\n39.040321\n26772.630334\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x10\\x00\\x00\\x00ip[[x\\xf2R\\xc0\\x1a\\x1aO\\x04q\\xdcC@\\x1dr3\\xdc\\x80\\xe7R\\xc0X8I\\xf3\\xc7\\xeaC@\\xfe\\xf34`\\x90\\xda\"…\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Iowa\"\n\"IA\"\n\"19\"\n-93.500512\n42.07356\n145800.681715\n37\n\"Missouri\"\n\"MO\"\n\"29\"\n-92.477487\n38.367533\n180621.339618\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00&lt;\\x00\\x00\\x00O%;6\\x02\\x1dX\\xc0\\x7f\\x9f\\x8e\\xc7\\x0c\\xc0E@F\\xb5\\xa5\\x0e\\xf2\\xe0W\\xc0]\\xb5\\x89\\x93\\xfb\\xbfE@\\xee\\x10\\x8d\\xee\\x20~\"…\n\n\n\"Iowa\"\n\"IA\"\n\"19\"\n-93.500512\n42.07356\n145800.681715\n28\n\"Illinois\"\n\"IL\"\n\"17\"\n-89.196324\n40.061609\n145928.761225\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00&lt;\\x00\\x00\\x00O%;6\\x02\\x1dX\\xc0\\x7f\\x9f\\x8e\\xc7\\x0c\\xc0E@F\\xb5\\xa5\\x0e\\xf2\\xe0W\\xc0]\\xb5\\x89\\x93\\xfb\\xbfE@\\xee\\x10\\x8d\\xee\\x20~\"…\n\n\n\"Iowa\"\n\"IA\"\n\"19\"\n-93.500512\n42.07356\n145800.681715\n14\n\"Wisconsin\"\n\"WI\"\n\"55\"\n-90.012344\n44.630868\n145016.280762\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00&lt;\\x00\\x00\\x00O%;6\\x02\\x1dX\\xc0\\x7f\\x9f\\x8e\\xc7\\x0c\\xc0E@F\\xb5\\xa5\\x0e\\xf2\\xe0W\\xc0]\\xb5\\x89\\x93\\xfb\\xbfE@\\xee\\x10\\x8d\\xee\\x20~\"…\n\n\n\"Iowa\"\n\"IA\"\n\"19\"\n-93.500512\n42.07356\n145800.681715\n32\n\"Minnesota\"\n\"MN\"\n\"27\"\n-94.309123\n46.315559\n218497.340682\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00&lt;\\x00\\x00\\x00O%;6\\x02\\x1dX\\xc0\\x7f\\x9f\\x8e\\xc7\\x0c\\xc0E@F\\xb5\\xa5\\x0e\\xf2\\xe0W\\xc0]\\xb5\\x89\\x93\\xfb\\xbfE@\\xee\\x10\\x8d\\xee\\x20~\"…\n\n\n\"Iowa\"\n\"IA\"\n\"19\"\n-93.500512\n42.07356\n145800.681715\n7\n\"South Dakota\"\n\"SD\"\n\"46\"\n-100.232438\n44.436377\n199692.46709\nb\"\\x01\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00&lt;\\x00\\x00\\x00O%;6\\x02\\x1dX\\xc0\\x7f\\x9f\\x8e\\xc7\\x0c\\xc0E@F\\xb5\\xa5\\x0e\\xf2\\xe0W\\xc0]\\xb5\\x89\\x93\\xfb\\xbfE@\\xee\\x10\\x8d\\xee\\x20~\"…\n\n\n\n\n\n\nThis self-join matches each state to every other state that shares at least one point on its boundary. The result shows all pairs of neighboring states. Notice that each border appears twice: California touches Oregon, and Oregon touches California. If you only want each pair once, you would need to filter or deduplicate the results.\nOther useful predicates include:\n\nintersects: Geometries share any space (including boundaries)\ncontains: The first geometry completely encloses the second\nwithin: The first geometry is completely enclosed by the second\ncrosses: Geometries share some but not all interior points\noverlaps: Geometries share some space but neither contains the other",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "16_spatial_data.html#nearest-neighbor-joins",
    "href": "16_spatial_data.html#nearest-neighbor-joins",
    "title": "16  Spatial Data",
    "section": "16.10 Nearest Neighbor Joins",
    "text": "16.10 Nearest Neighbor Joins\nSometimes we want to match features not based on overlap or containment but based on proximity. The sjoin_nearest function finds, for each feature in the first DataFrame, the closest feature in the second DataFrame. This is useful for questions like “What is the nearest hospital to each school?” or “Which weather station is closest to each measurement location?”\n\nDSGeo.sjoin_nearest(metro, metro, exclusive=True)\n\n\nshape: (934, 28)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\nindex_right\nname_right\ngeoid_right\nquad_right\nlon_right\nlat_right\npop_right\ndensity_right\nage_median_right\nhh_income_median_right\npercent_own_right\nrent_1br_median_right\nrent_perc_income_right\ndivision_right\ngeometry\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\ni64\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\nbinary\n\n\n\n\n\"New York\"\n35620\n\"NE\"\n-74.101056\n40.76877\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\n\"Middle Atlantic\"\n141\n\"Trenton\"\n45940\n\"NE\"\n-74.701703\n40.283417\n0.384951\n650.172055\n44.7\n85687\n65.7\n1126\n30.2\n\"Middle Atlantic\"\nb\"\\x01\\x01\\x00\\x00\\x000\\xadY\\xb2w\\x86R\\xc0\\x13\\x04\\xd7\\x10gbD@\"\n\n\n\"Los Angeles\"\n31080\n\"W\"\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\n\"Pacific\"\n70\n\"Oxnard\"\n37100\n\"W\"\n-119.083405\n34.45556\n0.845255\n175.81324\n42.1\n94150\n63.3\n1582\n34.2\n\"Pacific\"\nb\"\\x01\\x01\\x00\\x00\\x00\\x1d\\xadx\\xa7\\x84\\x89]\\xc0[\\xf9\\x8c|\\x15\\x1cA@\"\n\n\n\"Chicago\"\n16980\n\"NC\"\n-87.95882\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\n\"East North Central\"\n368\n\"Kankakee\"\n28100\n\"NC\"\n-87.861944\n41.137788\n0.108104\n61.34992\n41.8\n61664\n70.4\n701\n30.5\n\"East North Central\"\nb\"\\x01\\x01\\x00\\x00\\x00\\x07\\x8boM]\\xfdU\\xc0F\\xce\\x0dn\\xad\\xd9D@\"\n\n\n\"Dallas\"\n19100\n\"S\"\n-96.970508\n32.84948\n7.54334\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\n\"West South Central\"\n685\n\"Gainesville\"\n23620\n\"S\"\n-97.212584\n33.639178\n0.041215\n17.700303\n42.5\n63338\n69.6\n666\n25.0\n\"West South Central\"\nb\"\\x01\\x01\\x00\\x00\\x00\\xaf\\xe3\\xc9\\xcc\\x1c&gt;X\\xc0\\x82]\\x16\\xc0\\xbbl@@\"\n\n\n\"Houston\"\n26420\n\"S\"\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\n\"West South Central\"\n683\n\"El Campo\"\n20900\n\"S\"\n-96.222207\n29.277782\n0.041602\n14.65937\n41.8\n53963\n69.2\n600\n26.3\n\"West South Central\"\nb\"\\x01\\x01\\x00\\x00\\x00gd\\x01c\\xb3\\xd9W\\xc0\\x87\\xbbJH~\\xc9=@\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Zapata\"\n49820\n\"S\"\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\n\"West South Central\"\n501\n\"Rio Grande City\"\n40100\n\"S\"\n-98.738748\n26.562072\n0.065568\n20.57443\n40.5\n33334\n75.3\n390\n32.5\n\"West South Central\"\nb\"\\x01\\x01\\x00\\x00\\x00$l\\xfa=\\xc9\\xcaX\\xc0\\x15L[\\x93%\\x00;@\"\n\n\n\"Ketchikan\"\n28540\n\"O\"\n-130.927217\n55.582484\n0.013939\n1.046556\n44.6\n77820\n67.5\n946\n31.2\n\"Pacific\"\n799\n\"Juneau\"\n27940\n\"O\"\n-134.169542\n58.45342\n0.03224\n4.445802\n41.8\n90126\n68.5\n1095\n25.0\n\"Pacific\"\nb\"\\x01\\x01\\x00\\x00\\x00\\x99\\xe3\\xda\\xc3\\xab]`\\xc0\\xc4\\x90\\x8f\\xd8\\x8e\\xcaK@\"\n\n\n\"Craig\"\n18780\n\"W\"\n-108.207523\n40.618749\n0.01324\n1.077471\n44.5\n58583\n72.8\n485\n29.0\n\"Mountain\"\n864\n\"Steamboat Springs\"\n44460\n\"W\"\n-106.990829\n40.484142\n0.024899\n4.063905\n41.4\n83725\n76.2\n1371\n29.7\n\"Mountain\"\nb\"\\x01\\x01\\x00\\x00\\x00\\x92\\x88u\\x0dH\\x0d[\\xc0\\xe7Q\\x16+3OD@\"\n\n\n\"Vernon\"\n46900\n\"S\"\n-99.240853\n34.080611\n0.012887\n5.086411\n41.5\n45262\n62.0\n503\n22.0\n\"West South Central\"\n861\n\"Altus\"\n11060\n\"S\"\n-99.414999\n34.587933\n0.02496\n11.981281\n40.8\n55551\n62.7\n489\n23.4\n\"West South Central\"\nb\"\\x01\\x01\\x00\\x00\\x00\\xcf\\x9a\\x09!j\\xcfX\\xc0\\xd3z\\xccuQ\\x0aA@\"\n\n\n\"Lamesa\"\n29500\n\"S\"\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\n\"West South Central\"\n253\n\"Midland\"\n33260\n\"S\"\n-101.991236\n32.08914\n0.172177\n36.548597\n38.9\n87812\n70.7\n1128\n26.8\n\"West South Central\"\nb\"\\x01\\x01\\x00\\x00\\x00oF\\xe3\\x15\\xa6|Y\\xc0R\\xc7~\\xdc\\x09_@@\"\n\n\n\n\n\n\nHere we join the metro dataset to itself to find each metro area’s nearest neighbor. The exclusive=True parameter prevents each metro area from matching with itself (which would always be distance zero). The result shows pairs of metro areas that are nearest neighbors, along with the distance between them.\nNearest neighbor joins are computationally expensive because they potentially require comparing every feature in the first DataFrame to every feature in the second. For large datasets, this can be slow. Spatial libraries use sophisticated indexing structures (like R-trees) to speed up these queries, but they still scale less favorably than simple predicate-based joins.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "16_spatial_data.html#best-practices",
    "href": "16_spatial_data.html#best-practices",
    "title": "16  Spatial Data",
    "section": "16.11 Best Practices",
    "text": "16.11 Best Practices\nWorking with spatial data introduces several potential sources of error that do not arise with ordinary tabular data.\nCRS mismatches: If two datasets use different coordinate reference systems, spatial operations will give incorrect results. Points might appear to be in the wrong location, or spatial joins might fail to find matches. Always verify that your datasets are in compatible CRSs before performing spatial operations, and transform them to a common CRS if necessary.\nUnprojected coordinates for calculations: Never calculate distances or areas using raw latitude/longitude coordinates as if they were Cartesian. The errors can be substantial, especially for features that span large areas or are located far from the equator. Always project your data to an appropriate CRS first.\nPolygon validity: Complex polygons can sometimes become geometrically invalid due to self-intersections or other topological errors. Many spatial functions will fail or give incorrect results on invalid geometries. If you encounter strange errors, check whether your geometries are valid and repair them if necessary.\nBoundary effects: Features that span boundaries between regions can cause problems for spatial joins. A metro area that straddles a state line might be assigned to one state or the other depending on where its center point falls. Be aware of these edge cases and handle them appropriately for your analysis.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "16_spatial_data.html#references",
    "href": "16_spatial_data.html#references",
    "title": "16  Spatial Data",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "17_temporal_data.html",
    "href": "17_temporal_data.html",
    "title": "17  Temporal Data",
    "section": "",
    "text": "17.1 Setup\nLoad all of the modules and datasets needed for the chapter.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nmeta = pl.read_csv(\"data/wiki_uk_meta.csv.gz\")\npage_views = pl.read_csv(\"data/wiki_uk_page_views.csv\")\npage_revisions = pl.read_csv(\"data/wiki_uk_page_revisions.csv\")",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "17_temporal_data.html#introduction",
    "href": "17_temporal_data.html#introduction",
    "title": "17  Temporal Data",
    "section": "17.2 Introduction",
    "text": "17.2 Introduction\nIn this chapter, we discuss techniques for working with data that has some temporal component. This includes any kind of data that has one or more variables that record dates or times, as well as any dataset that has a meaningful ordering of its rows. For example, the annotation object that we created for textual data in Chapter 19 has a meaningful ordering and can be treated as having a temporal ordering even if it is not associated with fixed timestamps. We will start by focusing specifically on datasets that contain explicit information about dates and times. In the later sections, we will illustrate window functions and range joins, both of which have a wider set of applications to all ordered datasets.\nAs we saw in Chapter 4, it is possible to store information about dates and times in a tabular dataset. There are many different formats for storing this information; we recommend that most users start by recording these with separate columns for each numeric component of the date or time. This makes it easier to avoid errors and to record partial information, the latter being a common complication of many humanities datasets. We will begin by looking at a dataset related to the Wikipedia pages we saw in the previous two chapters that has date information stored in such a format.\nIn showing the application of line graphs in Chapter 3, we saw how to visualize a dataset of food prices over a 140-year period. This visualization was fairly straightforward. There was exactly one row for each year. We were able to treat the year variable as any other continuous measurement, with the only change being that it made sense to connect dots with a line when building the visualization. Here we will work with a slightly more complex example corresponding to the Wikipedia pages from the preceding chapters.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "17_temporal_data.html#temporal-data-and-ordering",
    "href": "17_temporal_data.html#temporal-data-and-ordering",
    "title": "17  Temporal Data",
    "section": "17.3 Temporal Data and Ordering",
    "text": "17.3 Temporal Data and Ordering\nLet’s start by loading some data with a temporal component. Below, we read in data related to the 75 Wikipedia pages from a selection of British authors. Here, we have a different set of information about the pages than we used in the text and network analysis chapters. For each page, we have grabbed page view statistics for a 60-day period from Wikipedia. In other words, we have a record of how many people looked at a particular page each day, for each author. The data are organized with one row for each combination of item and day.\n\npage_views\n\n\nshape: (4_490, 5)\n\n\n\ndoc_id\nyear\nmonth\nday\nviews\n\n\nstr\ni64\ni64\ni64\ni64\n\n\n\n\n\"Marie de France\"\n2023\n8\n1\n121\n\n\n\"Marie de France\"\n2023\n8\n2\n138\n\n\n\"Marie de France\"\n2023\n8\n3\n138\n\n\n\"Marie de France\"\n2023\n8\n4\n129\n\n\n\"Marie de France\"\n2023\n8\n5\n104\n\n\n…\n…\n…\n…\n…\n\n\n\"Seamus Heaney\"\n2023\n9\n25\n719\n\n\n\"Seamus Heaney\"\n2023\n9\n26\n802\n\n\n\"Seamus Heaney\"\n2023\n9\n27\n694\n\n\n\"Seamus Heaney\"\n2023\n9\n28\n655\n\n\n\"Seamus Heaney\"\n2023\n9\n29\n719\n\n\n\n\n\n\nThe time variables are given the way we recommended in Chapter 4, with individual columns for year, month, and day. Here, our dataset is already ordered (within each item type) from the earliest records to the latest. If this were not the case, because all of our variables are stored as numbers, we could use the .sort() method to sort by year, followed by month, followed by day, to get the same ordering.\nHow could we show the change in page views over time for a particular variable? One approach is to add a numeric column running down the dataset using a row number. Below is an example of the code to create a line plot using this approach:\n\n(\n    page_views\n    .filter(c.doc_id == \"Geoffrey Chaucer\")\n    .with_row_index(\"row_number\")\n    .pipe(ggplot, aes(\"row_number\", \"views\"))\n    + geom_line(color=\"red\")\n    + labs(\n        title=\"Page Views for Geoffrey Chaucer (by Day Number)\",\n        x=\"Day Number\",\n        y=\"Views\"\n    )\n)\n\n\n\n\n\n\n\n\nIn this case, our starting plot is not a bad place to begin. The x-axis corresponds to the day number, and in many applications that may be exactly what we need. We can clearly see that the number of page views for Chaucer has a relatively stable count, possibly with some periodic swings over the course of the week. There is one day about two-thirds of the way through the plot in which the count spikes. Notice, though, that it is very hard to tell anything from the plot about exactly what days of the year are being represented. We cannot easily see which day has the spike in views, for example. Also, note that the correspondence between the row number and day only works because the data are uniformly sampled (one observation each day) and there is no missing data.\nAnother way to work with dates is to convert the data to a fractional year format. Here, the months and days are added to form a fractional day. A quick way to do this is to compute the following fractional year:\n\\[ year_{frac} = year + \\frac{month - 1}{12} + \\frac{day - 1}{12 \\cdot 31}\\]\nWe are subtracting one from the month and day so, for example, on a date such as 1 July 2020 (halfway through the year) we have the fractional year equal to 2020.5. We could make this even more exact by accounting for the fact that some months have fewer than 31 days, but as a first pass this works relatively well.\n\n(\n    page_views\n    .filter(c.doc_id == \"Geoffrey Chaucer\")\n    .with_columns(\n        year_frac = c.year + (c.month - 1) / 12 + (c.day - 1) / (12 * 31)\n    )\n    .pipe(ggplot, aes(\"year_frac\", \"views\"))\n    + geom_line(color=\"red\")\n    + labs(\n        title=\"Page Views for Geoffrey Chaucer (Fractional Year)\",\n        x=\"Fractional Year\",\n        y=\"Views\"\n    )\n)\n\n\n\n\n\n\n\n\nThis revised visualization improves on several aspects of the original plot. For one thing, we can roughly see exactly what dates correspond to each data point. Also, the code will work fine regardless of whether the data are sorted, evenly distributed, or contain any missing values. As a downside, the axis labels take some explaining. We can extend the same approach to working with time data. For example, if we also had the (24-hour) time of our data points the formula would become:\n\\[ year_{frac} = year + \\frac{month - 1}{12} + \\frac{day - 1}{12 \\cdot 31} + \\frac{hour - 1}{24 \\cdot 12 \\cdot 31}\\]\nIf we are only interested in the time since a specific event, say the start of an experiment, we can use the same approach but take the difference relative to a specific fractional year.\nFractional times have a number of important applications. Fractional times are convenient because they can represent an arbitrarily precise date or date-time with an ordinary number. This means that they can be used in other models and applications without any special treatment. They may require different model assumptions, but at least the code should work with minimal effort. This is a great way to explore our data. However, particularly when we want to create nice publishable visualizations, it can be useful to work with specific functions for manipulating dates and times.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "17_temporal_data.html#date-objects",
    "href": "17_temporal_data.html#date-objects",
    "title": "17  Temporal Data",
    "section": "17.4 Date Objects",
    "text": "17.4 Date Objects\nMost of the variables that we have worked with so far are either strings or numbers. Dates are in some ways like numbers: they have a natural ordering, we can talk about the difference between two numbers, and it makes sense to color and plot them on a continuous scale. However, they do have some unique properties, particularly when we want to extract information such as the day of the week from a date, that require a unique data type. To create a date object in Polars, we can use the pl.date() function to construct a date from its components.\n\nchaucer_with_dates = (\n    page_views\n    .filter(c.doc_id == \"Geoffrey Chaucer\")\n    .with_columns(\n        date = pl.date(c.year, c.month, c.day)\n    )\n)\nchaucer_with_dates\n\n\nshape: (60, 6)\n\n\n\ndoc_id\nyear\nmonth\nday\nviews\ndate\n\n\nstr\ni64\ni64\ni64\ni64\ndate\n\n\n\n\n\"Geoffrey Chaucer\"\n2023\n8\n1\n2133\n2023-08-01\n\n\n\"Geoffrey Chaucer\"\n2023\n8\n2\n1977\n2023-08-02\n\n\n\"Geoffrey Chaucer\"\n2023\n8\n3\n1860\n2023-08-03\n\n\n\"Geoffrey Chaucer\"\n2023\n8\n4\n1739\n2023-08-04\n\n\n\"Geoffrey Chaucer\"\n2023\n8\n5\n1849\n2023-08-05\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Geoffrey Chaucer\"\n2023\n9\n25\n2384\n2023-09-25\n\n\n\"Geoffrey Chaucer\"\n2023\n9\n26\n2188\n2023-09-26\n\n\n\"Geoffrey Chaucer\"\n2023\n9\n27\n1950\n2023-09-27\n\n\n\"Geoffrey Chaucer\"\n2023\n9\n28\n2590\n2023-09-28\n\n\n\"Geoffrey Chaucer\"\n2023\n9\n29\n2082\n2023-09-29\n\n\n\n\n\n\nNotice that the new column has the special data type Date. If we build a visualization using a date object, plotnine is able to make helpful built-in choices about how to label the axis. For example, the following code will make a line plot that has nicely labeled values on the x-axis.\n\n(\n    chaucer_with_dates\n    .pipe(ggplot, aes(\"date\", \"views\"))\n    + geom_line(color=\"red\")\n    + labs(\n        title=\"Page Views for Geoffrey Chaucer\",\n        x=\"Date\",\n        y=\"Views\"\n    )\n)\n\n\n\n\n\n\n\n\nThe output shows that the algorithm decided to label the dates appropriately. We can manually change the frequency of the labels using scale_x_date() and setting the date_breaks option. For example, the code below will display one label for each week:\n\n(\n    chaucer_with_dates\n    .pipe(ggplot, aes(\"date\", \"views\"))\n    + geom_line(color=\"red\")\n    + scale_x_date(date_breaks=\"1 week\", date_labels=\"%Y-%m-%d\")\n    + theme(axis_text_x=element_text(angle=45, hjust=1))\n    + labs(\n        title=\"Page Views for Geoffrey Chaucer (Weekly Labels)\",\n        x=\"Date\",\n        y=\"Views\"\n    )\n)\n\n\n\n\n\n\n\n\nOnce we have a date object, we can also extract useful information from it. For example, we can extract the weekday of the date using the .dt namespace in Polars. Here, we will compute the weekday and then calculate the average number of page views for each day of the week:\n\n(\n    chaucer_with_dates\n    .with_columns(\n        weekday = c.date.dt.weekday()\n    )\n    .group_by(c.weekday)\n    .agg(\n        views_avg = c.views.mean()\n    )\n    .sort(c.views_avg, descending=True)\n)\n\n\nshape: (7, 2)\n\n\n\nweekday\nviews_avg\n\n\ni8\nf64\n\n\n\n\n2\n2273.111111\n\n\n4\n2098.555556\n\n\n3\n1988.111111\n\n\n1\n1975.75\n\n\n7\n1865.5\n\n\n5\n1828.222222\n\n\n6\n1762.375\n\n\n\n\n\n\nHere we see the average number of page views by day of the week, where Monday is 1 and Sunday is 7. We can also use the date objects to filter the dataset. For example, we can filter the dataset to only include those dates after 15 January 2020, about two-thirds of the way through our dataset:\n\n(\n    chaucer_with_dates\n    .filter(c.date &gt; pl.date(2020, 1, 15))\n)\n\n\nshape: (60, 6)\n\n\n\ndoc_id\nyear\nmonth\nday\nviews\ndate\n\n\nstr\ni64\ni64\ni64\ni64\ndate\n\n\n\n\n\"Geoffrey Chaucer\"\n2023\n8\n1\n2133\n2023-08-01\n\n\n\"Geoffrey Chaucer\"\n2023\n8\n2\n1977\n2023-08-02\n\n\n\"Geoffrey Chaucer\"\n2023\n8\n3\n1860\n2023-08-03\n\n\n\"Geoffrey Chaucer\"\n2023\n8\n4\n1739\n2023-08-04\n\n\n\"Geoffrey Chaucer\"\n2023\n8\n5\n1849\n2023-08-05\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Geoffrey Chaucer\"\n2023\n9\n25\n2384\n2023-09-25\n\n\n\"Geoffrey Chaucer\"\n2023\n9\n26\n2188\n2023-09-26\n\n\n\"Geoffrey Chaucer\"\n2023\n9\n27\n1950\n2023-09-27\n\n\n\"Geoffrey Chaucer\"\n2023\n9\n28\n2590\n2023-09-28\n\n\n\"Geoffrey Chaucer\"\n2023\n9\n29\n2082\n2023-09-29\n\n\n\n\n\n\nNote that we use the pl.date() function to create a date literal for comparison. Polars will also accept string representations of dates in ISO format for filtering.\n\n\n\n\n\n\nExtracting date components\n\n\n\n\n\nPolars provides many methods in the .dt namespace for extracting components from date and datetime objects. Common examples include .dt.year(), .dt.month(), .dt.day(), .dt.weekday(), .dt.ordinal_day() (day of year), and .dt.quarter(). These are useful for creating features for analysis or for grouping data by time periods.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "17_temporal_data.html#datetime-objects",
    "href": "17_temporal_data.html#datetime-objects",
    "title": "17  Temporal Data",
    "section": "17.5 Datetime Objects",
    "text": "17.5 Datetime Objects\nThe page_views dataset records the date of each observation. Sometimes we have data that describes the time of an event more specifically in terms of hours, minutes, and even possibly seconds. We will use the term datetime to describe an object that stores the precise time that an event occurs. The idea is that to describe the time that something happens we need to specify a date and a time. Later in the chapter, we will see an object that stores time without reference to a particular day. Whereas dates have a natural precision (a single day), we might desire to work with datetime objects of different levels of granularity. In some cases we might have just hours of the day and in others we might have access to records at the level of a millisecond such as data from radio and TV. In Polars, datetime objects are stored with microsecond precision by default, but we can regard the precision as whatever granularity we have given in our data for all practical purposes.\nDatetime objects largely function the same as date objects. Let’s grab another dataset from Wikipedia that has precise timestamps. Below, we read in a dataset consisting of the last 500 edits made to each of the 75 British author pages in our collection.\n\npage_revisions = (\n    page_revisions\n    .with_columns(\n        datetime = c.datetime.str.to_datetime(time_zone=\"UTC\")\n    )\n)\npage_revisions\n\n\nshape: (35_470, 5)\n\n\n\ndoc_id\nuser\ndatetime\nsize\ncomment\n\n\nstr\nstr\ndatetime[μs, UTC]\ni64\nstr\n\n\n\n\n\"Marie de France\"\n\"YurikBot\"\n2006-07-06 23:24:54 UTC\n3061\n\"robot  Adding: [[it:Maria di F…\n\n\n\"Marie de France\"\n\"YurikBot\"\n2006-07-24 18:39:08 UTC\n3085\n\"robot  Adding: [[pt:Maria de F…\n\n\n\"Marie de France\"\n\"Ccarroll\"\n2006-09-10 13:36:44 UTC\n3085\nnull\n\n\n\"Marie de France\"\n\"63.231.20.88\"\n2006-09-18 04:59:53 UTC\n3105\nnull\n\n\n\"Marie de France\"\n\"ExplicitImplicity\"\n2006-09-23 22:27:20 UTC\n3104\n\"i believe it is stupid to tran…\n\n\n…\n…\n…\n…\n…\n\n\n\"Seamus Heaney\"\n\"InternetArchiveBot\"\n2023-09-21 04:51:45 UTC\n85491\n\"Rescuing 1 sources and tagging…\n\n\n\"Seamus Heaney\"\n\"81.110.56.233\"\n2023-09-24 12:10:49 UTC\n85493\nnull\n\n\n\"Seamus Heaney\"\n\"149.50.162.177\"\n2023-09-25 16:06:01 UTC\n85482\n\"/* Death */\"\n\n\n\"Seamus Heaney\"\n\"--WikiUser1234945--\"\n2023-09-25 16:06:11 UTC\n85493\n\"Reverted edits by [[Special:Co…\n\n\n\"Seamus Heaney\"\n\"BD2412\"\n2023-09-27 02:39:46 UTC\n85493\n\"/* 1957–1969 */Clean up spacin…\n\n\n\n\n\n\nNotice that each row has a record in the column datetime that provides a precise datetime object giving the second at which the page was modified. The data were stored using the ISO-8601 format (“YYYY-MM-DD HH:MM:SS”), which Polars can automatically parse with the .str.to_datetime() method.\nOur page_revisions dataset includes several pieces of information about each of the edits. We have a username for the person who made the edit (recall that anyone can edit a Wikipedia page), the size in bytes of the page after the edit was made, and a short comment describing what was done in the change. Looking at the page size over time shows when large additions and deletions were made to each record. The code below yields a temporal visualization:\n\nselected_authors = [\"Geoffrey Chaucer\", \"Emily Brontë\"]\n(\n    page_revisions\n    .filter(c.doc_id.is_in(selected_authors))\n    .pipe(ggplot, aes(\"datetime\", \"size\", color=\"doc_id\"))\n    + geom_line()\n    + scale_color_manual(values=[\"red\", \"blue\"])\n    + labs(\n        title=\"Wikipedia Page Size Over Time\",\n        x=\"Date\",\n        y=\"Page Size (bytes)\",\n        color=\"Author\"\n    )\n)\n\n\n\n\n\n\n\n\nLooking at the plot, we can see that there are a few very large edits (both deletions and additions), likely consisting of large sections added and subtracted from the page. If we want to visualize when these large changes occurred, it would be useful to include a more granular set of labels on the x-axis. We can do this using scale_x_datetime() with custom formatting:\n\n(\n    page_revisions\n    .filter(c.doc_id.is_in(selected_authors))\n    .pipe(ggplot, aes(\"datetime\", \"size\", color=\"doc_id\"))\n    + geom_line()\n    + scale_color_manual(values=[\"red\", \"blue\"])\n    + scale_x_datetime(date_breaks=\"6 months\", date_labels=\"%b %Y\")\n    + theme(axis_text_x=element_text(angle=90, hjust=1))\n    + labs(\n        title=\"Wikipedia Page Size Over Time (Custom Labels)\",\n        x=\"Date\",\n        y=\"Page Size (bytes)\",\n        color=\"Author\"\n    )\n)\n\n\n\n\n\n\n\n\nWe can also filter our dataset by a particular range of dates or times. This is useful to zoom into a specific region of our data to investigate patterns that may be otherwise lost. For example, if we wanted to see all of the page sizes for two authors from 2021 onward:\n\n(\n    page_revisions\n    .filter(c.doc_id.is_in(selected_authors))\n    .filter(c.datetime &gt; pl.datetime(2021, 1, 1, time_zone=\"UTC\"))\n)\n\n\nshape: (506, 5)\n\n\n\ndoc_id\nuser\ndatetime\nsize\ncomment\n\n\nstr\nstr\ndatetime[μs, UTC]\ni64\nstr\n\n\n\n\n\"Geoffrey Chaucer\"\n\"86.31.15.58\"\n2021-01-18 09:42:33 UTC\n68291\n\"/* Origin */\"\n\n\n\"Geoffrey Chaucer\"\n\"86.31.15.58\"\n2021-01-18 09:43:16 UTC\n68288\n\"/* Taco Bell */\"\n\n\n\"Geoffrey Chaucer\"\n\"88.108.207.22\"\n2021-01-18 11:15:33 UTC\n68284\n\"/* Career */\"\n\n\n\"Geoffrey Chaucer\"\n\"Pahunkat\"\n2021-01-18 11:16:09 UTC\n68288\n\"Rollback edit(s) by [[Special:…\n\n\n\"Geoffrey Chaucer\"\n\"88.108.207.22\"\n2021-01-18 11:16:40 UTC\n68293\n\"/* Origin */\"\n\n\n…\n…\n…\n…\n…\n\n\n\"Emily Brontë\"\n\"HeyElliott\"\n2023-09-16 20:05:37 UTC\n41598\n\"[[WP:LQ]]\"\n\n\n\"Emily Brontë\"\n\"Qwerfjkl (bot)\"\n2023-09-21 16:34:46 UTC\n41586\n\"Converting Gutenberg author ID…\n\n\n\"Emily Brontë\"\n\"Ficaia\"\n2023-09-27 12:47:22 UTC\n41717\nnull\n\n\n\"Emily Brontë\"\n\"Keith D\"\n2023-09-27 20:36:18 UTC\n41688\n\"Remove namespace | Replaced cu…\n\n\n\"Emily Brontë\"\n\"Keith D\"\n2023-09-27 20:39:51 UTC\n41693\n\"/* Adulthood */ Cite fix\"\n\n\n\n\n\n\nNotice that the filter includes data from 2021, even though we use a strictly greater than condition. The reason for this is that pl.datetime(2021, 1, 1) is interpreted as the exact time corresponding to 1 January 2021 at 00:00. Any record that comes at any other time during the year of 2021 will be included in the filter.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "17_temporal_data.html#language-and-time-zones",
    "href": "17_temporal_data.html#language-and-time-zones",
    "title": "17  Temporal Data",
    "section": "17.6 Language and Time Zones",
    "text": "17.6 Language and Time Zones\nSo far, we have primarily worked with numeric summaries of the date and datetime objects. In the previous sections, notice that our example of working with the days of the week used numeric values (1-7) rather than names. Polars provides methods to extract string names for weekdays and months, though the output depends on your system locale. Below, for example, we extract various datetime components:\n\n(\n    page_revisions\n    .head(10)\n    .with_columns(\n        weekday_num = c.datetime.dt.weekday(),\n        month_num = c.datetime.dt.month(),\n        hour = c.datetime.dt.hour(),\n        date_only = c.datetime.dt.date()\n    )\n    .select(c.datetime, c.weekday_num, c.month_num, c.hour, c.date_only)\n)\n\n\nshape: (10, 5)\n\n\n\ndatetime\nweekday_num\nmonth_num\nhour\ndate_only\n\n\ndatetime[μs, UTC]\ni8\ni8\ni8\ndate\n\n\n\n\n2006-07-06 23:24:54 UTC\n4\n7\n23\n2006-07-06\n\n\n2006-07-24 18:39:08 UTC\n1\n7\n18\n2006-07-24\n\n\n2006-09-10 13:36:44 UTC\n7\n9\n13\n2006-09-10\n\n\n2006-09-18 04:59:53 UTC\n1\n9\n4\n2006-09-18\n\n\n2006-09-23 22:27:20 UTC\n6\n9\n22\n2006-09-23\n\n\n2006-09-23 22:31:03 UTC\n6\n9\n22\n2006-09-23\n\n\n2006-10-03 18:18:09 UTC\n2\n10\n18\n2006-10-03\n\n\n2006-10-03 18:19:32 UTC\n2\n10\n18\n2006-10-03\n\n\n2006-10-10 18:48:26 UTC\n2\n10\n18\n2006-10-10\n\n\n2006-10-11 05:29:32 UTC\n3\n10\n5\n2006-10-11\n\n\n\n\n\n\n\n\n\n\n\n\nWeekday and month names\n\n\n\n\n\nIf you need weekday or month names as strings, you can create a mapping from the numeric values. For example, you could use pl.when() chains or join with a lookup table. The numeric representation is often more convenient for analysis and avoids locale-dependent issues.\n\n\n\nAnother regional issue that arises when working with dates and times are time zones. While seemingly not too difficult a concept, getting time zones to work correctly with complex datasets can be incredibly complicated. A wide range of programming bugs have been attributed to all sorts of edge-cases surrounding the processing of time zones.\nAll times stored in Polars can be timezone-aware or timezone-naive. By default, times are stored as timezone-naive (no timezone information). The data can be localized to a specific timezone using the .dt.replace_time_zone() method, and converted between time zones using .dt.convert_time_zone(). All of the times recorded in the dataset page_revisions are given in UTC. This is not surprising; most technical sources with a global focus will use this convention.\nWe can convert between time zones using Polars datetime methods. Since we parsed our data with time_zone=\"UTC\", the datetime column is already timezone-aware. We can convert to another timezone using .dt.convert_time_zone(). Let’s convert our UTC times to New York time:\n\n(\n    page_revisions\n    .head(10)\n    .with_columns(\n        datetime_utc = c.datetime,\n        datetime_nyc = c.datetime.dt.convert_time_zone(\"America/New_York\")\n    )\n    .with_columns(\n        hour_utc = c.datetime_utc.dt.hour(),\n        hour_nyc = c.datetime_nyc.dt.hour()\n    )\n    .select(c.datetime_utc, c.datetime_nyc, c.hour_utc, c.hour_nyc)\n)\n\n\nshape: (10, 4)\n\n\n\ndatetime_utc\ndatetime_nyc\nhour_utc\nhour_nyc\n\n\ndatetime[μs, UTC]\ndatetime[μs, America/New_York]\ni8\ni8\n\n\n\n\n2006-07-06 23:24:54 UTC\n2006-07-06 19:24:54 EDT\n23\n19\n\n\n2006-07-24 18:39:08 UTC\n2006-07-24 14:39:08 EDT\n18\n14\n\n\n2006-09-10 13:36:44 UTC\n2006-09-10 09:36:44 EDT\n13\n9\n\n\n2006-09-18 04:59:53 UTC\n2006-09-18 00:59:53 EDT\n4\n0\n\n\n2006-09-23 22:27:20 UTC\n2006-09-23 18:27:20 EDT\n22\n18\n\n\n2006-09-23 22:31:03 UTC\n2006-09-23 18:31:03 EDT\n22\n18\n\n\n2006-10-03 18:18:09 UTC\n2006-10-03 14:18:09 EDT\n18\n14\n\n\n2006-10-03 18:19:32 UTC\n2006-10-03 14:19:32 EDT\n18\n14\n\n\n2006-10-10 18:48:26 UTC\n2006-10-10 14:48:26 EDT\n18\n14\n\n\n2006-10-11 05:29:32 UTC\n2006-10-11 01:29:32 EDT\n5\n1\n\n\n\n\n\n\nWe can use the timezone information to display data in a useful way to a local audience. For example, the code below displays the frequency of updates as a function of the hour of the day in New York City:\n\nhourly_edits = (\n    page_revisions\n    .with_columns(\n        datetime_nyc = c.datetime.dt.convert_time_zone(\"America/New_York\")\n    )\n    .with_columns(\n        hour_nyc = c.datetime_nyc.dt.hour()\n    )\n    .group_by(c.hour_nyc)\n    .agg(\n        count = pl.len()\n    )\n    .sort(c.hour_nyc)\n)\n\n(\n    hourly_edits\n    .pipe(ggplot, aes(\"hour_nyc\", \"count\"))\n    + geom_col()\n    + labs(\n        title=\"Wikipedia Edits by Hour (New York Time)\",\n        x=\"Hour of Day\",\n        y=\"Number of Edits\"\n    )\n)\n\n\n\n\n\n\n\n\nWhile certainly many editors are living in other English-speaking cities (London, Los Angeles, or Mumbai), it is generally easier for people to do the mental math for what times correspond relative to their own time zone than relative to UTC.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "17_temporal_data.html#truncating-dates-data",
    "href": "17_temporal_data.html#truncating-dates-data",
    "title": "17  Temporal Data",
    "section": "17.7 Truncating Dates Data",
    "text": "17.7 Truncating Dates Data\nWe have shown above how to create date and datetime objects using Polars functions. Also, we have seen how to extract the components from date and datetime objects with the .dt namespace. There are a variety of other functions that help us create and manipulate these temporal objects. While we will not give an entire list of all the available functions in Polars datetime functionality, let’s look at a few of the most useful and representative examples for converting between different ways of representing information about time.\nThe page_revisions dataset has revisions recorded with the precision of a second. This is likely overly granular for many applications; it might be better to have the data in a format that is only at the level of an hour, for example. We can truncate any date or datetime object by using the .dt.truncate() method along with a specific duration. Setting the duration to “1h” (one hour), for example, will remove all of the minutes and seconds from the time:\n\n(\n    page_revisions\n    .head(10)\n    .with_columns(\n        datetime_hour = c.datetime.dt.truncate(\"1h\"),\n        datetime_day = c.datetime.dt.truncate(\"1d\")\n    )\n    .select(c.datetime, c.datetime_hour, c.datetime_day)\n)\n\n\nshape: (10, 3)\n\n\n\ndatetime\ndatetime_hour\ndatetime_day\n\n\ndatetime[μs, UTC]\ndatetime[μs, UTC]\ndatetime[μs, UTC]\n\n\n\n\n2006-07-06 23:24:54 UTC\n2006-07-06 23:00:00 UTC\n2006-07-06 00:00:00 UTC\n\n\n2006-07-24 18:39:08 UTC\n2006-07-24 18:00:00 UTC\n2006-07-24 00:00:00 UTC\n\n\n2006-09-10 13:36:44 UTC\n2006-09-10 13:00:00 UTC\n2006-09-10 00:00:00 UTC\n\n\n2006-09-18 04:59:53 UTC\n2006-09-18 04:00:00 UTC\n2006-09-18 00:00:00 UTC\n\n\n2006-09-23 22:27:20 UTC\n2006-09-23 22:00:00 UTC\n2006-09-23 00:00:00 UTC\n\n\n2006-09-23 22:31:03 UTC\n2006-09-23 22:00:00 UTC\n2006-09-23 00:00:00 UTC\n\n\n2006-10-03 18:18:09 UTC\n2006-10-03 18:00:00 UTC\n2006-10-03 00:00:00 UTC\n\n\n2006-10-03 18:19:32 UTC\n2006-10-03 18:00:00 UTC\n2006-10-03 00:00:00 UTC\n\n\n2006-10-10 18:48:26 UTC\n2006-10-10 18:00:00 UTC\n2006-10-10 00:00:00 UTC\n\n\n2006-10-11 05:29:32 UTC\n2006-10-11 05:00:00 UTC\n2006-10-11 00:00:00 UTC\n\n\n\n\n\n\nThe benefit of using the .dt.truncate() method is that we could then group, join, or summarize the data in a way that treats each value of datetime the same as long as they occur during the same hour. There is also a .dt.round() method for rounding the datetime object to the nearest desired unit. In the special case in which we want to extract just the date part of a datetime, we can use the .dt.date() method. The code below illustrates this process, as well as showing how reducing the temporal granularity can be a useful first step before grouping and summarizing:\n\n(\n    page_revisions\n    .with_columns(\n        date = c.datetime.dt.date()\n    )\n    .group_by(c.date)\n    .agg(\n        count = pl.len()\n    )\n    .sort(c.date, descending=True)\n)\n\n\nshape: (4_831, 2)\n\n\n\ndate\ncount\n\n\ndate\nu32\n\n\n\n\n2023-09-30\n10\n\n\n2023-09-29\n11\n\n\n2023-09-28\n9\n\n\n2023-09-27\n14\n\n\n2023-09-26\n12\n\n\n…\n…\n\n\n2003-02-01\n1\n\n\n2003-01-29\n2\n\n\n2003-01-14\n1\n\n\n2003-01-04\n1\n\n\n2002-10-26\n1\n\n\n\n\n\n\nAbove, we effectively remove the time component of the datetime object and treat the variable as having only a date element. Occasionally, we might want to do the opposite: considering only the time component of a datetime object without worrying about the specific date. For example, we might want to summarize the number of edits that are made based on the time of the day. We can do this by extracting the hour component:\n\n(\n    page_revisions\n    .with_columns(\n        hour = c.datetime.dt.hour()\n    )\n    .group_by(c.hour)\n    .agg(\n        count = pl.len()\n    )\n    .sort(c.hour)\n)\n\n\nshape: (24, 2)\n\n\n\nhour\ncount\n\n\ni8\nu32\n\n\n\n\n0\n1316\n\n\n1\n1229\n\n\n2\n1149\n\n\n3\n1010\n\n\n4\n982\n\n\n…\n…\n\n\n19\n1837\n\n\n20\n1838\n\n\n21\n1797\n\n\n22\n1675\n\n\n23\n1349\n\n\n\n\n\n\nThis creates a variable that stores the hour without any date information, which is useful for analyzing patterns that repeat daily.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "17_temporal_data.html#window-functions",
    "href": "17_temporal_data.html#window-functions",
    "title": "17  Temporal Data",
    "section": "17.8 Window Functions",
    "text": "17.8 Window Functions\nAt the start of this chapter, we considered time series to be a sequence of events without too much focus on the specific dates and times. This viewpoint can be a useful construct when we want to look at changes over time. For example, we have the overall size of each Wikipedia page after an edit. A measurement that would be useful is the difference in page size made by an edit. To add a variable to a dataset, we usually use the .with_columns() method, and that will again work here. However, in this case we need to reference values that come before or after a certain value. This requires the use of window functions.\nA window function transforms a variable in a dataset into a new variable with the same length in a way that takes into account the entire ordering of the data. Two examples of window functions that are useful when working with time series data are .shift() with positive and negative values, which give access to rows preceding or following a row, respectively. Let’s apply this to our dataset of page revisions to get the previous and next values of the page size variable.\n\n(\n    page_revisions\n    .with_columns(\n        size_last = c.size.shift(1),\n        size_next = c.size.shift(-1)\n    )\n    .select(c.doc_id, c.datetime, c.size, c.size_last, c.size_next)\n)\n\n\nshape: (35_470, 5)\n\n\n\ndoc_id\ndatetime\nsize\nsize_last\nsize_next\n\n\nstr\ndatetime[μs, UTC]\ni64\ni64\ni64\n\n\n\n\n\"Marie de France\"\n2006-07-06 23:24:54 UTC\n3061\nnull\n3085\n\n\n\"Marie de France\"\n2006-07-24 18:39:08 UTC\n3085\n3061\n3085\n\n\n\"Marie de France\"\n2006-09-10 13:36:44 UTC\n3085\n3085\n3105\n\n\n\"Marie de France\"\n2006-09-18 04:59:53 UTC\n3105\n3085\n3104\n\n\n\"Marie de France\"\n2006-09-23 22:27:20 UTC\n3104\n3105\n3132\n\n\n…\n…\n…\n…\n…\n\n\n\"Seamus Heaney\"\n2023-09-21 04:51:45 UTC\n85491\n85341\n85493\n\n\n\"Seamus Heaney\"\n2023-09-24 12:10:49 UTC\n85493\n85491\n85482\n\n\n\"Seamus Heaney\"\n2023-09-25 16:06:01 UTC\n85482\n85493\n85493\n\n\n\"Seamus Heaney\"\n2023-09-25 16:06:11 UTC\n85493\n85482\n85493\n\n\n\"Seamus Heaney\"\n2023-09-27 02:39:46 UTC\n85493\n85493\nnull\n\n\n\n\n\n\nNotice that the first value of size_last is missing because there is no last value for the first item in our data. Similarly, the variable size_next will have a missing value at the end of the dataset. As written above, the code incorrectly crosses the time points at the boundary of each page. That is, for the first row of the second page (Geoffrey Chaucer) it thinks that the size of the last page is the size of the final page of the Marie de France record. To fix this, we can use the .over() method to apply the window function within groups. Window functions respect the grouping of the data:\n\n(\n    page_revisions\n    .sort(c.doc_id, c.datetime)\n    .with_columns(\n        size_last = c.size.shift(1).over(c.doc_id),\n        size_next = c.size.shift(-1).over(c.doc_id)\n    )\n    .select(c.doc_id, c.datetime, c.size, c.size_last, c.size_next)\n    .slice(495, 10)\n)\n\n\nshape: (10, 5)\n\n\n\ndoc_id\ndatetime\nsize\nsize_last\nsize_next\n\n\nstr\ndatetime[μs, UTC]\ni64\ni64\ni64\n\n\n\n\n\"A. A. Milne\"\n2023-08-05 20:59:54 UTC\n44019\n43990\n44038\n\n\n\"A. A. Milne\"\n2023-08-18 09:26:21 UTC\n44038\n44019\n44044\n\n\n\"A. A. Milne\"\n2023-08-21 22:05:31 UTC\n44044\n44038\n44045\n\n\n\"A. A. Milne\"\n2023-08-21 22:07:46 UTC\n44045\n44044\n44077\n\n\n\"A. A. Milne\"\n2023-08-31 20:41:54 UTC\n44077\n44045\nnull\n\n\n\"Alexander Pope\"\n2016-04-17 04:03:45 UTC\n31345\nnull\n31531\n\n\n\"Alexander Pope\"\n2016-05-05 22:17:54 UTC\n31531\n31345\n31532\n\n\n\"Alexander Pope\"\n2016-05-05 22:24:29 UTC\n31532\n31531\n31585\n\n\n\"Alexander Pope\"\n2016-05-07 19:05:49 UTC\n31585\n31532\n31614\n\n\n\"Alexander Pope\"\n2016-05-09 17:22:21 UTC\n31614\n31585\n31585\n\n\n\n\n\n\nNotice that now, correctly, the dataset has a missing size_next for the final Marie de France record and a missing size_last for the first Geoffrey Chaucer record. Now, let’s use this to compute the change in the page sizes for each of the revisions:\n\n(\n    page_revisions\n    .sort(c.doc_id, c.datetime)\n    .with_columns(\n        size_diff = c.size - c.size.shift(1).over(c.doc_id)\n    )\n    .select(c.doc_id, c.datetime, c.size, c.size_diff)\n)\n\n\nshape: (35_470, 4)\n\n\n\ndoc_id\ndatetime\nsize\nsize_diff\n\n\nstr\ndatetime[μs, UTC]\ni64\ni64\n\n\n\n\n\"A. A. Milne\"\n2015-09-04 16:30:26 UTC\n30702\nnull\n\n\n\"A. A. Milne\"\n2015-09-22 14:51:08 UTC\n30400\n-302\n\n\n\"A. A. Milne\"\n2015-11-07 10:23:31 UTC\n30389\n-11\n\n\n\"A. A. Milne\"\n2015-11-08 00:48:23 UTC\n30397\n8\n\n\n\"A. A. Milne\"\n2015-11-16 03:12:19 UTC\n30427\n30\n\n\n…\n…\n…\n…\n\n\n\"William Wordsworth\"\n2023-09-25 11:19:40 UTC\n41466\n0\n\n\n\"William Wordsworth\"\n2023-09-27 15:10:27 UTC\n41464\n-2\n\n\n\"William Wordsworth\"\n2023-09-27 15:11:44 UTC\n41466\n2\n\n\n\"William Wordsworth\"\n2023-09-27 15:13:48 UTC\n41467\n1\n\n\n\"William Wordsworth\"\n2023-09-27 15:18:10 UTC\n41466\n-1\n\n\n\n\n\n\nIn the above output, we can see the changes in page sizes. If we wanted to find reversions in the dataset, we could apply the .shift() function several times. As an alternative, we can also give a parameter to .shift() to indicate that we want to go back (or forward) more than one row. Let’s put this together to indicate which commits seem to be a reversion (the page size exactly matches the page size from two commits prior) as well as the overall size of the reversion:\n\n(\n    page_revisions\n    .sort(c.doc_id, c.datetime)\n    .with_columns(\n        size_diff = c.size - c.size.shift(1).over(c.doc_id),\n        size_two_back = c.size.shift(2).over(c.doc_id),\n        is_reversion = c.size == c.size.shift(2).over(c.doc_id)\n    )\n    .filter(c.is_reversion)\n    .select(c.doc_id, c.datetime, c.size_diff, c.is_reversion)\n)\n\n\nshape: (4_927, 4)\n\n\n\ndoc_id\ndatetime\nsize_diff\nis_reversion\n\n\nstr\ndatetime[μs, UTC]\ni64\nbool\n\n\n\n\n\"A. A. Milne\"\n2015-12-17 18:13:11 UTC\n-6\ntrue\n\n\n\"A. A. Milne\"\n2016-01-07 18:55:26 UTC\n30407\ntrue\n\n\n\"A. A. Milne\"\n2016-04-14 17:38:45 UTC\n36\ntrue\n\n\n\"A. A. Milne\"\n2016-04-17 15:06:58 UTC\n-77\ntrue\n\n\n\"A. A. Milne\"\n2016-04-27 20:04:19 UTC\n-1\ntrue\n\n\n…\n…\n…\n…\n\n\n\"William Wordsworth\"\n2023-09-18 06:35:43 UTC\n-1\ntrue\n\n\n\"William Wordsworth\"\n2023-09-24 14:54:33 UTC\n8\ntrue\n\n\n\"William Wordsworth\"\n2023-09-25 11:19:40 UTC\n0\ntrue\n\n\n\"William Wordsworth\"\n2023-09-27 15:11:44 UTC\n2\ntrue\n\n\n\"William Wordsworth\"\n2023-09-27 15:18:10 UTC\n-1\ntrue\n\n\n\n\n\n\nThese reversions can be studied to see the nature of the Wikipedia editing process. For example, how long do these reversions tend to take? Are certain pages more likely to undergo reversions? Do these take place during a certain time of the day? These are all questions that we should now be able to address using this dataset and the tools described above.\n\n\n\n\n\n\nAdditional window functions\n\n\n\n\n\nPolars provides many other window functions beyond .shift(). Common examples include .cum_sum() for cumulative sums, .rolling_mean() for moving averages, .rank() for ranking values, and .diff() which computes the difference between consecutive values (equivalent to x - x.shift(1)). All of these can be combined with .over() to apply within groups.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "17_temporal_data.html#range-joins",
    "href": "17_temporal_data.html#range-joins",
    "title": "17  Temporal Data",
    "section": "17.9 Range Joins",
    "text": "17.9 Range Joins\nWe will finish this chapter by looking at range joins, which allow for combining datasets based on inequalities between keys contained in two different datasets. Range joins are functionality that can greatly simplify some operations that arise when working with temporal data. Recall that all of the join functions that we saw in Chapter 4 work by finding a correspondence where keys from one dataset equal the values of keys in another dataset. In some cases it happens that we want to join two tables on inequalities rather than exact values.\nTake, for example, the metadata table for the 75 authors in our Wikipedia collection. Recall that this dataset contains the years that each author was born and the years each author died. What if we wanted to make a dataset by joining the metadata to itself, matching each author with other authors that would have been alive in overlapping years? We can do this using Polars’ .join_where() method:\n\n(\n    meta\n    .join_where(\n        meta,\n        (c.born &lt;= c.born_right) & (c.died &gt; c.born_right) & (c.doc_id != c.doc_id_right)\n    )\n    .select(c.doc_id, c.doc_id_right, c.born, c.died, c.born_right, c.died_right)\n)\n\n\nshape: (693, 6)\n\n\n\ndoc_id\ndoc_id_right\nborn\ndied\nborn_right\ndied_right\n\n\nstr\nstr\ni64\ni64\ni64\ni64\n\n\n\n\n\"William Langland\"\n\"Geoffrey Chaucer\"\n1332\n1386\n1343\n1400\n\n\n\"William Langland\"\n\"Margery Kempe\"\n1332\n1386\n1373\n1438\n\n\n\"Geoffrey Chaucer\"\n\"Margery Kempe\"\n1343\n1400\n1373\n1438\n\n\n\"John Gower\"\n\"William Langland\"\n1330\n1408\n1332\n1386\n\n\n\"John Gower\"\n\"Geoffrey Chaucer\"\n1330\n1408\n1343\n1400\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Edward Upward\"\n\"Daphne du Maurier\"\n1903\n2009\n1907\n1989\n\n\n\"Edward Upward\"\n\"W. H. Auden\"\n1903\n2009\n1907\n1973\n\n\n\"Edward Upward\"\n\"Louis MacNeice\"\n1903\n2009\n1907\n1963\n\n\n\"Edward Upward\"\n\"Stephen Spender\"\n1903\n2009\n1909\n1995\n\n\n\"Edward Upward\"\n\"Seamus Heaney\"\n1903\n2009\n1939\n1939\n\n\n\n\n\n\nThe .join_where() method takes one or more expressions to specify which pairs of rows to keep in the joined dataset. To refer to columns in the second (right) dataset that share the same name, use the _right suffix. In the example above, we find all pairs where the first author’s lifespan overlaps with the second author’s birth, excluding self-matches.\nThe resulting dataset would make an interesting type of network, showing temporal overlap of authors in the dataset. We will investigate this in the following chapter.\n\n\n\n\n\n\nPerformance of conditional joins\n\n\n\n\n\nThe .join_where() method can be slow because it needs to check every combination of rows in the two datasets. For large datasets, consider whether you can use .join_asof() instead, which is optimized for nearest-value matching and requires the data to be sorted. The .join_asof() method is particularly useful when you want to match each row to the nearest (or nearest preceding/following) value in another dataset.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "17_temporal_data.html#temporal-durations",
    "href": "17_temporal_data.html#temporal-durations",
    "title": "17  Temporal Data",
    "section": "17.10 Temporal Durations",
    "text": "17.10 Temporal Durations\nSometimes we want to work not with specific dates or times, but with durations—the amount of time between two events. Polars represents durations with the Duration data type. We can compute durations by subtracting one datetime from another, or by using the pl.duration() function to create specific intervals.\n\n(\n    page_revisions\n    .sort(c.doc_id, c.datetime)\n    .with_columns(\n        time_since_last = c.datetime - c.datetime.shift(1).over(c.doc_id)\n    )\n    .select(c.doc_id, c.datetime, c.time_since_last)\n    .drop_nulls()\n)\n\n\nshape: (35_395, 3)\n\n\n\ndoc_id\ndatetime\ntime_since_last\n\n\nstr\ndatetime[μs, UTC]\nduration[μs]\n\n\n\n\n\"A. A. Milne\"\n2015-09-22 14:51:08 UTC\n17d 22h 20m 42s\n\n\n\"A. A. Milne\"\n2015-11-07 10:23:31 UTC\n45d 19h 32m 23s\n\n\n\"A. A. Milne\"\n2015-11-08 00:48:23 UTC\n14h 24m 52s\n\n\n\"A. A. Milne\"\n2015-11-16 03:12:19 UTC\n8d 2h 23m 56s\n\n\n\"A. A. Milne\"\n2015-11-17 07:52:16 UTC\n1d 4h 39m 57s\n\n\n…\n…\n…\n\n\n\"William Wordsworth\"\n2023-09-25 11:19:40 UTC\n1h 17m\n\n\n\"William Wordsworth\"\n2023-09-27 15:10:27 UTC\n2d 3h 50m 47s\n\n\n\"William Wordsworth\"\n2023-09-27 15:11:44 UTC\n1m 17s\n\n\n\"William Wordsworth\"\n2023-09-27 15:13:48 UTC\n2m 4s\n\n\n\"William Wordsworth\"\n2023-09-27 15:18:10 UTC\n4m 22s\n\n\n\n\n\n\nThe resulting duration can be converted to various units using methods like .dt.total_days(), .dt.total_hours(), .dt.total_minutes(), or .dt.total_seconds(). This is useful for analyzing how frequently events occur or how long processes take.\n\n(\n    page_revisions\n    .sort(c.doc_id, c.datetime)\n    .with_columns(\n        time_since_last = c.datetime - c.datetime.shift(1).over(c.doc_id)\n    )\n    .with_columns(\n        days_since_last = c.time_since_last.dt.total_days()\n    )\n    .select(c.doc_id, c.datetime, c.days_since_last)\n    .drop_nulls()\n    .group_by(c.doc_id)\n    .agg(\n        avg_days_between_edits = c.days_since_last.mean()\n    )\n    .sort(c.avg_days_between_edits)\n)\n\n\nshape: (75, 2)\n\n\n\ndoc_id\navg_days_between_edits\n\n\nstr\nf64\n\n\n\n\n\"George Orwell\"\n1.138277\n\n\n\"James Joyce\"\n1.224449\n\n\n\"Oscar Wilde\"\n1.58517\n\n\n\"Thomas More\"\n1.791583\n\n\n\"Percy Bysshe Shelley\"\n1.815631\n\n\n…\n…\n\n\n\"John Gower\"\n24.467105\n\n\n\"Edward Upward\"\n27.0\n\n\n\"Katherine Philipps\"\n31.182692\n\n\n\"Rex Warner\"\n39.488506\n\n\n\"Charlotte Smith\"\n168.222222\n\n\n\n\n\n\nThis shows us which Wikipedia pages are edited most frequently (smallest average days between edits) and which are edited less often.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "17_temporal_data.html#summary",
    "href": "17_temporal_data.html#summary",
    "title": "17  Temporal Data",
    "section": "17.11 Summary",
    "text": "17.11 Summary\nIn this chapter, we explored techniques for working with temporal data in Polars. We learned how to create date and datetime objects from their components, extract useful information from them, and visualize temporal patterns. We saw how to work with time zones and how to truncate temporal data to different levels of granularity. Window functions allowed us to compare values across time within groups, and range joins provided a way to combine datasets based on temporal overlap. These tools are essential for any analysis involving time-based data.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "17_temporal_data.html#references",
    "href": "17_temporal_data.html#references",
    "title": "17  Temporal Data",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "18_network_data.html",
    "href": "18_network_data.html",
    "title": "18  Network Data",
    "section": "",
    "text": "18.1 Setup\nLoad all of the modules and datasets needed for the chapter.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\npage_citation = pl.read_csv(\"data/wiki_uk_citations.csv\")\npage_cocitation = pl.read_csv(\"data/wiki_uk_cocitations.csv\")",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "18_network_data.html#introduction",
    "href": "18_network_data.html#introduction",
    "title": "18  Network Data",
    "section": "18.2 Introduction",
    "text": "18.2 Introduction\nA network consists of a set of objects and a collection of links identifying relationships between pairs of these objects [1]. Networks form a very generic data model with extensive applications across the humanities and social sciences [2]. They can be a powerful way to understand connections and relationships between a wide range of entities. For example, one might want to explore friendship relationships between people, citations between books, or connections between computers. Whenever there exists a set of relationships that connects objects to each other, a network can be a useful tool for visualization and data exploration.\nA critical step in interpreting network data is deciding exactly what elements correspond to the nodes (the objects) and the edges (links between the nodes). For example, consider a study of citations. A node might be an author and an edge may connect two authors when one cites the other. The SignsAt40 project is a great example: they examine 40 years of feminist scholarship through network and text analysis, including co-citation networks. Being very clear about how nodes and edges are defined is key to exploring networks.\nIn this chapter, we start by working with network data taken from Wikipedia using the same set of pages we saw in the previous chapter. Wikipedia pages contain many internal links between pages; we collected information about each time one of the pages in our dataset provided a link to another page in our collection. Only links within the main body of the text were used.\nThe DSNetwork.process function is a helper that takes an edge list and returns node and edge DataFrames suitable for visualization, along with network metrics. We will use it throughout this chapter.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "18_network_data.html#creating-a-network-object",
    "href": "18_network_data.html#creating-a-network-object",
    "title": "18  Network Data",
    "section": "18.3 Creating a Network Object",
    "text": "18.3 Creating a Network Object\nWe can describe a network structure with a tabular dataset. Specifically, we can create a dataset with one row for each edge in the network. This dataset needs one column giving an identifier for the starting node of the edge and another column giving the ending node. The set of links between the Wikipedia pages is read into Python and displayed by the following block of code. Notice that we are using the same values in the doc_id column that were used as unique identifiers for each page text in Chapter 19.\n\npage_citation\n\n\nshape: (377, 2)\n\n\n\ndoc_id\ndoc_id2\n\n\nstr\nstr\n\n\n\n\n\"Marie de France\"\n\"Geoffrey Chaucer\"\n\n\n\"Geoffrey Chaucer\"\n\"William Langland\"\n\n\n\"Geoffrey Chaucer\"\n\"John Gower\"\n\n\n\"Geoffrey Chaucer\"\n\"John Dryden\"\n\n\n\"Geoffrey Chaucer\"\n\"Philip Sidney\"\n\n\n…\n…\n\n\n\"Rex Warner\"\n\"Stephen Spender\"\n\n\n\"Seamus Heaney\"\n\"W. B. Yeats\"\n\n\n\"Seamus Heaney\"\n\"George Bernard Shaw\"\n\n\n\"Seamus Heaney\"\n\"Samuel Beckett\"\n\n\n\"Seamus Heaney\"\n\"Mary Robinson\"\n\n\n\n\n\n\nLooking at the first few rows, we see that Marie de France has only one link (to Geoffrey Chaucer). Chaucer, on the other hand, has links to six other authors. As a starting way to analyze the data, we can see how many pages link into each author’s page. Arranging the data by the count will give a quick understanding of how central each author’s page is to the other authors.\n\n(\n    page_citation\n    .group_by(c.doc_id2)\n    .agg(count = pl.len())\n    .sort(c.count, descending=True)\n)\n\n\nshape: (64, 2)\n\n\n\ndoc_id2\ncount\n\n\nstr\nu32\n\n\n\n\n\"William Shakespeare\"\n25\n\n\n\"William Wordsworth\"\n23\n\n\n\"T. S. Eliot\"\n19\n\n\n\"W. H. Auden\"\n13\n\n\n\"Edmund Spenser\"\n12\n\n\n…\n…\n\n\n\"Beatrix Potter\"\n1\n\n\n\"Samuel Pepys\"\n1\n\n\n\"Margaret Cavendish\"\n1\n\n\n\"George Herbert\"\n1\n\n\n\"Bram Stoker\"\n1\n\n\n\n\n\n\nBy summarizing links into each page rather than out of each page, we avoid a direct bias towards focusing on longer author pages.\nLooking at the counts, we see that there are more links into the Shakespeare page than any other in our collection. While that is perhaps not surprising, it is interesting to see that Wordsworth is only two links short of Shakespeare, with 23 links. While raw counts are a useful starting point, they can only get us so far. These say nothing, for example, about how easily we can hop between two pages by following 2, 3, or 4 links. In order to understand the dataset we need a way of visualizing and modelling all of the connections at once. This requires considering the entire network structure of our dataset.\nBefore we create a network structure from a dataset, we need to decide on what kind of network we will create. Specifically, networks can be either directed, in which case we distinguish between the starting and ending vertex, or undirected, in which we do not. For example, in our counts above, we took the direction into account for the links. Next, let’s start by treating our dataset of Wikipedia page links as undirected; all we want to consider is whether there is at least one way to click on a link and go between the two pages. Later in the chapter, we will show what changes when we add direction into the data. Any directed network can be considered undirected by ignoring the direction; undirected networks cannot in general be converted into a directed format. So, it will be a good starting point to consider approaches that can be applied to any network dataset.\nThe dataset that we have in Python is called an edge list [3]. It consists of a dataset where each observation is an edge. We can use the igraph library to create network objects and compute various network metrics [4]. Let’s create our network using our wrapper method DSNetwork.process.\n\nnode, edge, G = DSNetwork.process(page_citation, directed=False)\nnode\n\n\nshape: (72, 10)\n\n\n\nid\nx\ny\ncomponent\ncomponent_size\ncluster\ndegree\neigen\nbetween\nclose\n\n\nstr\nf64\nf64\ni64\ni64\nstr\ni64\nf64\nf64\nf64\n\n\n\n\n\"Marie de France\"\n5.538941\n-1.383726\n1\n72\n\"0\"\n1\n0.024402\n0.0\n0.324201\n\n\n\"Geoffrey Chaucer\"\n2.191939\n-0.511004\n1\n72\n\"1\"\n16\n0.409418\n147.136045\n0.47651\n\n\n\"William Langland\"\n2.132365\n-2.707444\n1\n72\n\"1\"\n3\n0.062166\n4.723756\n0.375661\n\n\n\"John Gower\"\n3.144224\n-1.356045\n1\n72\n\"1\"\n5\n0.11872\n7.74378\n0.408046\n\n\n\"John Dryden\"\n0.811461\n1.245217\n1\n72\n\"1\"\n25\n0.730764\n197.454422\n0.5546875\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Cecil Day-Lewis\"\n-2.056276\n-2.619206\n1\n72\n\"6\"\n11\n0.209379\n20.181833\n0.415205\n\n\n\"Christopher Isherwood\"\n-1.894226\n-3.480311\n1\n72\n\"6\"\n9\n0.149254\n0.893551\n0.364103\n\n\n\"Louis MacNeice\"\n-0.767073\n-2.379231\n1\n72\n\"6\"\n12\n0.259567\n26.553664\n0.435583\n\n\n\"Rex Warner\"\n-2.52617\n-4.050665\n1\n72\n\"6\"\n4\n0.072261\n0.0\n0.356784\n\n\n\"Edward Upward\"\n-2.802774\n-3.583059\n1\n72\n\"6\"\n6\n0.094188\n0.0\n0.358586\n\n\n\n\n\n\nThe node dataset contains extracted information about each of the objects in our collection. We will describe each of these throughout the remainder of this chapter. Note that we also have metadata about the nodes, which is something that we can join into the data to help deepen our understanding of subsequent analyses.\nThe first column gives a label for the row. In the next two columns, named x and y, is a computed way to layout the objects in two-dimensions that maximizes linked pages being close to one another while minimizing the amount that all of the nodes are bunched together [5]. This is an example of a network drawing (also known as graph drawing in mathematics) algorithm. As with the PCA and UMAP dimensions in Chapter 19, there is no exact meaning of the individual variables. Rather, its the relationships that they show that are interesting. Using the first three variables, we could plot the pages as a scatter plot with labels to see what pages appear to be closely related to one another.\nBefore actually looking at this plot, it will be useful to first make some additions. The relationships that would be shown in this plot generally try to put pages that have links between them close to one another. It would be helpful to additionally put these links onto the plot as well. This is where the edge dataset becomes useful. The edge dataset contains one row for each edge in the dataset. The dataset has four columns. These describe the x and y values of one node in the edge and variables xend and yend to indicate where in the scatter plot the ending point of the edge is.\n\nedge\n\n\nshape: (377, 4)\n\n\n\nx\ny\nxend\nyend\n\n\nf64\nf64\nf64\nf64\n\n\n\n\n5.538941\n-1.383726\n2.191939\n-0.511004\n\n\n2.191939\n-0.511004\n2.132365\n-2.707444\n\n\n2.191939\n-0.511004\n3.144224\n-1.356045\n\n\n2.191939\n-0.511004\n0.811461\n1.245217\n\n\n2.191939\n-0.511004\n3.211966\n0.777386\n\n\n…\n…\n…\n…\n\n\n-1.619322\n-2.620255\n-2.52617\n-4.050665\n\n\n0.203618\n-0.898555\n-0.587649\n-2.958056\n\n\n0.11255\n-1.741517\n-0.587649\n-2.958056\n\n\n0.084148\n-3.884805\n-0.587649\n-2.958056\n\n\n-1.667334\n-0.683446\n-0.587649\n-2.958056\n\n\n\n\n\n\nWe can include edges into the plot by adding a geom layer of type geom_segment. This geometry takes four aesthetics, named exactly the same as the names in the edge dataset. The plot gets busy with all of these lines, so we will set the opacity (alpha) of them lower so as to not clutter the visual space with the connections.\n\n(\n    node\n    .pipe(ggplot, aes(\"x\", \"y\"))\n    + geom_segment(\n        aes(x=\"x\", y=\"y\", xend=\"xend\", yend=\"yend\"),\n        data=edge,\n        alpha=0.1\n    )\n    + geom_point(alpha=0.5)\n    + geom_text(aes(label=\"id\"), size=6)\n    + theme_void()\n    + labs(title=\"Wikipedia Page Link Network\")\n)\n\n\n\n\n\n\n\n\nThe output of the plot shows the underlying data that describes the plot as well as the relationships between the pages. Notice that the relationship between the pages is quite different than the textual-based ones in the previous chapter. When using textual distances, we saw a clustering based on the time period in which each author wrote and the formats that they wrote in. Here, the pattern is driven much more strongly by the general popularity of each author. The most well-known authors of each era—Shakespeare, Chaucer, Jonathan Swift—are clustered in the middle of the plot. Lesser known authors, such as Daphne du Maurier and Samuel Pepys, are pushed to the exterior. In the next section, we will see if we can more formally study the centrality of different pages in our collection.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "18_network_data.html#centrality",
    "href": "18_network_data.html#centrality",
    "title": "18  Network Data",
    "section": "18.4 Centrality",
    "text": "18.4 Centrality\nOne of the core questions that arises when working with network data is trying to identify the relative centrality of each node in the network. Several of the derived measurements in the node dataset capture various forms of centrality. Let’s move through each of these measurements to see how they reveal different aspects of our network’s centrality.\nA component of a network is a collection of all the nodes that can be reached by following along the edges. The node dataset contains a variable called component describing each of the components in the network. These are ordered in descending order of size, so component 1 will always be the largest (or at least, tied for the largest) component of the network. The total size of each component is the first measurement of the centrality of a node. Those nodes that are in the largest component can, in some sense, be said to have a larger centrality than other nodes that are completely cut-off from this cluster. All of the nodes on our network are contained in one large cluster, so this measurement is not particularly helpful in this specific case. Networks that have a single component are known as connected networks. All of the other metrics for centrality are defined in terms of a connected network. In order to apply them to networks with multiple components, each algorithm is applied separately to each component.\nAnother measurement of centrality is a node’s degree. The degree of a node is the number of neighbors it has. In other words, it counts how many edges the node is a part of. The degree of each node has been computed in the node table. This is similar to the counts that were produced in the first section by counting occurrences in the raw edge list. The difference here is that we are counting all edges, not just those edges going into a node. As a visualization technique, we can plot the degree centrality scores on a plot of the network to show that the nodes with the highest degree do seem to sit in the middle of the plot and correspond to having a large number of edges.\n\n(\n    node\n    .pipe(ggplot, aes(\"x\", \"y\"))\n    + geom_segment(\n        aes(x=\"x\", y=\"y\", xend=\"xend\", yend=\"yend\"),\n        data=edge,\n        alpha=0.1\n    )\n    + geom_point(aes(color=\"degree\"), size=3)\n    + scale_color_cmap(cmap_name=\"viridis\")\n    + theme_void()\n    + labs(\n        title=\"Network Colored by Degree Centrality\",\n        color=\"Degree\"\n    )\n)\n\n\n\n\n\n\n\n\nThe degree of a node only accounts for direct neighbors. A more holistic measurement is given by a quantity called the eigenvalue centrality. This metric is provided in the node table. It provides a centrality score for each node that is proportional to the sum of the scores of its neighbors. Mathematically, it assigns a set of scores \\(s_j\\) for each node such that:\n\\[ s_j = \\lambda \\cdot \\sum_{i \\in \\text{Neighbors}(j)} s_i \\]\nThe eigenvalue score, by convention, scales so that the largest score is 1. It is only possible to describe the eigenvalue centrality scores for a connected set of nodes on a network, so the computation is done individually for each component. For comparison, we will use the following code to plot the eigenvalue centrality scores of our network.\n\n(\n    node\n    .pipe(ggplot, aes(\"x\", \"y\"))\n    + geom_segment(\n        aes(x=\"x\", y=\"y\", xend=\"xend\", yend=\"yend\"),\n        data=edge,\n        alpha=0.1\n    )\n    + geom_point(aes(color=\"eigen\"), size=3)\n    + scale_color_cmap(cmap_name=\"viridis\")\n    + theme_void()\n    + labs(\n        title=\"Network Colored by Eigenvalue Centrality\",\n        color=\"Eigenvalue\\nCentrality\"\n    )\n)\n\n\n\n\n\n\n\n\nThe visualization shows a slightly different pattern compared to the degree centrality scores. The biggest difference is that the eigenvalue centrality is more concentrated on the most central connections, whereas degree centrality is more spread out. We will see in the next few sections that this is primarily a result of using a linear scale to plot the colors. If we transform the eigenvalue centrality scores with another function first, we would see that the pattern more gradually shows differences across the entire network.\nAnother measurement of centrality is given by the closeness centrality score. For each node in the network, consider the minimum number of edges that are needed to go from this node to any other node within its component. Adding the reciprocal of these scores together gives a measurement of how close a node is to all of the other nodes in the network. The closeness centrality score for a node is given as the variable close in our node table. Again, we will plot these scores with the following code to compare to the other types of centrality scores.\n\n(\n    node\n    .filter(c.component == 1)\n    .pipe(ggplot, aes(\"x\", \"y\"))\n    + geom_segment(\n        aes(x=\"x\", y=\"y\", xend=\"xend\", yend=\"yend\"),\n        data=edge,\n        alpha=0.1\n    )\n    + geom_point(aes(color=\"close\"), size=3)\n    + scale_color_cmap(cmap_name=\"viridis\")\n    + theme_void()\n    + labs(\n        title=\"Network Colored by Closeness Centrality\",\n        color=\"Closeness\\nCentrality\"\n    )\n)\n\n\n\n\n\n\n\n\nThe output of the closeness centrality scores shows different patterns than the eigenvalue centrality scores. Here we see a much smoother transition from the most central to the least central nodes. We will look at different kinds of networks later in the chapter that illustrate further differences between each type of centrality score.\nThe final measurement of centrality we have in our table, betweenness centrality, also comes from considering minimal paths. For every two nodes in a connected component, consider all of the possible ways to go from one to the other along edges in the network. Then, consider all of the paths (there may be only one) between the two nodes that require a minimal number of hops. The betweenness centrality score measures how many of these minimal paths go through each node (there is some normalization to account for the case when there are many minimal paths, so the counts are not exact integers). This score is stored in the variable between. A plot of the betweenness score is given by the following code.\n\n(\n    node\n    .pipe(ggplot, aes(\"x\", \"y\"))\n    + geom_segment(\n        aes(x=\"x\", y=\"y\", xend=\"xend\", yend=\"yend\"),\n        data=edge,\n        alpha=0.1\n    )\n    + geom_point(aes(color=\"between\"), size=3)\n    + scale_color_cmap(cmap_name=\"viridis\")\n    + theme_void()\n    + labs(\n        title=\"Network Colored by Betweenness Centrality\",\n        color=\"Betweenness\\nCentrality\"\n    )\n)\n\n\n\n\n\n\n\n\nThe betweenness score often tends to have a different pattern than the other centrality scores. It gives a high score to bridges between different parts of the network, rather than giving high weight to how central a node is within a particular cluster. One challenge with the page link network over this small set of pages is that we need to create a different kind of network in order to really see the differences between the betweenness centrality and the other types of centrality that we’ve discussed so far. To better understand the centrality scores, we will delve further into another set of networks such as co-citation and nearest neighbor networks.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "18_network_data.html#clusters",
    "href": "18_network_data.html#clusters",
    "title": "18  Network Data",
    "section": "18.5 Clusters",
    "text": "18.5 Clusters\nThe centrality of a node is not the only thing that we can measure when looking at networks. Another algorithm that we can perform is that of clustering. Here, we try to split the nodes into groups such that a large number of the edges are between nodes within a group rather than across groups. When we created our network, a clustering of the nodes based on the edges was automatically performed. The identifiers for the clusters are in the column called cluster. We can visualize the clusters defined on our Wikipedia-page network using the following code.\n\n(\n    node\n    .pipe(ggplot, aes(\"x\", \"y\"))\n    + geom_segment(\n        aes(x=\"x\", y=\"y\", xend=\"xend\", yend=\"yend\"),\n        data=edge,\n        alpha=0.1\n    )\n    + geom_point(aes(color=\"cluster\"), size=3)\n    + theme_void()\n    + labs(\n        title=\"Network Colored by Cluster\",\n        color=\"Cluster\"\n    )\n)\n\n\n\n\n\n\n\n\nThe output of the cluster visualization shows the different communities detected in the network. We are running out of space to put labels on the plot. This is one major consideration when thinking of networks as a form of visual exploration and communication; bigger is not necessarily better. Even on a large screen, networks with hundreds of nodes or more become unwieldy to plot. As an alternative, we can summarize the network data in the form of tables. For example, we can paste together the nodes within a cluster to try to further understand the internal structure of the relationships.\n\n(\n    node\n    .group_by(c.cluster)\n    .agg(members = c.id.str.concat(\"; \"))\n)\n\n\nshape: (8, 2)\n\n\n\ncluster\nmembers\n\n\nstr\nstr\n\n\n\n\n\"5\"\n\"Samuel Pepys\"\n\n\n\"4\"\n\"George Herbert; Henry Vaughan\"\n\n\n\"2\"\n\"Charles Dickens; T. S. Eliot; …\n\n\n\"6\"\n\"W. H. Auden; Stephen Spender; …\n\n\n\"0\"\n\"Marie de France\"\n\n\n\"7\"\n\"Daphne du Maurier\"\n\n\n\"3\"\n\"William Wordsworth; John Keats…\n\n\n\"1\"\n\"Geoffrey Chaucer; William Lang…\n\n\n\n\n\n\nNetwork clusters can be very insightful for understanding the structure of a large network. The example data that we have been working with so far is relatively small and forms a larger single cluster around a few well-known authors. Because of the length and richness of the textual information, this set of 75 authors produced interesting results on its own in the previous chapter. It is also a great size to visualize and illustrate the core computed metrics associated with network analysis since it is small enough to plot every node and edge. To go farther and show the full power of these methods as analytic tools, we need to expand our collection.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "18_network_data.html#co-citation-networks",
    "href": "18_network_data.html#co-citation-networks",
    "title": "18  Network Data",
    "section": "18.6 Co-citation Networks",
    "text": "18.6 Co-citation Networks\nThe network structure we have been working with is a form called a citation network. Pages are joined whenever one page links to another. This is a popular method in understanding academic articles, friendship networks on social media (i.e., tracking mentions on Twitter), or investigating the relative importance of court cases. There are some drawbacks of using citation counts, however. They are sensitive to the time-order of publication, they are affected by the relative length of each document, and they are easily affected by small changes. Wikipedia articles are continually edited, so there is no clear temporal ordering of the pages, and there is relatively little benefit for someone to artificially inflate the network centrality of an article. The length of Wikipedia articles, though, are highly variable and not always well correlated with the notoriety of the subject matter. So, partially to avoid biasing our results due to page length, and more so to illustrate the general concept when applying these techniques to other sets of citation data, let’s look at an alternative that helps to avoid all of these issues.\nA co-citation network is a method of showing links across a citation network while avoiding some of the pitfalls that arise when using direct links. A co-citation is formed between two pages whenever a third entry cites both of them. The idea is that if a third source talks about two sources in the same reference, there is likely a relationship between the documents. We can create a co-citation dataset from Wikipedia by first downloading all of the pages linked to from any of the author pages in our collection. We can then count how often any pair of pages in our dataset were both linked into from the same source. Here, we will load the dataset into Python as a structured table. Co-citations are, by definition, undirected. In the dataset below, we have sorted the edges so that doc_id always comes alphabetically before doc_id_out. The column count tells how many third pages cite both of the respective articles.\n\npage_cocitation\n\n\nshape: (2_114, 3)\n\n\n\ndoc_id\ndoc_id_out\ncount\n\n\nstr\nstr\ni64\n\n\n\n\n\"Marie de France\"\n\"Matthew Arnold\"\n1\n\n\n\"Marie de France\"\n\"Oscar Wilde\"\n2\n\n\n\"Marie de France\"\n\"Samuel Pepys\"\n1\n\n\n\"Marie de France\"\n\"T. S. Eliot\"\n1\n\n\n\"Marie de France\"\n\"Thomas Malory\"\n1\n\n\n…\n…\n…\n\n\n\"Seamus Heaney\"\n\"Virginia Woolf\"\n2\n\n\n\"Seamus Heaney\"\n\"W. B. Yeats\"\n4\n\n\n\"Seamus Heaney\"\n\"W. H. Auden\"\n5\n\n\n\"Seamus Heaney\"\n\"William Shakespeare\"\n1\n\n\n\"Seamus Heaney\"\n\"William Wordsworth\"\n1\n\n\n\n\n\n\nWhen working with co-citations, it is useful to only include links between two pages when the count is above some threshold. In the code below, we will filter our new edge list to include only the links that have a count of at least 6 different citations. We then create a new network, node, and edge set.\n\nfiltered_cocitations = (\n    page_cocitation\n    .filter(c.count &gt; 6)\n    .rename({\"doc_id_out\": \"doc_id2\"})\n)\n\ncocite_node, cocite_edge, cocite_G = DSNetwork.process(\n    filtered_cocitations, \n    directed=False\n)\n\nIn the code block below, we will look at the nodes with the largest eigenvalue centrality scores, with the top ten printed in the text. As with the previous network, there is only one component in this network; otherwise, we would want to filter the data such that we are looking at pages in the largest component, which will always be component number 1.\n\n(\n    cocite_node\n    .sort(c.eigen, descending=True)\n    .head(10)\n    .select(c.id, c.eigen, c.degree)\n)\n\n\nshape: (10, 3)\n\n\n\nid\neigen\ndegree\n\n\nstr\nf64\ni64\n\n\n\n\n\"John Milton\"\n1.0\n36\n\n\n\"Charles Dickens\"\n0.890534\n30\n\n\n\"John Dryden\"\n0.772376\n23\n\n\n\"George Orwell\"\n0.769806\n25\n\n\n\"William Shakespeare\"\n0.769678\n23\n\n\n\"William Blake\"\n0.768104\n23\n\n\n\"Samuel Johnson\"\n0.766416\n21\n\n\n\"T. S. Eliot\"\n0.730718\n23\n\n\n\"Samuel Taylor Coleridge\"\n0.695994\n21\n\n\n\"John Keats\"\n0.60477\n16\n\n\n\n\n\n\nThe most central nodes in the co-citation network show a similar set of pages showing up in the top-10 list but with some notable differences in the relative ordering. Shakespeare is no longer at the top of the list. Focusing on the first name on the list, there are at least 36 other authors in our relatively small set that are cited at least six times in the same page as John Milton. Looking into the example links, we see that this is because Milton is associated with most other figures of the 17th Century, poets, and writers that have religious themes in their works. These overlapping sets creates a relatively large number of other pages that Milton is connected to. As mentioned at the start of the section, the differences between citation and co-citation are somewhat muted with the Wikipedia pages because they are constantly being modified and edited, making it possible for pages to directly link to other pages even if they were originally created before them. When working with other citation networks, such as scholarly citations or court records, the differences between the two are often striking.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "18_network_data.html#directed-networks",
    "href": "18_network_data.html#directed-networks",
    "title": "18  Network Data",
    "section": "18.7 Directed Networks",
    "text": "18.7 Directed Networks\nAt the start of the chapter, we noted that networks can be given edges that are either directed or undirected. The algorithms and metrics we have looked at so far all work on undirected networks and so even in the case of the original links, which do have a well-defined direction, we have been treating the links as undirected. It is possible to create a network object that takes this relationship into account by setting the directed argument to True.\n\ndirected_node, directed_edge, directed_G = DSNetwork.process(\n    page_citation, \n    directed=True\n)\ndirected_node\n\n\nshape: (72, 11)\n\n\n\nid\nx\ny\ncomponent\ncomponent_size\ncluster\ndegree_out\ndegree_in\ndegree_total\neigen\nbetween\n\n\nstr\nf64\nf64\ni64\ni64\nstr\ni64\ni64\ni64\nf64\nf64\n\n\n\n\n\"Marie de France\"\n5.795698\n-0.479973\n9\n1\n\"0\"\n1\n0\n1\nnull\nnull\n\n\n\"Geoffrey Chaucer\"\n2.285049\n-0.117465\n10\n60\n\"1\"\n6\n10\n16\n0.526894\n143.911999\n\n\n\"William Langland\"\n2.771146\n-2.514972\n12\n1\n\"1\"\n0\n3\n3\nnull\nnull\n\n\n\"John Gower\"\n3.547754\n-0.863886\n10\n60\n\"1\"\n3\n2\n5\n0.194129\n27.454978\n\n\n\"John Dryden\"\n0.460255\n-0.182116\n10\n60\n\"1\"\n13\n12\n25\n0.588326\n282.521198\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Cecil Day-Lewis\"\n-1.080815\n-2.638103\n10\n60\n\"6\"\n5\n6\n11\n0.129866\n104.32109\n\n\n\"Christopher Isherwood\"\n-0.458387\n-3.408866\n10\n60\n\"6\"\n5\n4\n9\n0.104236\n8.040945\n\n\n\"Louis MacNeice\"\n0.18922\n-1.816749\n10\n60\n\"6\"\n9\n3\n12\n0.097687\n34.675758\n\n\n\"Rex Warner\"\n-1.728768\n-3.922177\n10\n60\n\"6\"\n3\n1\n4\n0.016511\n0.35\n\n\n\"Edward Upward\"\n-0.972036\n-3.999017\n10\n60\n\"6\"\n3\n3\n6\n0.05806\n0.85\n\n\n\n\n\n\nThe node table contains a slightly different set of measurements. Closeness centrality is no longer available; eigenvalue and betweenness centrality are still computed, but are done so without using the directions of the edges. There are now three different degree counts: the out-degree (number of links on the page), the in-degree (number of links into a page), and the total of these two.\nWe can, if desired, use the directed structure in our plot. To visualize a directed network, we add an arrow argument to the geom_segment layer.\n\n(\n    directed_node\n    .pipe(ggplot, aes(\"x\", \"y\"))\n    + geom_segment(\n        aes(x=\"x\", y=\"y\", xend=\"xend\", yend=\"yend\"),\n        data=directed_edge,\n        alpha=0.7,\n        arrow=arrow(length=0.02)\n    )\n    + geom_point()\n    + theme_void()\n    + labs(title=\"Directed Page Link Network\")\n)\n\n\n\n\n\n\n\n\nVisualizing the direction of the arrows can be helpful for illustrating concepts in smaller networks.\nAn interesting analysis that we can do with a directed network is to look at the in-degree as a function of the out-degree. The code below creates a plot that investigates the relationship between these two degree counts. We have highlighted a set of six authors that show interesting relationships between the two variables.\n\nhighlight_authors = [\n    \"William Shakespeare\", \"William Wordsworth\", \"T. S. Eliot\", \n    \"Lord Byron\", \"W. H. Auden\", \"George Orwell\"\n]\n\ndirected_highlight = (\n    directed_node\n    .filter(c.id.is_in(highlight_authors))\n)\n\n(\n    directed_node\n    .filter(c.component == 1)\n    .pipe(ggplot, aes(\"degree_out\", \"degree_in\"))\n    + geom_point()\n    + geom_text(\n        aes(label=\"id\"),\n        data=directed_highlight,\n        nudge_y=1,\n        nudge_x=-1\n    )\n    + geom_abline(slope=1, intercept=0, linetype=\"dashed\")\n    + labs(\n        title=\"In-degree vs Out-degree in British Authors Network\",\n        x=\"Out-degree\",\n        y=\"In-degree\"\n    )\n)\n\n\n\n\n\n\n\n\nThis plot reveals some interesting details about all of the citation networks that we have seen so far. It was probably not surprising to see Shakespeare as the node with the highest centrality score. Here, we see that this is only partially because many other author’s pages link into his. A parallel reason is that the Shakespeare page also has more links out to other authors than any other page. While the two metrics largely mirror one another, it is insightful to identify pages that have an unbalanced number of in or out citations. George Orwell, for example, is not referenced by many other pages in our collection, but has many outgoing links. It’s possible that this is partially a temporal effect of Orwell being a later author in the set; the page cites his literary influences and it’s not hard to see why those influences would not cite back into him. Wordsworth and Lord Byron show the opposite pattern, with more links into them than might be expected given their number of out links. Both of these are interesting observations that merit further study.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "18_network_data.html#distance-networks",
    "href": "18_network_data.html#distance-networks",
    "title": "18  Network Data",
    "section": "18.8 Distance Networks",
    "text": "18.8 Distance Networks\nFor a final common type of network found in humanities research, we return to a task that we saw in Chapter 19. After having built a table of textual annotations, recall that we were able to create links between two documents whenever they are sufficiently close to one another based on the angle distance between their term-frequency scores. By choosing a suitable cutoff score for the maximal distance between pages, we can create an edge list between the pages. These edges, along with the DSNetwork.process function, can be used to construct what is called a distance network. As with co-citations, the network here has no notion of direction and therefore we will create another undirected network.\nDistance networks are particularly useful when we want to explore relationships that are not explicitly encoded in the data. For example, in our Wikipedia corpus, the citation links tell us about explicit references between pages, but the distance network can reveal implicit similarities based on the content of the pages themselves.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "18_network_data.html#nearest-neighbor-networks",
    "href": "18_network_data.html#nearest-neighbor-networks",
    "title": "18  Network Data",
    "section": "18.9 Nearest Neighbor Networks",
    "text": "18.9 Nearest Neighbor Networks\nWe finish this chapter by looking at one final network type that we can apply to our Wikipedia corpus. The example here generates a network structure that is designed to avoid a small set of central nodes by balancing the degree of the network across all of the nodes. This approach can be applied to any set of distance scores defined between pairs of objects.\nThe idea behind a symmetric nearest neighbors network is straightforward. For each document, we find its k nearest neighbors based on some distance metric. We then create an edge between two documents only if they are mutually in each other’s nearest neighbor set. This constraint ensures that the resulting network is symmetric and avoids the problem of popular documents accumulating too many connections.\nThe network we create this way can never have a degree larger than k (the number of neighbors we consider). This stops the network from focusing on lots of weak connections to popular pages and focuses on links between pages that go both ways.\nThe structure of the symmetric nearest neighbors network is quite different from the other networks we have explored. One way to see this is by looking at the relationship between eigenvalue centrality and betweenness centrality. In most of the other networks, these were highly correlated to one another, but in symmetric nearest neighbor networks that is often not the case. There can be several nodes that have a high betweenness score despite having a lower eigenvalue centrality score. These are the gatekeeper nodes that link other, more densely connected parts of the plot.\nSince the symmetric nearest neighbors plot avoids placing too many nodes all at the center of the network, the clusters resulting from the network are also often more interesting and uniform in size than other kinds of networks. While all of the network types here have their place, when given a set of distances, using symmetric nearest neighbors is often a good choice to get interesting results that show the entire structure of the network rather than focusing on only the most centrally located nodes.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "18_network_data.html#summary",
    "href": "18_network_data.html#summary",
    "title": "18  Network Data",
    "section": "18.10 Summary",
    "text": "18.10 Summary\nIn this chapter, we explored several fundamental concepts in network analysis. We began by understanding how to represent network data as edge lists and how to compute basic statistics about network connectivity. We then examined various measures of centrality, including degree, eigenvalue, closeness, and betweenness centrality, each of which captures different aspects of a node’s importance within a network.\nWe also explored network clustering, which allows us to identify communities of closely connected nodes. Different types of networks—citation networks, co-citation networks, directed networks, distance networks, and nearest neighbor networks—each offer unique perspectives on the relationships within our data. The choice of which network type to use depends on the specific questions we want to answer and the nature of our underlying data.\nNetwork analysis is particularly powerful when combined with the textual analysis techniques from the previous chapter. Together, these methods provide complementary views of the relationships between documents in a corpus.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "18_network_data.html#references",
    "href": "18_network_data.html#references",
    "title": "18  Network Data",
    "section": "References",
    "text": "References\n\n\n\n\n[1] Newman, M (2018 ). Networks. Oxford University Press\n\n\n[2] Wasserman, S and Faust, K (1994 ). Social network analysis: Methods and applications. Cambridge university press\n\n\n[3] Kolaczyk, E D and Csárdi, G (2014 ). Statistical Analysis of Network Data with r. Springer\n\n\n[4] Csárdi, G, Nepusz, T, Traag, V, Horvát, S, Zanini, F, Noom, D and Müller, K (2023 ). igraph: Network Analysis and Visualization in r. https://CRAN.R-project.org/package=igraph\n\n\n[5] Fruchterman, T M and Reingold, E M (1991 ). Graph drawing by force-directed placement. Software: Practice and experience. Wiley Online Library. 21 1129–64",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "19_textual_data.html",
    "href": "19_textual_data.html",
    "title": "19  Textual Data",
    "section": "",
    "text": "19.1 Setup\nLoad all of the modules and datasets needed for the chapter. We also load the spacy module designed specifically for processing large collections of text.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nimport spacy\n\nmeta = pl.read_csv(\"data/wiki_uk_meta.csv.gz\")\ndocs = pl.read_csv(\"data/wiki_uk_authors_text.csv\")\ndocs_fr = pl.read_csv(\"data/wiki_uk_authors_text_fr.csv\")",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "19_textual_data.html#introduction",
    "href": "19_textual_data.html#introduction",
    "title": "19  Textual Data",
    "section": "19.2 Introduction",
    "text": "19.2 Introduction\nEvery dataset we have encountered so far in this book has consisted of structured observations: rows with well-defined columns containing numbers, categories, or dates. But an enormous amount of the world’s information exists as unstructured text. Medical records contain free-text physician notes alongside coded diagnoses. Customer feedback arrives as open-ended survey responses. Historical archives preserve centuries of human thought in letters, newspapers, and books. Social scientists study political speeches, journalists analyze leaked documents, and humanists trace the evolution of literary style across generations.\nWorking with textual data requires a fundamentally different approach than working with numbers. A sentence is not just a sequence of characters but a structured object with grammar, meaning, and context. The word “bank” means something different in “river bank” than in “bank account.” The phrase “not bad” typically means something positive despite containing a negation. These subtleties make text both rich and challenging to analyze computationally.\nIn this chapter, we introduce tools for transforming unstructured text into structured data that can be analyzed using the techniques developed throughout this book. The key insight is that once we have converted text into tables of tokens, counts, and annotations, we can apply familiar operations like filtering, grouping, and joining to answer questions about language. What words distinguish one author from another? Which terms best summarize a document? How does writing style vary across languages or time periods?\nWe will use spaCy, a modern natural language processing library, to handle the linguistic heavy lifting. SpaCy provides pre-trained models that can tokenize text, identify parts of speech, recognize named entities, and parse grammatical structure. Our job is to take the output of these models and reshape it into forms suitable for analysis.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "19_textual_data.html#nlp-pipeline",
    "href": "19_textual_data.html#nlp-pipeline",
    "title": "19  Textual Data",
    "section": "19.3 NLP Pipeline",
    "text": "19.3 NLP Pipeline\nWe load spaCy’s English language model, which contains statistical models trained on large corpora of English text. The “sm” suffix indicates the small model, which balances accuracy with speed and memory usage. Larger models are available for applications requiring higher precision.\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nOur primary dataset consists of Wikipedia pages for authors from the United Kingdom. We have a metadata table containing information about each author.\n\nmeta\n\n\nshape: (75, 7)\n\n\n\ndoc_id\nborn\ndied\nera\ngender\nlink\nshort\n\n\nstr\ni64\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n\"Marie de France\"\n1160\n1215\n\"Early\"\n\"female\"\n\"Marie_de_France\"\n\"Marie d. F.\"\n\n\n\"Geoffrey Chaucer\"\n1343\n1400\n\"Early\"\n\"male\"\n\"Geoffrey_Chaucer\"\n\"Chaucer\"\n\n\n\"John Gower\"\n1330\n1408\n\"Early\"\n\"male\"\n\"John_Gower\"\n\"Gower\"\n\n\n\"William Langland\"\n1332\n1386\n\"Early\"\n\"male\"\n\"William_Langland\"\n\"Langland\"\n\n\n\"Margery Kempe\"\n1373\n1438\n\"Early\"\n\"female\"\n\"Margery_Kempe\"\n\"Kempe\"\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Stephen Spender\"\n1909\n1995\n\"Twentieth C\"\n\"male\"\n\"Stephen_Spender\"\n\"Spender\"\n\n\n\"Christopher Isherwood\"\n1904\n1986\n\"Twentieth C\"\n\"male\"\n\"Christopher_Isherwood\"\n\"Isherwood\"\n\n\n\"Edward Upward\"\n1903\n2009\n\"Twentieth C\"\n\"male\"\n\"Edward_Upward\"\n\"Upward\"\n\n\n\"Rex Warner\"\n1905\n1986\n\"Twentieth C\"\n\"male\"\n\"Rex_Warner\"\n\"Warner\"\n\n\n\"Seamus Heaney\"\n1939\n1939\n\"Twentieth C\"\n\"male\"\n\"Seamus_Heaney\"\n\"Heaney\"\n\n\n\n\n\n\nAs well as a seperate file giving each of the texts from the documents.\n\ndocs\n\n\nshape: (75, 2)\n\n\n\ndoc_id\ntext\n\n\nstr\nstr\n\n\n\n\n\"Marie de France\"\n\"Marie de France was a poet pos…\n\n\n\"Geoffrey Chaucer\"\n\"Geoffrey Chaucer was an Englis…\n\n\n\"John Gower\"\n\"John Gower was an English poet…\n\n\n\"William Langland\"\n\"William Langland is the presum…\n\n\n\"Margery Kempe\"\n\"Margery Kempe was an English C…\n\n\n…\n…\n\n\n\"Stephen Spender\"\n\"Sir Stephen Harold Spender CBE…\n\n\n\"Christopher Isherwood\"\n\"Christopher William Bradshaw I…\n\n\n\"Edward Upward\"\n\"Edward Falaise Upward FRSL was…\n\n\n\"Rex Warner\"\n\"Rex Warner was an English clas…\n\n\n\"Seamus Heaney\"\n\"Seamus Justin Heaney MRIA was …\n\n\n\n\n\n\nThe metadata table contains structured information extracted from Wikipedia’s infoboxes: birth and death dates, occupations, and other biographical details. The text table contains the prose content of each page, which we will process using natural language techniques.\nNatural language processing transforms raw text into structured annotations. The DSText.process method sends each document through spaCy’s processing pipeline, which performs several analyses in sequence: tokenization (splitting text into words and punctuation), part-of-speech tagging (identifying nouns, verbs, adjectives), lemmatization (reducing words to their base forms), named entity recognition (identifying people, places, organizations), and dependency parsing (analyzing grammatical relationships).\n\nanno = DSText.process(docs, nlp)\nanno\n\n\nshape: (408_700, 15)\n\n\n\ndoc_id\nsid\ntid\ntoken\ntoken_with_ws\nlemma\nupos\ntag\nis_alpha\nis_stop\nis_punct\ndep\nhead_idx\nent_type\nent_iob\n\n\nstr\ni64\ni64\nstr\nstr\nstr\nstr\nstr\nbool\nbool\nbool\nstr\ni64\nstr\nstr\n\n\n\n\n\"Marie de France\"\n1\n1\n\"Marie\"\n\"Marie \"\n\"Marie\"\n\"PROPN\"\n\"NNP\"\ntrue\nfalse\nfalse\n\"compound\"\n3\n\"PERSON\"\n\"B\"\n\n\n\"Marie de France\"\n1\n2\n\"de\"\n\"de \"\n\"de\"\n\"X\"\n\"FW\"\ntrue\nfalse\nfalse\n\"nmod\"\n3\n\"PERSON\"\n\"I\"\n\n\n\"Marie de France\"\n1\n3\n\"France\"\n\"France \"\n\"France\"\n\"PROPN\"\n\"NNP\"\ntrue\nfalse\nfalse\n\"nsubj\"\n4\n\"PERSON\"\n\"I\"\n\n\n\"Marie de France\"\n1\n4\n\"was\"\n\"was \"\n\"be\"\n\"AUX\"\n\"VBD\"\ntrue\ntrue\nfalse\n\"ROOT\"\n4\n\"\"\n\"O\"\n\n\n\"Marie de France\"\n1\n5\n\"a\"\n\"a \"\n\"a\"\n\"DET\"\n\"DT\"\ntrue\ntrue\nfalse\n\"det\"\n6\n\"\"\n\"O\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Seamus Heaney\"\n242\n18\n\"of\"\n\"of \"\n\"of\"\n\"ADP\"\n\"IN\"\ntrue\ntrue\nfalse\n\"prep\"\n17\n\"\"\n\"O\"\n\n\n\"Seamus Heaney\"\n242\n19\n\"his\"\n\"his \"\n\"his\"\n\"PRON\"\n\"PRP$\"\ntrue\ntrue\nfalse\n\"poss\"\n21\n\"\"\n\"O\"\n\n\n\"Seamus Heaney\"\n242\n20\n\"finest\"\n\"finest \"\n\"fine\"\n\"ADJ\"\n\"JJS\"\ntrue\nfalse\nfalse\n\"amod\"\n21\n\"\"\n\"O\"\n\n\n\"Seamus Heaney\"\n242\n21\n\"poems\"\n\"poems\"\n\"poem\"\n\"NOUN\"\n\"NNS\"\ntrue\nfalse\nfalse\n\"pobj\"\n18\n\"\"\n\"O\"\n\n\n\"Seamus Heaney\"\n242\n22\n\".\"\n\".\"\n\".\"\n\"PUNCT\"\n\".\"\nfalse\nfalse\ntrue\n\"punct\"\n8\n\"\"\n\"O\"\n\n\n\n\n\n\nThere is a lot of information that has been automatically added to this table, thanks to the collective results of decades of research in computational linguistics and natural language processing. Each row corresponds to a word or a punctuation mark (created by the process of tokenization), along with metadata describing the token. Notice that reading down the column token reproduces the original text. The columns available are:\n\ndoc_id: A key that allows us to group tokens into documents and to link back into the original input table.\nsid: Numeric identifier of the sentence number.\ntid: Numeric identifier of the token within a sentence. The first three columns form a primary key for the table.\ntoken: A character variable containing the detected token, which is either a word or a punctuation mark.\ntoken_with_ws: The token with white space (spaces and new-line characters) added. This is useful if we wanted to re-create the original text from the token table.\nlemma: A normalized version of the token. For example, it removes start-of-sentence capitalization, turns all nouns into their singular form, and converts verbs into their infinitive form.\nupos: The universal part of speech code, which are parts of speech that can be defined in most spoken languages. These tend to correspond to the parts of speech taught in primary schools, such as “NOUN”, “ADJ” (adjective), and “ADV” (adverb).\ntag: A fine-grained part of speech code that depends on the specific language (here, English) and models being used.\nis_alpha, is_stop, is_punct: Boolean flags for alphabetic characters, stop words, and punctuation.\ndep: The dependency relation label describing how this token relates grammatically to another token.\nhead_idx: The token index of the word in the sentence that this token is grammatically related to.\nent_type: The named entity type, if this token is part of a recognized entity.\n\nThere are many analyses that can be performed on the extracted features that are present in the anno table. Fortunately, many of these can be performed by directly using Polars operations covered in the first five chapters of this text, without the need for any new text-specific functions. For example, we can find the most common nouns in the dataset by filtering on the universal part of speech and grouping by lemma with the code below.\n\n(\n    anno\n    .filter(c.upos == \"NOUN\")\n    .group_by(c.lemma)\n    .agg(count = pl.len())\n    .sort(c.count, descending=True)\n    .head(10)\n)\n\n\nshape: (10, 2)\n\n\n\nlemma\ncount\n\n\nstr\nu32\n\n\n\n\n\"work\"\n1154\n\n\n\"year\"\n1012\n\n\n\"time\"\n846\n\n\n\"poem\"\n744\n\n\n\"life\"\n740\n\n\n\"book\"\n591\n\n\n\"death\"\n540\n\n\n\"novel\"\n527\n\n\n\"poet\"\n513\n\n\n\"family\"\n467\n\n\n\n\n\n\nThe most frequent nouns across the set of documents roughly fall into one of two categories. Those such as “year”, “life”, “death”, and “family” are nouns that we would frequently associate with biographical entries for nearly any group of people. Others, such as “poem”, “book”, “poet”, and the somewhat more generic “work”, capture the specific objects that authors would produce and therefore would be prominent elements of their respective Wikipedia pages. The fact that these two types of nouns show up at the top of the list helps to verify that both the dataset and the NLP pipeline are working as expected.\nWe can use a similar technique to learn about the contents of each of the individual documents. Suppose we wanted to know which adjectives are most used on each page. This can be done by a sequence of Polars operations. First, we filter the data by the part of speech and group the rows of the dataset by the document id and lemma. Then, we count the number of rows for each unique combination of document and lemma and arrange the dataset in descending order of count. We can use the head() method on grouped data to take the most frequent adjectives within each document:\n\n(\n    anno\n    .filter(c.upos == \"ADJ\")\n    .group_by([c.doc_id, c.lemma])\n    .agg(count = pl.len())\n    .sort([c.doc_id, c.count], descending=[False, True])\n    .group_by(c.doc_id)\n    .head(8)\n    .group_by(c.doc_id)\n    .agg(top_adj = c.lemma.sort().str.join(\"; \"))\n)\n\n\nshape: (75, 2)\n\n\n\ndoc_id\ntop_adj\n\n\nstr\nstr\n\n\n\n\n\"Christopher Isherwood\"\n\"-; american; few; first; many;…\n\n\n\"Marie de France\"\n\"12th; close; common; first; la…\n\n\n\"Charles Dickens\"\n\"early; first; great; literary;…\n\n\n\"Alexander Pope\"\n\"18th; english; famous; many; n…\n\n\n\"James Joyce\"\n\"british; english; first; irish…\n\n\n…\n…\n\n\n\"T. S. Eliot\"\n\"early; first; literary; many; …\n\n\n\"Percy Bysshe Shelley\"\n\"early; first; long; major; oth…\n\n\n\"A. A. Milne\"\n\"british; many; more; old; orig…\n\n\n\"James Boswell\"\n\"first; great; literary; old; o…\n\n\n\"Joseph Conrad\"\n\"british; first; french; litera…\n\n\n\n\n\n\nThe output shows many connections between adjectives and the authors. Here, the connections again fall roughly into two groups. Some of the adjectives are fairly generic—such as “more”, “other”, and “many”—and probably say more about the people writing the pages than the subjects of the pages themselves. Other adjectives provide more contextual information about each of the authors. For example, several selected adjectives are key descriptions of an author’s work, such as “Victorian” associated with certain authors and “Gothic” with others. While it is good to see expected relationships to demonstrate the data and techniques are functioning properly, it is also valuable when computational techniques highlight the unexpected.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "19_textual_data.html#n-grams-and-collocations",
    "href": "19_textual_data.html#n-grams-and-collocations",
    "title": "19  Textual Data",
    "section": "19.4 N-grams and Collocations",
    "text": "19.4 N-grams and Collocations\nSo far we have analyzed individual words in isolation. But meaning often emerges from combinations of words. The phrase “New York” refers to a specific city, not something novel and a name. “Machine learning” describes a field of study, not appliances that acquire knowledge. “Poet laureate” is a title, not just any poet who happens to be a laureate. To capture these multi-word expressions, we turn to n-grams: contiguous sequences of n tokens.\nA unigram is a single token (what we have been working with). A bigram is a pair of adjacent tokens. A trigram is a sequence of three tokens. In general, an n-gram captures local word order, which is lost when we treat documents as unordered collections of words.\nConstructing n-grams requires us to look at tokens in context. Polars window functions allow us to access neighboring rows, which we can use to pair each token with the tokens that follow it. The key is to shift the token column to align adjacent words on the same row.\n\nbigrams = (\n    anno\n    .filter(c.is_alpha)\n    .with_columns(\n        next_token = c.lemma.shift(-1).over([c.doc_id, c.sid]),\n        next_is_alpha = c.is_alpha.shift(-1).over([c.doc_id, c.sid])\n    )\n    .filter(c.next_is_alpha == True)\n    .with_columns(\n        bigram = c.lemma + \" \" + c.next_token\n    )\n)\nbigrams.select(c.doc_id, c.sid, c.tid, c.lemma, c.next_token, c.bigram)\n\n\nshape: (363_449, 6)\n\n\n\ndoc_id\nsid\ntid\nlemma\nnext_token\nbigram\n\n\nstr\ni64\ni64\nstr\nstr\nstr\n\n\n\n\n\"Marie de France\"\n1\n1\n\"Marie\"\n\"de\"\n\"Marie de\"\n\n\n\"Marie de France\"\n1\n2\n\"de\"\n\"France\"\n\"de France\"\n\n\n\"Marie de France\"\n1\n3\n\"France\"\n\"be\"\n\"France be\"\n\n\n\"Marie de France\"\n1\n4\n\"be\"\n\"a\"\n\"be a\"\n\n\n\"Marie de France\"\n1\n5\n\"a\"\n\"poet\"\n\"a poet\"\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Seamus Heaney\"\n242\n16\n\"inspire\"\n\"many\"\n\"inspire many\"\n\n\n\"Seamus Heaney\"\n242\n17\n\"many\"\n\"of\"\n\"many of\"\n\n\n\"Seamus Heaney\"\n242\n18\n\"of\"\n\"his\"\n\"of his\"\n\n\n\"Seamus Heaney\"\n242\n19\n\"his\"\n\"fine\"\n\"his fine\"\n\n\n\"Seamus Heaney\"\n242\n20\n\"fine\"\n\"poem\"\n\"fine poem\"\n\n\n\n\n\n\nThe shift(-1) operation moves each column up by one position within each document and sentence, so that each row contains both the current token and the following token. We filter to keep only cases where both tokens are alphabetic, excluding bigrams that span punctuation or sentence boundaries.\nNow we can count bigram frequencies just as we counted unigram frequencies:\n\n(\n    bigrams\n    .group_by(c.bigram)\n    .agg(count = pl.len())\n    .sort(c.count, descending=True)\n    .head(15)\n)\n\n\nshape: (15, 2)\n\n\n\nbigram\ncount\n\n\nstr\nu32\n\n\n\n\n\"of the\"\n3254\n\n\n\"in the\"\n2374\n\n\n\"to the\"\n1138\n\n\n\"of his\"\n1009\n\n\n\"he be\"\n963\n\n\n…\n…\n\n\n\"on the\"\n691\n\n\n\"it be\"\n654\n\n\n\"for the\"\n647\n\n\n\"in his\"\n628\n\n\n\"be the\"\n611\n\n\n\n\n\n\nThe most frequent bigrams by raw counts largely consist of functional phrases that appear in many types of text. The phrase “of the” appears often simply because both words are common, not because they form a meaningful unit. To identify true collocations—word pairs that occur together more often than chance would predict—we use pointwise mutual information (PMI). PMI compares the observed frequency of a bigram to the frequency we would expect if the two words were independent:\n\\[ \\text{PMI}(w_1, w_2) = \\log_2 \\frac{P(w_1, w_2)}{P(w_1) \\cdot P(w_2)} \\]\nA high PMI indicates that the words co-occur much more frequently than their individual frequencies would suggest. A PMI of zero means the words are independent. Negative PMI (rare in practice for bigrams that actually occur) would indicate the words avoid each other.\nTo compute this we need a few intermediate steps. First of all, we can count the number of times each word appears in all of the texts.\n\nword_counts = (\n    anno\n    .filter(c.is_alpha)\n    .group_by(c.lemma)\n    .agg(word_count = pl.len())\n)\n\ntotal_words = anno.filter(c.is_alpha).height\n\nThen, we could the number of teachings each bigram occurs.\n\nbigram_counts = (\n    bigrams\n    .group_by(c.bigram, c.lemma, c.next_token)\n    .agg(bigram_count = pl.len())\n)\n\ntotal_bigrams = bigrams.height\n\nAnd, finally, we can combine the data together to get the PMI scores for each bigram and sort to find those with the highest scores.\n\n(\n    bigram_counts\n    .join(\n        word_counts.rename({\"lemma\": \"w1\", \"word_count\": \"w1_count\"}),\n        left_on=\"lemma\",\n        right_on=\"w1\"\n    )\n    .join(\n        word_counts.rename({\"lemma\": \"w2\", \"word_count\": \"w2_count\"}),\n        left_on=\"next_token\",\n        right_on=\"w2\"\n    )\n    .with_columns(\n        p_bigram = c.bigram_count / total_bigrams,\n        p_w1 = c.w1_count / total_words,\n        p_w2 = c.w2_count / total_words\n    )\n    .with_columns(\n        pmi = (c.p_bigram / (c.p_w1 * c.p_w2)).log() / np.log(2)\n    )\n    .filter(c.bigram_count &gt;= 5)\n    .sort(c.pmi, descending=True)\n    .select(c.bigram, c.bigram_count, c.pmi)\n    .head(15)\n)\n\n\nshape: (15, 3)\n\n\n\nbigram\nbigram_count\npmi\n\n\nstr\nu32\nf64\n\n\n\n\n\"Lang Syne\"\n5\n16.27981\n\n\n\"El Dorado\"\n5\n16.27981\n\n\n\"Biographia Literaria\"\n6\n16.016776\n\n\n\"Corpus Christi\"\n5\n16.016776\n\n\n\"magnum opus\"\n5\n16.016776\n\n\n…\n…\n…\n\n\n\"gross indecency\"\n5\n15.794384\n\n\n\"Encyclopædia Britannica\"\n7\n15.794384\n\n\n\"Chatterleys Lover\"\n7\n15.794384\n\n\n\"Sweden Norway\"\n5\n15.794384\n\n\n\"Vox Clamantis\"\n5\n15.794384\n\n\n\n\n\n\nThe high-PMI bigrams tell a different story than the high-frequency bigrams. These are phrases where the component words strongly predict each other: proper names, technical terms, and domain-specific expressions. Many of these would be good candidates for treating as single units in downstream analysis.\nWe can extend the same logic to trigrams by shifting twice:\n\n(\n    anno\n    .filter(c.is_alpha)\n    .with_columns(\n        next_token = c.lemma.shift(-1).over([c.doc_id, c.sid]),\n        next_next_token = c.lemma.shift(-2).over([c.doc_id, c.sid]),\n        next_is_alpha = c.is_alpha.shift(-1).over([c.doc_id, c.sid]),\n        next_next_is_alpha = c.is_alpha.shift(-2).over([c.doc_id, c.sid])\n    )\n    .filter((c.next_is_alpha == True) & (c.next_next_is_alpha == True))\n    .with_columns(\n        trigram = c.lemma + \" \" + c.next_token + \" \" + c.next_next_token\n    )\n    .group_by(c.trigram)\n    .agg(count = pl.len())\n    .sort(c.count, descending=True)\n    .head(15)\n)\n\n\nshape: (15, 2)\n\n\n\ntrigram\ncount\n\n\nstr\nu32\n\n\n\n\n\"one of the\"\n217\n\n\n\"as well as\"\n125\n\n\n\"at the time\"\n122\n\n\n\"the age of\"\n120\n\n\n\"be publish in\"\n115\n\n\n…\n…\n\n\n\"a number of\"\n74\n\n\n\"that he be\"\n74\n\n\n\"a series of\"\n64\n\n\n\"to have be\"\n64\n\n\n\"member of the\"\n64\n\n\n\n\n\n\nTrigrams capture even longer expressions, but again we see the raw scores simply find combinations of frequent words. Extending the code for PMI would be required to get more useful information from this table.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "19_textual_data.html#named-entity-recognition",
    "href": "19_textual_data.html#named-entity-recognition",
    "title": "19  Textual Data",
    "section": "19.5 Named Entity Recognition",
    "text": "19.5 Named Entity Recognition\nNamed entity recognition (NER) identifies and classifies proper nouns and other specific references in text. SpaCy’s NER model recognizes several entity types: people (PERSON), organizations (ORG), geopolitical entities like countries and cities (GPE), dates (DATE), works of art (WORK_OF_ART), and others. These annotations are stored in the ent_type column of our token table.\n\n(\n    anno\n    .filter(c.ent_type != \"\")\n    .select(c.doc_id, c.sid, c.tid, c.token, c.ent_type)\n)\n\n\nshape: (73_326, 5)\n\n\n\ndoc_id\nsid\ntid\ntoken\nent_type\n\n\nstr\ni64\ni64\nstr\nstr\n\n\n\n\n\"Marie de France\"\n1\n1\n\"Marie\"\n\"PERSON\"\n\n\n\"Marie de France\"\n1\n2\n\"de\"\n\"PERSON\"\n\n\n\"Marie de France\"\n1\n3\n\"France\"\n\"PERSON\"\n\n\n\"Marie de France\"\n1\n13\n\"France\"\n\"GPE\"\n\n\n\"Marie de France\"\n1\n17\n\"England\"\n\"GPE\"\n\n\n…\n…\n…\n…\n…\n\n\n\"Seamus Heaney\"\n240\n17\n\"BBC\"\n\"ORG\"\n\n\n\"Seamus Heaney\"\n240\n18\n\"Two\"\n\"CARDINAL\"\n\n\n\"Seamus Heaney\"\n241\n3\n\"Marie\"\n\"PERSON\"\n\n\n\"Seamus Heaney\"\n242\n3\n\"first\"\n\"ORDINAL\"\n\n\n\"Seamus Heaney\"\n242\n6\n\"four\"\n\"CARDINAL\"\n\n\n\n\n\n\nEach token that is part of a named entity receives a type label. Multi-word entities like “United Kingdom” have the same label on each constituent token. To work with complete entities rather than individual tokens, we need to group consecutive tokens with the same entity type.\nWe can identify entity boundaries by detecting where the entity type changes:\n\nentities = (\n    anno\n    .filter(c.ent_type != \"\")\n    .with_columns(\n        new_entity = c.ent_iob == \"B\"\n    )\n    .with_columns(\n        entity_id = c.new_entity.cum_sum().over([c.doc_id])\n    )\n    .group_by([c.doc_id, c.entity_id, c.ent_type])\n    .agg(\n        entity_text = c.token.str.join(\" \"),\n    )\n)\nentities.select(c.doc_id, c.ent_type, c.entity_text)\n\n\nshape: (42_261, 3)\n\n\n\ndoc_id\nent_type\nentity_text\n\n\nstr\nstr\nstr\n\n\n\n\n\"George Orwell\"\n\"NORP\"\n\"Jews\"\n\n\n\"Jane Austen\"\n\"TIME\"\n\"Later in the century\"\n\n\n\"Matthew Arnold\"\n\"PERSON\"\n\"Thomas Arnold\"\n\n\n\"Samuel Johnson\"\n\"CARDINAL\"\n\"sixteen hundred\"\n\n\n\"Beatrix Potter\"\n\"DATE\"\n\"1900\"\n\n\n…\n…\n…\n\n\n\"James Joyce\"\n\"NORP\"\n\"Exiles\"\n\n\n\"Thomas Hobbes\"\n\"NORP\"\n\"European\"\n\n\n\"Samuel Pepys\"\n\"ORG\"\n\"Medway\"\n\n\n\"John Keats\"\n\"PERSON\"\n\"Thomas Barnes\"\n\n\n\"Lord Byron\"\n\"PERSON\"\n\"Childe Harolds Pilgrimage\"\n\n\n\n\n\n\nNow we can analyze entities at the appropriate level of granularity. For example, we can find the most frequently mentioned people across all documents:\n\n(\n    entities\n    .filter(c.ent_type == \"PERSON\")\n    .group_by(c.entity_text)\n    .agg(count = pl.len())\n    .sort(c.count, descending=True)\n    .head(15)\n)\n\n\nshape: (15, 2)\n\n\n\nentity_text\ncount\n\n\nstr\nu32\n\n\n\n\n\"Johnson\"\n239\n\n\n\"Shakespeare\"\n183\n\n\n\"Dickens\"\n166\n\n\n\"Shelley\"\n137\n\n\n\"Joyce\"\n136\n\n\n…\n…\n\n\n\"Austen\"\n81\n\n\n\"Mill\"\n80\n\n\n\"Marlowe\"\n74\n\n\n\"Lawrence\"\n72\n\n\n\"Mary\"\n70\n\n\n\n\n\n\nThe most frequently mentioned people likely include both the subjects of the Wikipedia pages and other figures who appear across multiple biographies—editors, patrons, family members, or influential contemporaries.\nWe can examine which entity types are most common overall:\n\n(\n    entities\n    .group_by(c.ent_type)\n    .agg(count = pl.len())\n    .sort(c.count, descending=True)\n)\n\n\nshape: (18, 2)\n\n\n\nent_type\ncount\n\n\nstr\nu32\n\n\n\n\n\"PERSON\"\n13547\n\n\n\"ORG\"\n7412\n\n\n\"DATE\"\n7054\n\n\n\"GPE\"\n4547\n\n\n\"NORP\"\n2864\n\n\n…\n…\n\n\n\"TIME\"\n143\n\n\n\"LAW\"\n77\n\n\n\"QUANTITY\"\n73\n\n\n\"MONEY\"\n17\n\n\n\"PERCENT\"\n1\n\n\n\n\n\n\nBiographical articles naturally contain many dates (birth, death, publication) and references to people and places. The distribution of entity types provides a high-level characterization of the content.\nEntity co-occurrence within documents can reveal relationships. Which people are mentioned together? Which places are associated with which organizations?\n\n# Find all pairs of people mentioned in the same document\npeople = (\n    entities\n    .filter(c.ent_type == \"PERSON\")\n    .select(c.doc_id, person = c.entity_text)\n)\n\nperson_pairs = (\n    people\n    .join(people, on=\"doc_id\", suffix=\"_2\")\n    .filter(c.person &lt; c.person_2)  # Avoid duplicates and self-pairs\n    .group_by([c.person, c.person_2])\n    .agg(co_occurrences = pl.len())\n    .sort(c.co_occurrences, descending=True)\n    .head(10)\n)\nperson_pairs\n\n\nshape: (10, 3)\n\n\n\nperson\nperson_2\nco_occurrences\n\n\nstr\nstr\nu32\n\n\n\n\n\"Mary\"\n\"Shelley\"\n3762\n\n\n\"Shaw\"\n\"Shaws\"\n3717\n\n\n\"Shelley\"\n\"Shelleys\"\n3509\n\n\n\"Johnson\"\n\"Shakespeare\"\n3353\n\n\n\"Austen\"\n\"Austens\"\n2765\n\n\n\"Dickens\"\n\"Oliver Twist\"\n2754\n\n\n\"Jonson\"\n\"Shakespeare\"\n2250\n\n\n\"Keats\"\n\"Keatss\"\n2241\n\n\n\"Vanessa\"\n\"Woolf\"\n2112\n\n\n\"David Copperfield\"\n\"Dickens\"\n2107\n\n\n\n\n\n\nPairs of people who frequently appear together across documents may have historical connections: collaborators, rivals, members of the same literary movement, or subjects of comparative study.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "19_textual_data.html#dependency-parsing",
    "href": "19_textual_data.html#dependency-parsing",
    "title": "19  Textual Data",
    "section": "19.6 Dependency Parsing",
    "text": "19.6 Dependency Parsing\nWhile named entities tell us what is mentioned, dependency parsing tells us how words relate grammatically. Each token in a sentence has a head—another token that it modifies or depends on—and a dependency relation describing the nature of that relationship. The root of the sentence is typically the main verb, and all other tokens connect to it through a tree structure.\nThe dependency annotations are stored in the dep and head_idx columns. Common dependency relations include:\n\nnsubj: Nominal subject (the doer of an action)\ndobj / obj: Direct object (the receiver of an action)\namod: Adjectival modifier\nprep / pobj: Prepositional phrases\ncompound: Compound words or phrases\nROOT: The root of the sentence\n\nLet’s look at a particular example from the Seamus Heaney article.\n\n(\n    anno\n    .filter(c.doc_id == \"Seamus Heaney\")\n    .filter(c.sid == 1)\n    .select(c.tid, c.token, c.upos, c.dep, c.head_idx)\n)\n\n\nshape: (12, 5)\n\n\n\ntid\ntoken\nupos\ndep\nhead_idx\n\n\ni64\nstr\nstr\nstr\ni64\n\n\n\n\n1\n\"Seamus\"\n\"PROPN\"\n\"compound\"\n4\n\n\n2\n\"Justin\"\n\"PROPN\"\n\"compound\"\n4\n\n\n3\n\"Heaney\"\n\"PROPN\"\n\"compound\"\n4\n\n\n4\n\"MRIA\"\n\"PROPN\"\n\"nsubj\"\n5\n\n\n5\n\"was\"\n\"AUX\"\n\"ROOT\"\n5\n\n\n…\n…\n…\n…\n…\n\n\n8\n\"poet\"\n\"NOUN\"\n\"compound\"\n9\n\n\n9\n\"playwright\"\n\"NOUN\"\n\"attr\"\n5\n\n\n10\n\"and\"\n\"CCONJ\"\n\"cc\"\n9\n\n\n11\n\"translator\"\n\"NOUN\"\n\"conj\"\n9\n\n\n12\n\".\"\n\"PUNCT\"\n\"punct\"\n5\n\n\n\n\n\n\nWe can use dependency relations to extract specific grammatical patterns. For example, to find what subjects do what actions, we can look for subject-verb pairs.\n\nverbs = (\n    anno\n    .filter(c.upos == \"VERB\")\n    .select(\n        c.doc_id, c.sid,\n        verb_idx = c.tid,\n        verb = c.lemma\n    )\n)\n\n(\n    anno\n    .filter(c.dep == \"nsubj\")\n    .select(\n        c.doc_id, c.sid, \n        subject = c.lemma,\n        verb_idx = c.head_idx\n    )\n    .join(verbs, on=[c.doc_id, c.sid, c.verb_idx])\n    .group_by([c.subject, c.verb])\n    .agg(count = pl.len())\n    .sort(c.count, descending=True)\n    .head(10)\n)\n\n\nshape: (10, 3)\n\n\n\nsubject\nverb\ncount\n\n\nstr\nstr\nu32\n\n\n\n\n\"he\"\n\"write\"\n273\n\n\n\"he\"\n\"have\"\n131\n\n\n\"he\"\n\"become\"\n105\n\n\n\"she\"\n\"write\"\n79\n\n\n\"he\"\n\"make\"\n61\n\n\n\"he\"\n\"meet\"\n60\n\n\n\"he\"\n\"begin\"\n56\n\n\n\"he\"\n\"say\"\n53\n\n\n\"he\"\n\"return\"\n52\n\n\n\"he\"\n\"leave\"\n51\n\n\n\n\n\n\nThis reveals the typical actions associated with different subjects in our corpus. We might see that authors “write”, “publish”, and “die”, while works “appear”, “receive”, and “influence”.\nWe can also extract adjective-noun pairs to see how different concepts are described:\n\n(\n    anno\n    .filter(c.dep == \"amod\", c.is_alpha)\n    .select(\n        c.doc_id, c.sid,\n        adjective = c.lemma,\n        noun_idx = c.head_idx\n    )\n    .join(\n        anno.filter(c.upos == \"NOUN\").select(\n            c.doc_id, c.sid, noun_idx = c.tid, noun = c.lemma,\n        ),\n        on=[c.doc_id, c.sid, c.noun_idx]\n    )\n    .group_by([c.adjective, c.noun])\n    .agg(count = pl.len())\n    .sort(c.count, descending=True)\n    .head(10)\n)\n\n\nshape: (10, 3)\n\n\n\nadjective\nnoun\ncount\n\n\nstr\nstr\nu32\n\n\n\n\n\"short\"\n\"story\"\n60\n\n\n\"next\"\n\"year\"\n50\n\n\n\"early\"\n\"work\"\n44\n\n\n\"same\"\n\"time\"\n44\n\n\n\"same\"\n\"year\"\n42\n\n\n\"close\"\n\"friend\"\n41\n\n\n\"english\"\n\"poet\"\n38\n\n\n\"major\"\n\"work\"\n33\n\n\n\"literary\"\n\"critic\"\n32\n\n\n\"young\"\n\"man\"\n32\n\n\n\n\n\n\nThe adjective-noun combinations reveal the conceptual vocabulary of the corpus: what kinds of things exist in this textual world, and how are they characterized?",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "19_textual_data.html#keyword-in-context-kwic",
    "href": "19_textual_data.html#keyword-in-context-kwic",
    "title": "19  Textual Data",
    "section": "19.7 Keyword in Context (KWIC)",
    "text": "19.7 Keyword in Context (KWIC)\nSometimes we want to see how a specific word is used across the corpus. A concordance or keyword in context (KWIC) display shows each occurrence of a target word along with the words that surround it. This technique, which predates computers, remains invaluable for understanding how language is actually used.\nTo build a KWIC display, we need to extract a window of tokens around each occurrence of our keyword. This logic is implemented in the DSText.kwic method.\n\nkwic_results = DSText.kwic(anno, \"poetry\", max_num=15, window=5)\nkwic_results\n\nMarie de France:101                             exhibit a form of lyrical [poetry] that influenced the way that            \nMarie de France:101                     influenced the way that narrative [poetry] was subsequently composed adding another\nGeoffrey Chaucer:2                    alternatively the father of English [poetry] .                                       \nGeoffrey Chaucer:45                    introduced him to medieval Italian [poetry] the forms and stories of                \nGeoffrey Chaucer:110                         nettle in Chaucers garden of [poetry] .                                       \nGeoffrey Chaucer:118                                                  The [poetry] of Chaucer along with other             \nGeoffrey Chaucer:147                         the enduring interest in his [poetry] prior to the arrival of                 \nJohn Gower:55                                                      Gowers [poetry] has had a mixed critical                \nJohn Gower:56                                    as the father of English [poetry] .                                       \nThomas More:22                                   flute and viol and wrote [poetry] .                                       \nEdmund Spenser:6                         despite their differing views on [poetry] .                                       \nEdmund Spenser:22                              place at court through his [poetry] but his next significant publication    \nEdmund Spenser:38                                 many pens and pieces of [poetry] into his grave with many                \nEdmund Spenser:41                              one hundred pounds for his [poetry] .                                       \nEdmund Spenser:84                            scholars have noted that his [poetry] does not rehash tradition but           \n\n\nThe KWIC display reveals patterns that aggregate statistics miss. We can see the actual phrases in which a word appears, the grammatical constructions it participates in, and the semantic contexts that surround it. Is “poetry” typically the subject of a sentence or the object? What verbs and adjectives accompany it?",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "19_textual_data.html#complexity-metrics",
    "href": "19_textual_data.html#complexity-metrics",
    "title": "19  Textual Data",
    "section": "19.8 Complexity Metrics",
    "text": "19.8 Complexity Metrics\nBeyond analyzing content, we can characterize the style of texts using quantitative measures of readability and complexity. These metrics, originally developed to assess the difficulty of reading materials for educational purposes, provide useful descriptive statistics for comparing texts.\nFor example, sentence length is one of the simplest style measures. Longer sentences tend to be more complex and harder to read.\n\n(\n    anno\n    .group_by([c.doc_id, c.sid])\n    .agg(\n        n_tokens = pl.len(),\n        n_words = c.is_alpha.sum()\n    )\n    .group_by(c.doc_id)\n    .agg(\n        mean_sentence_length = c.n_words.mean(),\n        max_sentence_length = c.n_words.max(),\n        n_sentences = pl.len()\n    )\n    .sort(c.mean_sentence_length, descending=True)\n    .head(10)\n)\n\n\nshape: (10, 4)\n\n\n\ndoc_id\nmean_sentence_length\nmax_sentence_length\nn_sentences\n\n\nstr\nf64\nu32\nu32\n\n\n\n\n\"Thomas Malory\"\n28.231214\n83\n173\n\n\n\"Mary Wollstonecraft\"\n25.902878\n76\n278\n\n\n\"Samuel Taylor Coleridge\"\n25.831858\n102\n226\n\n\n\"W. H. Auden\"\n25.502222\n91\n225\n\n\n\"Marie de France\"\n25.481132\n71\n106\n\n\n\"John Stuart Mill\"\n25.358065\n118\n310\n\n\n\"Ben Jonson\"\n25.204724\n82\n254\n\n\n\"Katherine Philipps\"\n25.137931\n68\n58\n\n\n\"Christopher Marlowe\"\n25.019802\n150\n202\n\n\n\"Geoffrey Chaucer\"\n25.018182\n73\n220\n\n\n\n\n\n\nType-token ratio (TTR) measures vocabulary richness: the number of unique words (types) divided by the total number of words (tokens). A higher TTR indicates more varied vocabulary.\n\n(\n    anno\n    .filter(c.is_alpha)\n    .group_by(c.doc_id)\n    .agg(\n        n_tokens = pl.len(),\n        n_types = c.lemma.n_unique()\n    )\n    .with_columns(\n        ttr = c.n_types / c.n_tokens\n    )\n    .sort(c.ttr, descending=True)\n    .head(10)\n)\n\n\nshape: (10, 4)\n\n\n\ndoc_id\nn_tokens\nn_types\nttr\n\n\nstr\nu32\nu32\nf64\n\n\n\n\n\"Rex Warner\"\n726\n371\n0.511019\n\n\n\"Felicia Hemans\"\n1473\n691\n0.469111\n\n\n\"William Langland\"\n908\n407\n0.448238\n\n\n\"Cecil Day-Lewis\"\n1294\n559\n0.431994\n\n\n\"Edward Upward\"\n1065\n458\n0.430047\n\n\n\"John Gower\"\n1439\n606\n0.421126\n\n\n\"Stephen Spender\"\n1798\n756\n0.420467\n\n\n\"Christina Rossetti\"\n1909\n779\n0.408067\n\n\n\"Philip Sidney\"\n1542\n629\n0.407912\n\n\n\"Katherine Philipps\"\n1458\n585\n0.401235\n\n\n\n\n\n\nTTR is sensitive to document length: longer documents naturally have lower TTR because common words get repeated. For fair comparison across documents of different lengths, we can compute TTR on a fixed-size sample of tokens.\nAverage word length correlates with vocabulary sophistication. Documents using more polysyllabic, Latinate vocabulary will have higher average word lengths.\n\n(\n    anno\n    .filter(c.is_alpha)\n    .with_columns(\n        word_length = c.token.str.len_chars()\n    )\n    .group_by(c.doc_id)\n    .agg(\n        mean_word_length = c.word_length.mean()\n    )\n    .sort(c.mean_word_length, descending=True)\n    .head(10)\n)\n\n\nshape: (10, 2)\n\n\n\ndoc_id\nmean_word_length\n\n\nstr\nf64\n\n\n\n\n\"Mary Wollstonecraft\"\n5.123455\n\n\n\"John Milton\"\n5.099143\n\n\n\"William Shakespeare\"\n5.086957\n\n\n\"Rex Warner\"\n5.081267\n\n\n\"Percy Bysshe Shelley\"\n5.07918\n\n\n\"John Locke\"\n5.051898\n\n\n\"Ann Radcliffe\"\n5.04314\n\n\n\"Christopher Marlowe\"\n5.034824\n\n\n\"Jane Austen\"\n5.021875\n\n\n\"William Langland\"\n5.020925\n\n\n\n\n\n\nFinally, function word ratio measures the proportion of grammatical words (articles, prepositions, pronouns) versus content words (nouns, verbs, adjectives). Different genres and styles have characteristic function word profiles.\n\ncontent_pos = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]\n\n(\n    anno\n    .filter(c.is_alpha)\n    .with_columns(\n        is_content = c.upos.is_in(content_pos)\n    )\n    .group_by(c.doc_id)\n    .agg(\n        n_words = pl.len(),\n        n_content = c.is_content.sum()\n    )\n    .with_columns(\n        content_ratio = c.n_content / c.n_words\n    )\n    .sort(c.content_ratio, descending=True)\n    .head(10)\n)\n\n\nshape: (10, 4)\n\n\n\ndoc_id\nn_words\nn_content\ncontent_ratio\n\n\nstr\nu32\nu32\nf64\n\n\n\n\n\"Margaret Cavendish\"\n4737\n2213\n0.467173\n\n\n\"John Stuart Mill\"\n7861\n3664\n0.466098\n\n\n\"Charlotte Smith\"\n3543\n1568\n0.442563\n\n\n\"Joseph Conrad\"\n12546\n5539\n0.441495\n\n\n\"John Locke\"\n5241\n2300\n0.438848\n\n\n\"William Shakespeare\"\n6532\n2812\n0.430496\n\n\n\"Jane Austen\"\n8137\n3494\n0.429397\n\n\n\"Alexander Pope\"\n3256\n1397\n0.429054\n\n\n\"Mary Wollstonecraft\"\n7201\n3068\n0.426052\n\n\n\"George Bernard Shaw\"\n11544\n4896\n0.424116",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "19_textual_data.html#tf-idf",
    "href": "19_textual_data.html#tf-idf",
    "title": "19  Textual Data",
    "section": "19.9 TF-IDF",
    "text": "19.9 TF-IDF\nIn the previous sections, we saw that counting the number of times each token or lemma occurs in a document is a useful way of quickly summarizing the content of a document. This approach can be improved by using a scaled version of the count metric. The issue with raw counts is that they tend to highlight very common words such as “the”, “have”, and “her”. These can be somewhat avoided by removing a pre-compiled set of known common words—often called stopwords—or by doing part of speech filtering. These coarse approaches, however, mostly just move the issue down to a slightly less common set of words that also do not necessarily summarize the contents of each document very well. For example, “publisher” is a frequently used term in many of the documents in this collection due to the subject matter, but that does not mean that it is particularly informative since it occurs in almost every page.\nA common alternative technique is to combine information about the frequency of a word within a document with the frequency of the term across the entire collection. We return here to the importance of how we define a document, which will shape our analysis. Metrics of this form are known as term frequency–inverse document frequency scores (TF-IDF). A common version of TF-IDF computes a score for every combination of term and document by multiplying the term frequency by the logarithm of the inverse document frequency. The logarithm is a function that is used to make sure that counts do not grow too fast. For example, a count of about 1000 is only approximately twice as big on the logarithmic scale as a count of 25, in comparison to being 40 times larger on a linear scale. Mathematically, we define this TF-IDF function using the following formula, where tf gives the term frequency and df gives the document frequency:\n\\[ \\text{tfidf} = \\text{tf} \\times \\log\\left(\\frac{N + 1}{\\text{df} + 1}\\right) + 1 \\]\nHere, N is the total number of documents. This score gives a measurement of how important a term is in describing a document in the context of the other documents. If we select words with the highest TF-IDF score for each document, these should give a good measurement of what terms best describe each document uniquely from the rest of the collection. Note that while the scaling functions given above are popular choices, they are not universal. Other papers and software may make different choices with moderate effects on the output results.\n\ntfidf = DSText.compute_tfidf(anno, min_df=0.1, max_df=0.5)\ntfidf\n\n\nshape: (41_040, 7)\n\n\n\ndoc_id\nlemma\ntf\ntf_norm\ndf_docs\nidf\ntfidf\n\n\nstr\nstr\nu32\nf64\nu32\nf64\nf64\n\n\n\n\n\"Joseph Conrad\"\n\"collaborator\"\n1\n0.00008\n8\n3.133509\n0.00025\n\n\n\"Thomas Malory\"\n\"Chaucer\"\n1\n0.000205\n13\n2.691676\n0.000551\n\n\n\"Louis MacNeice\"\n\"involve\"\n2\n0.000733\n34\n1.775385\n0.001301\n\n\n\"Edward Upward\"\n\"deal\"\n1\n0.000939\n34\n1.775385\n0.001667\n\n\n\"Elizabeth Barrett Browning\"\n\"intensity\"\n1\n0.000243\n11\n2.845827\n0.000691\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"William Shakespeare\"\n\"shift\"\n1\n0.000153\n17\n2.440362\n0.000374\n\n\n\"Samuel Taylor Coleridge\"\n\"connection\"\n1\n0.000171\n31\n1.864997\n0.000319\n\n\n\"Thomas Malory\"\n\"hardly\"\n1\n0.000205\n9\n3.028148\n0.00062\n\n\n\"John Milton\"\n\"discover\"\n1\n0.000124\n30\n1.896746\n0.000236\n\n\n\"Cecil Day-Lewis\"\n\"reach\"\n1\n0.000773\n29\n1.929536\n0.001491\n\n\n\n\n\n\nThen we can combine the top terms from\n\n(\n    tfidf\n    .sort([c.doc_id, c.tfidf], descending=[False, True])\n    .group_by(c.doc_id)\n    .head(10)\n    .group_by(c.doc_id)\n    .agg(top_lemmas = c.lemma.str.join(\"; \"))\n)\n\n\nshape: (75, 2)\n\n\n\ndoc_id\ntop_lemmas\n\n\nstr\nstr\n\n\n\n\n\"William Langland\"\n\"narrator; medieval; passage; p…\n\n\n\"Christopher Isherwood\"\n\"Berlin; Auden; war; fight; art…\n\n\n\"George Herbert\"\n\"Herbert; musical; Temple; East…\n\n\n\"Jonathan Swift\"\n\"Temple; Ireland; Dublin; Jonat…\n\n\n\"Emilia Lanier\"\n\"Lady; Dark; Countess; feminist…\n\n\n…\n…\n\n\n\"William Blake\"\n\"Blake; Catherine; Reynolds; vi…\n\n\n\"Thomas More\"\n\"More; king; execution; Tower; …\n\n\n\"George Orwell\"\n\"Four; communist; Road; Party; …\n\n\n\"Henry Vaughan\"\n\"Herbert; Welsh; prose; spiritu…\n\n\n\"W. H. Auden\"\n\"Auden; partly; York; summer; l…\n\n\n\n\n\n\nThe top TF-IDF terms for each document tend to be proper nouns and domain-specific vocabulary that distinguish one article from another. Compare these to the simple frequency counts from earlier: TF-IDF successfully downweights generic terms like “year” and “work” that appear across all biographies.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "19_textual_data.html#sparse-encoding",
    "href": "19_textual_data.html#sparse-encoding",
    "title": "19  Textual Data",
    "section": "19.10 Sparse Encoding",
    "text": "19.10 Sparse Encoding\nThe analyses so far have kept our data in long format: one row per token or one row per document-term combination. For many applications, it is useful to reshape the data into a document-term matrix (DTM), where each row represents a document, each column represents a term, and each cell contains a count or weight. This transformation converts each document into a numerical vector, enabling us to apply the full toolkit of machine learning and dimensionality reduction techniques.\nWhen we construct a DTM using TF-IDF weights, each document becomes a vector in a very high-dimensional space. If our vocabulary contains 10,000 unique terms, then each document is represented as a point in 10,000-dimensional space. The coordinate along each dimension indicates the TF-IDF weight for that term in that document. Crucially, most of these coordinates are zero: any given document uses only a small fraction of the total vocabulary. A vector where most entries are zero is called a sparse vector, and this representation is therefore known as a sparse embedding.\nMathematically, we can represent document \\(d\\) as a vector \\(\\mathbf{x}_d \\in \\mathbb{R}^V\\) where \\(V\\) is the vocabulary size:\n\\[\n\\mathbf{x}_d = \\begin{bmatrix} \\text{tfidf}(t_1, d) \\\\ \\text{tfidf}(t_2, d) \\\\ \\vdots \\\\ \\text{tfidf}(t_V, d) \\end{bmatrix}\n\\]\nFor a vocabulary of 10,000 terms, this vector has 10,000 entries, but a typical document might have non-zero values for only a few hundred of them.\nThis sparse approach contrasts sharply with the dense embeddings introduced in Chapter 13. Dense embeddings, produced by neural network models, represent each document as a vector in a much lower-dimensional space—typically 384 to 1024 dimensions—where every coordinate carries meaningful information. Rather than having thousands of interpretable dimensions (one per word), dense embeddings compress semantic meaning into a few hundred opaque dimensions learned during training.\nThere are several key differences between these approaches. A sparse TF-IDF vector might have \\(V = 10{,}000\\) dimensions with only 200 non-zero entries. A dense embedding has perhaps \\(d = 384\\) dimensions, all non-zero. We can express the sparsity ratio as \\(\\frac{\\text{non-zero entries}}{V}\\), which is typically below 5% for text data.\nAlso, each dimension of a sparse vector corresponds to a specific word, so we can inspect a document’s representation directly: a high value in the “algorithm” dimension means the document discusses algorithms. Dense embeddings offer no such transparency. Dimension 147 of a dense vector has no human-interpretable meaning; the representation is distributed across all dimensions in ways learned by the neural network. Furthermore, sparse vectors treat each word independently. The terms “car” and “automobile” occupy different dimensions with no built-in notion that they are synonyms. Dense embeddings, trained on massive text corpora, learn that semantically similar words should produce similar vectors. Two documents discussing the same topic in different vocabulary will have similar dense embeddings but potentially very different sparse vectors.\nFinally, sparse vectors are cheap to compute—just count words and apply the TF-IDF formula—but expensive to store naively and limited in semantic power. Dense embeddings require a neural network forward pass, which is more computationally intensive, but the resulting vectors are compact and capture deeper meaning.\nDespite these limitations, sparse embeddings remain valuable for many tasks. They require no pre-trained models, work immediately on any language or domain, and produce interpretable results. For exploratory analysis and visualization, sparse methods often provide sufficient structure to reveal patterns in the data.\nThe code below constructs a sparse document-term matrix from our annotated text, then applies UMAP to project these high-dimensional sparse vectors down to two dimensions for visualization. Documents that use similar vocabulary will cluster together in the resulting plot.\n\n(\n    anno\n    .pipe(\n        DSSklearnText.umap,\n        doc_id=c.doc_id,\n        term_id=c.lemma,\n        n_components=2,\n        min_df=0.01,\n        max_df=0.5\n    )\n    .predict(full=True)\n    .pipe(ggplot, aes(\"dr0\", \"dr1\"))\n    + geom_text(aes(label = \"doc_id\"), size=8)\n)\n\n\n\n\n\n\n\n\nThe min_df and max_df parameters filter the vocabulary: terms appearing in fewer than 1% of documents or more than 50% of documents are excluded. This removes both rare terms (which add noise) and ubiquitous terms (which add no discriminative power), focusing the representation on the most informative middle-frequency vocabulary.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "19_textual_data.html#across-languages",
    "href": "19_textual_data.html#across-languages",
    "title": "19  Textual Data",
    "section": "19.11 Across Languages",
    "text": "19.11 Across Languages\nOne of the reasons that we enjoy using the content of Wikipedia pages as example datasets for textual analysis is that it is possible to get the page text in a large number of different languages. One of the most interesting aspects of textual analysis is that we can apply our techniques to study how differences across languages and cultures affect the way that knowledge is created and distributed.\nLet’s see how our text analysis pipeline can be modified to work with Wikipedia pages from the French version of the site. SpaCy provides models for many different languages:\n\nnlp_fr = spacy.load(\"fr_core_news_sm\")\n\nAnd now, we can annotate the text as follows\n\nanno_fr = DSText.process(docs_fr, nlp_fr)\nanno_fr\n\n\nshape: (203_785, 15)\n\n\n\ndoc_id\nsid\ntid\ntoken\ntoken_with_ws\nlemma\nupos\ntag\nis_alpha\nis_stop\nis_punct\ndep\nhead_idx\nent_type\nent_iob\n\n\nstr\ni64\ni64\nstr\nstr\nstr\nstr\nstr\nbool\nbool\nbool\nstr\ni64\nstr\nstr\n\n\n\n\n\"Marie de France\"\n1\n1\n\"Marie\"\n\"Marie \"\n\"marie\"\n\"NOUN\"\n\"NOUN\"\ntrue\nfalse\nfalse\n\"nsubj\"\n6\n\"PER\"\n\"B\"\n\n\n\"Marie de France\"\n1\n2\n\"de\"\n\"de \"\n\"de\"\n\"ADP\"\n\"ADP\"\ntrue\ntrue\nfalse\n\"case\"\n3\n\"PER\"\n\"I\"\n\n\n\"Marie de France\"\n1\n3\n\"France\"\n\"France \"\n\"France\"\n\"PROPN\"\n\"PROPN\"\ntrue\nfalse\nfalse\n\"nmod\"\n1\n\"PER\"\n\"I\"\n\n\n\"Marie de France\"\n1\n4\n\"est\"\n\"est \"\n\"être\"\n\"AUX\"\n\"AUX\"\ntrue\ntrue\nfalse\n\"cop\"\n6\n\"\"\n\"O\"\n\n\n\"Marie de France\"\n1\n5\n\"une\"\n\"une \"\n\"un\"\n\"DET\"\n\"DET\"\ntrue\ntrue\nfalse\n\"det\"\n6\n\"\"\n\"O\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Seamus Heaney\"\n78\n5\n\"multitude\"\n\"multitude \"\n\"multitude\"\n\"NOUN\"\n\"NOUN\"\ntrue\nfalse\nfalse\n\"obj\"\n3\n\"\"\n\"O\"\n\n\n\"Seamus Heaney\"\n78\n6\n\"de\"\n\"de \"\n\"de\"\n\"ADP\"\n\"ADP\"\ntrue\ntrue\nfalse\n\"case\"\n7\n\"\"\n\"O\"\n\n\n\"Seamus Heaney\"\n78\n7\n\"doctorats\"\n\"doctorats \"\n\"doctorat\"\n\"NOUN\"\n\"NOUN\"\ntrue\nfalse\nfalse\n\"nmod\"\n5\n\"\"\n\"O\"\n\n\n\"Seamus Heaney\"\n78\n8\n\"honoris\"\n\"honoris \"\n\"honoris\"\n\"VERB\"\n\"VERB\"\ntrue\nfalse\nfalse\n\"acl\"\n7\n\"\"\n\"O\"\n\n\n\"Seamus Heaney\"\n78\n9\n\"causa\"\n\"causa\"\n\"causa\"\n\"NOUN\"\n\"NOUN\"\ntrue\nfalse\nfalse\n\"obj\"\n3\n\"\"\n\"O\"\n\n\n\n\n\n\nFrench and English have different grammatical structures that will be reflected in part-of-speech frequencies. French uses more determiners (articles), while English may use more auxiliary verbs. These differences are linguistic rather than content-based, but they affect how we should interpret comparative analyses.\nCross-linguistic analysis opens up fascinating questions about how knowledge is organized and transmitted across cultures. The same historical figure may be framed differently depending on the national perspective of the Wikipedia editors. Events that are central to one country’s narrative may be peripheral to another’s. Textual analysis provides the tools to investigate these questions systematically.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "19_textual_data.html#references",
    "href": "19_textual_data.html#references",
    "title": "19  Textual Data",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "20_image_data.html",
    "href": "20_image_data.html",
    "title": "20  Image Data",
    "section": "",
    "text": "20.1 Setup\nLoad all of the modules and datasets needed for the chapter. In addition to the standard tools, we will use OpenCV (cv2) for image processing, PIL for image display, and the Ultralytics library for accessing pre-trained YOLO models.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nfrom PIL import Image\nimport cv2\nfrom ultralytics import YOLO\n\nbirds = pl.read_parquet(\"data/birds10.parquet\")\nbirds_bbox = pl.read_csv(\"data/birds_1000.csv\")\nfsac = pl.read_csv(\"data/fsac.csv\").select(c.filepath, c.photographer)",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "20_image_data.html#introduction",
    "href": "20_image_data.html#introduction",
    "title": "20  Image Data",
    "section": "20.2 Introduction",
    "text": "20.2 Introduction\nImages are among the most ubiquitous forms of data in the modern world. Every day, billions of photographs are captured on smartphones, satellites orbit the Earth generating terabytes of imagery, medical scanners produce detailed views of the human body, and security cameras record countless hours of video. The ability to extract meaningful information from these images has become one of the most important capabilities in data science.\nAt their core, digital images are simply arrays of numbers. Each pixel in an image stores numerical values representing color or intensity. Yet from these raw numbers emerge faces, objects, scenes, and stories. The challenge of image analysis lies in bridging this gap between low-level pixel values and high-level semantic meaning. How do we go from a grid of numbers to understanding that an image contains a bird, or that a person in a photograph is smiling, or that a tumor is present in a medical scan?\nIn this chapter, we explore several approaches to extracting information from images. We begin with basic properties that can be computed directly from pixel values, including brightness and color distributions. These simple features, while limited in their semantic power, provide useful descriptors for large collections of images and can reveal interesting patterns in visual datasets. We then turn to more sophisticated computer vision techniques that leverage deep learning models trained on millions of images. These models can detect objects, segment regions, and identify human poses with remarkable accuracy.\nThe progression in this chapter mirrors the broader evolution of computer vision as a field. Early approaches relied on hand-crafted features computed from pixel statistics. Modern methods use neural networks to learn relevant features directly from data. Both perspectives remain valuable: simple pixel-based features are interpretable, fast to compute, and often sufficient for exploratory analysis, while deep learning models offer unprecedented accuracy for complex recognition tasks. Understanding both approaches equips you to choose the right tool for each analytical situation.\nNote that in addition to the task-specific models we explore here, images can also be converted into dense vector embeddings using the transfer learning techniques described in Chapter 15. These embeddings represent images as points in a high-dimensional space where visually or semantically similar images are close together. Once you have embeddings, all the familiar tools apply: you can train classifiers to predict categories, use dimensionality reduction techniques like PCA or UMAP to visualize collections, or apply clustering algorithms to discover groups of related images. The embedding approach is particularly powerful when you have domain-specific categories that don’t match the labels in pre-trained models.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "20_image_data.html#image-arrays",
    "href": "20_image_data.html#image-arrays",
    "title": "20  Image Data",
    "section": "20.3 Image Arrays",
    "text": "20.3 Image Arrays\nWe will work with the birds image dataset, which contains photographs of ten species of birds. Each image has been preprocessed to a standard size and format, making it suitable for comparative analysis. The dataset includes species with distinctive coloration, from the brilliant yellow of canaries to the iridescent blues and greens of peacocks.\n\nbirds\n\n\nshape: (1_555, 5)\n\n\n\nlabel\nfilepath\nindex\nvit\nsiglip\n\n\nstr\nstr\nstr\nlist[f64]\nlist[f64]\n\n\n\n\n\"canary\"\n\"media/birds10/00000.png\"\n\"test\"\n[0.017947, -0.0305, … 0.003526]\n[-0.00611, -0.042975, … -0.031687]\n\n\n\"canary\"\n\"media/birds10/00001.png\"\n\"train\"\n[0.024804, -0.045255, … -0.007233]\n[-0.033767, -0.011978, … -0.020352]\n\n\n\"canary\"\n\"media/birds10/00002.png\"\n\"train\"\n[0.050587, -0.024486, … 0.029895]\n[-0.033664, -0.008117, … -0.01725]\n\n\n\"canary\"\n\"media/birds10/00003.png\"\n\"train\"\n[0.047036, -0.038993, … -0.008446]\n[-0.010029, -0.018192, … -0.009869]\n\n\n\"canary\"\n\"media/birds10/00004.png\"\n\"train\"\n[0.036349, -0.02734, … -0.018185]\n[-0.027327, 0.003568, … -0.033407]\n\n\n…\n…\n…\n…\n…\n\n\n\"swallow\"\n\"media/birds10/01550.png\"\n\"train\"\n[-0.022461, -0.025098, … -0.061945]\n[-0.022029, -0.008476, … -0.003879]\n\n\n\"swallow\"\n\"media/birds10/01551.png\"\n\"train\"\n[-0.000212, -0.003448, … -0.058042]\n[-0.02476, -0.016369, … 0.003875]\n\n\n\"swallow\"\n\"media/birds10/01552.png\"\n\"train\"\n[-0.012531, -0.006788, … -0.047077]\n[-0.022153, 0.00765, … -0.011152]\n\n\n\"swallow\"\n\"media/birds10/01553.png\"\n\"test\"\n[-0.007587, -0.053535, … -0.046395]\n[-0.005022, 0.00878, … -0.017564]\n\n\n\"swallow\"\n\"media/birds10/01554.png\"\n\"train\"\n[-0.01325, -0.032453, … -0.050751]\n[-0.015117, -0.00685, … -0.029756]\n\n\n\n\n\n\nThe dataset contains a filepath column pointing to each image file and a label column identifying the species. This structure, with metadata in a table and actual image files stored separately, is a common pattern for working with image datasets. It allows us to use familiar tabular data tools for filtering, grouping, and summarizing while keeping the image files in their native format.\nTo work with an image programmatically, we need to read it into Python as a numerical array. The OpenCV library provides the imread function for this purpose. Let’s load the first image in our dataset.\n\nimg = cv2.imread(birds.select(pl.col(\"filepath\").get(0)).item())\nimg\n\narray([[[ 18,  20,  21],\n        [ 22,  24,  25],\n        [ 23,  25,  26],\n        ...,\n        [ 48,  48,  54],\n        [ 48,  47,  56],\n        [ 47,  46,  55]],\n\n       [[ 18,  20,  21],\n        [ 20,  22,  23],\n        [ 20,  22,  23],\n        ...,\n        [ 50,  50,  56],\n        [ 49,  48,  57],\n        [ 47,  46,  55]],\n\n       [[ 12,  15,  19],\n        [ 11,  14,  18],\n        [ 12,  15,  19],\n        ...,\n        [ 55,  53,  59],\n        [ 53,  51,  57],\n        [ 52,  50,  56]],\n\n       ...,\n\n       [[133, 148, 180],\n        [147, 159, 187],\n        [126, 130, 149],\n        ...,\n        [122,  98,  92],\n        [104,  81,  79],\n        [ 87,  66,  65]],\n\n       [[116, 133, 166],\n        [138, 152, 181],\n        [133, 140, 165],\n        ...,\n        [113,  91,  86],\n        [106,  87,  90],\n        [ 87,  70,  74]],\n\n       [[ 89, 108, 141],\n        [127, 142, 174],\n        [140, 149, 176],\n        ...,\n        [103,  82,  80],\n        [ 91,  74,  78],\n        [122, 107, 115]]], shape=(224, 224, 3), dtype=uint8)\n\n\nWhat we see is a NumPy array filled with numbers. Each number represents the intensity of a color channel at a specific pixel location. The values range from 0 (no intensity) to 255 (maximum intensity), using 8 bits per channel, which is the standard format for most digital images.\nThe shape of this array reveals the structure of the image data.\n\nimg.shape\n\n(224, 224, 3)\n\n\nThe three dimensions correspond to height (number of rows of pixels), width (number of columns), and color channels. Most color images use three channels: red, green, and blue (RGB). However, OpenCV reads images in BGR order (blue, green, red), a historical convention from early computer vision libraries. This ordering rarely matters for analysis but becomes important when displaying images or converting between color spaces.\nWe can examine a small region of the array to see the actual pixel values. Here are the values for a 4×4 block of pixels in the upper-left corner.\n\nimg[:4, :4, :]\n\narray([[[18, 20, 21],\n        [22, 24, 25],\n        [23, 25, 26],\n        [22, 24, 25]],\n\n       [[18, 20, 21],\n        [20, 22, 23],\n        [20, 22, 23],\n        [19, 21, 22]],\n\n       [[12, 15, 19],\n        [11, 14, 18],\n        [12, 15, 19],\n        [15, 18, 22]],\n\n       [[17, 20, 24],\n        [13, 16, 20],\n        [13, 16, 20],\n        [14, 17, 21]]], dtype=uint8)\n\n\nEach pixel is represented by three values, one for each color channel. A pixel with values [255, 255, 255] would be pure white, while [0, 0, 0] would be pure black. Values like [255, 0, 0] represent pure blue (remember, OpenCV uses BGR ordering), [0, 255, 0] is green, and [0, 0, 255] is red.\nOne of the simplest statistics we can compute from an image is the mean of all pixel values.\n\nimg.mean()\n\nnp.float64(119.86716757015306)\n\n\nThis single number summarizes the overall intensity of the image. Higher values indicate brighter images (more pixels closer to 255), while lower values indicate darker images (more pixels closer to 0). While crude, this measure provides a starting point for understanding the visual characteristics of image collections.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "20_image_data.html#brightness",
    "href": "20_image_data.html#brightness",
    "title": "20  Image Data",
    "section": "20.4 Brightness",
    "text": "20.4 Brightness\nLet’s compute the mean brightness for every image in our bird dataset. This requires iterating through all images, loading each one, and computing its mean pixel value. We store the results and add them as a new column to our DataFrame.\n\nresults = []\n\nfor row in birds.iter_rows(named=True):\n    img = cv2.imread(row[\"filepath\"])\n    results.append(img.mean())\n\nbirds = birds.with_columns(brightness = pl.Series(results))\nbirds\n\n\nshape: (1_555, 6)\n\n\n\nlabel\nfilepath\nindex\nvit\nsiglip\nbrightness\n\n\nstr\nstr\nstr\nlist[f64]\nlist[f64]\nf64\n\n\n\n\n\"canary\"\n\"media/birds10/00000.png\"\n\"test\"\n[0.017947, -0.0305, … 0.003526]\n[-0.00611, -0.042975, … -0.031687]\n119.867168\n\n\n\"canary\"\n\"media/birds10/00001.png\"\n\"train\"\n[0.024804, -0.045255, … -0.007233]\n[-0.033767, -0.011978, … -0.020352]\n135.112989\n\n\n\"canary\"\n\"media/birds10/00002.png\"\n\"train\"\n[0.050587, -0.024486, … 0.029895]\n[-0.033664, -0.008117, … -0.01725]\n97.398424\n\n\n\"canary\"\n\"media/birds10/00003.png\"\n\"train\"\n[0.047036, -0.038993, … -0.008446]\n[-0.010029, -0.018192, … -0.009869]\n97.469866\n\n\n\"canary\"\n\"media/birds10/00004.png\"\n\"train\"\n[0.036349, -0.02734, … -0.018185]\n[-0.027327, 0.003568, … -0.033407]\n144.174758\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"swallow\"\n\"media/birds10/01550.png\"\n\"train\"\n[-0.022461, -0.025098, … -0.061945]\n[-0.022029, -0.008476, … -0.003879]\n119.851436\n\n\n\"swallow\"\n\"media/birds10/01551.png\"\n\"train\"\n[-0.000212, -0.003448, … -0.058042]\n[-0.02476, -0.016369, … 0.003875]\n151.026155\n\n\n\"swallow\"\n\"media/birds10/01552.png\"\n\"train\"\n[-0.012531, -0.006788, … -0.047077]\n[-0.022153, 0.00765, … -0.011152]\n156.078922\n\n\n\"swallow\"\n\"media/birds10/01553.png\"\n\"test\"\n[-0.007587, -0.053535, … -0.046395]\n[-0.005022, 0.00878, … -0.017564]\n118.326265\n\n\n\"swallow\"\n\"media/birds10/01554.png\"\n\"train\"\n[-0.01325, -0.032453, … -0.050751]\n[-0.015117, -0.00685, … -0.029756]\n156.02357\n\n\n\n\n\n\nNow we have a brightness value for each image. What can this simple statistic tell us about our photographs? Let’s examine the darkest images in the collection.\n\n(\n    birds\n    .sort(c.brightness)\n    .head(12)\n    .pipe(DSImage.plot_image_grid, ncol=4)\n)\n\n\n\n\n\n\n\n\nWe see that the darkest images tend to have black backgrounds. This makes sense: a large area of pure black pixels (value 0) dramatically reduces the mean. The birds themselves may be brightly colored, but the dark backgrounds dominate the overall brightness calculation.\nThe brightest images show the opposite pattern.\n\n(\n    birds\n    .sort(c.brightness, descending=True)\n    .head(12)\n    .pipe(DSImage.plot_image_grid, ncol=4)\n)\n\n\n\n\n\n\n\n\nThese images feature white or light-colored backgrounds, which push the mean toward higher values. This observation reveals an important limitation of simple pixel statistics: they capture properties of the entire image, including background, lighting conditions, and photographic style, not just the subject we care about. A dark bird photographed against a white background will have a higher brightness score than the same bird against a dark background.\nDespite this limitation, brightness can reveal interesting patterns across categories. Here are the distributions of brightness by species.\n\n(\n    birds\n    .pipe(ggplot, aes(\"reorder(label, brightness)\", \"brightness\"))\n    + geom_boxplot()\n)\n\n\n\n\n\n\n\n\nOstriches have brighter images on average, followed by ducks and robins. Peacock images are the darkest. Why might this be? Consider how each species is typically photographed. Ostriches are often captured outdoors in bright savanna settings, while peacocks with their dark iridescent plumage are sometimes photographed against dramatic dark backgrounds that emphasize their colorful displays. The brightness distribution reflects not just the birds themselves but the entire photographic context.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "20_image_data.html#hue-saturation-and-value",
    "href": "20_image_data.html#hue-saturation-and-value",
    "title": "20  Image Data",
    "section": "20.5 Hue, Saturation, and Value",
    "text": "20.5 Hue, Saturation, and Value\nWhile brightness provides a single summary statistic, the RGB color model used by digital images is not ideal for describing color as humans perceive it. The HSV (Hue, Saturation, Value) color model offers a more intuitive representation that separates color information into three distinct components.\nHue represents the pure color itself, independent of how light or vivid it appears. Hue is measured as an angle around a color wheel, typically ranging from 0° to 360°. Red appears at 0° (and wraps around to 360°), yellow at 60°, green at 120°, cyan at 180°, blue at 240°, and magenta at 300°. This circular representation explains why red and magenta appear adjacent: they are neighbors on the color wheel.\nSaturation measures the purity or intensity of a color. A fully saturated color contains no gray: it is vivid and intense. As saturation decreases, the color becomes more washed out, eventually becoming a pure gray at zero saturation. Saturation ranges from 0 (completely desaturated, gray) to 1 (fully saturated, pure color).\nValue (also called brightness or lightness in related color models) indicates how light or dark the color is. A value of 0 produces black regardless of hue or saturation, while a value of 1 produces the brightest possible version of that hue and saturation combination.\nThe mathematical conversion from RGB to HSV proceeds as follows. Given RGB values normalized to the range [0, 1], we first compute:\n\\[\n\\begin{aligned}\nM &= \\max(R, G, B) \\\\\nm &= \\min(R, G, B) \\\\\nC &= M - m\n\\end{aligned}\n\\]\nThe value \\(M\\) is the maximum of the three channels, \\(m\\) is the minimum, and \\(C\\) (chroma) measures the range. The HSV components are then calculated as:\n\\[\nV = M\n\\]\n\\[\nS = \\begin{cases}\n0 & \\text{if } V = 0 \\\\\n\\frac{C}{V} & \\text{otherwise}\n\\end{cases}\n\\]\n\\[\nH = \\begin{cases}\n0° & \\text{if } C = 0 \\\\\n60° \\times \\frac{G - B}{C} \\mod 360° & \\text{if } M = R \\\\\n60° \\times \\left(\\frac{B - R}{C} + 2\\right) & \\text{if } M = G \\\\\n60° \\times \\left(\\frac{R - G}{C} + 4\\right) & \\text{if } M = B\n\\end{cases}\n\\]\nThe value component is simply the maximum RGB value. Saturation measures how much the color differs from gray by comparing the range of RGB values to the maximum. Hue is computed by determining which RGB component is dominant and calculating the position around the color wheel.\nOpenCV provides built-in functions for color space conversion. Here is how to convert an image from BGR to HSV.\n\nimg = cv2.imread(birds.select(pl.col(\"filepath\").get(0)).item())\n\nimg_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\nimg_hsv.shape\n\n(224, 224, 3)\n\n\nThe resulting array has the same shape as the original, but now the three channels represent hue, saturation, and value instead of blue, green, and red. Note that OpenCV scales hue to the range [0, 179] (to fit in an 8-bit value while covering the full 360° range) and saturation and value to [0, 255].",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "20_image_data.html#colors",
    "href": "20_image_data.html#colors",
    "title": "20  Image Data",
    "section": "20.6 Colors",
    "text": "20.6 Colors\nWith images represented in HSV color space, we can classify pixels by their dominant color. The hue value tells us where each pixel falls on the color wheel, allowing us to count how many pixels are predominantly red, orange, yellow, green, and so on.\nOur helper method DSImage.compute_colors takes an HSV image and returns a dictionary with the proportion of pixels falling into each color category. It bins the hue values into segments corresponding to common color names and also accounts for the saturation and value (very dark or desaturated pixels are classified separately as black, white, or gray).\n\nDSImage.compute_colors(img_hsv)\n\n{'red': np.float64(6.048708545918367),\n 'orange': np.float64(8.326690051020408),\n 'yellow': np.float64(9.394929846938775),\n 'green': np.float64(0.06377551020408163),\n 'cyan': np.float64(0.005978954081632653),\n 'blue': np.float64(1.0981345663265305),\n 'purple': np.float64(0.007971938775510204),\n 'magenta': np.float64(0.04185267857142857),\n 'neutral': np.float64(75.01195790816327)}\n\n\nThe output shows what fraction of pixels in this image fall into each color category. The proportions sum to 1, giving us a complete description of the color distribution.\nWe will cycle through the entire DataFrame and compute these color proportions for all images.\n\nresults = []\n\nfor row in birds.iter_rows(named=True):\n    img = cv2.imread(row[\"filepath\"])\n    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    results.append(DSImage.compute_colors(img_hsv))\n\n\nresults = pl.DataFrame(results)\nbirds = pl.concat([birds, results], how=\"horizontal\")\nbirds\n\n\nshape: (1_555, 15)\n\n\n\nlabel\nfilepath\nindex\nvit\nsiglip\nbrightness\nred\norange\nyellow\ngreen\ncyan\nblue\npurple\nmagenta\nneutral\n\n\nstr\nstr\nstr\nlist[f64]\nlist[f64]\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"canary\"\n\"media/birds10/00000.png\"\n\"test\"\n[0.017947, -0.0305, … 0.003526]\n[-0.00611, -0.042975, … -0.031687]\n119.867168\n6.048709\n8.32669\n9.39493\n0.063776\n0.005979\n1.098135\n0.007972\n0.041853\n75.011958\n\n\n\"canary\"\n\"media/birds10/00001.png\"\n\"train\"\n[0.024804, -0.045255, … -0.007233]\n[-0.033767, -0.011978, … -0.020352]\n135.112989\n8.968431\n23.794244\n6.806043\n1.7578125\n1.480788\n0.0\n0.0\n0.0\n57.192682\n\n\n\"canary\"\n\"media/birds10/00002.png\"\n\"train\"\n[0.050587, -0.024486, … 0.029895]\n[-0.033664, -0.008117, … -0.01725]\n97.398424\n0.01993\n30.29536\n52.800143\n0.049825\n0.009965\n0.932717\n0.0\n0.0\n15.89206\n\n\n\"canary\"\n\"media/birds10/00003.png\"\n\"train\"\n[0.047036, -0.038993, … -0.008446]\n[-0.010029, -0.018192, … -0.009869]\n97.469866\n0.0\n26.546556\n64.339525\n0.007972\n0.0\n0.0\n0.0\n0.0\n9.105947\n\n\n\"canary\"\n\"media/birds10/00004.png\"\n\"train\"\n[0.036349, -0.02734, … -0.018185]\n[-0.027327, 0.003568, … -0.033407]\n144.174758\n0.288983\n96.593989\n0.454401\n0.0\n0.0\n0.0\n0.0\n0.0\n2.662628\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"swallow\"\n\"media/birds10/01550.png\"\n\"train\"\n[-0.022461, -0.025098, … -0.061945]\n[-0.022029, -0.008476, … -0.003879]\n119.851436\n0.061783\n14.807876\n0.655692\n0.063776\n0.121572\n57.553412\n0.0\n0.0\n26.73589\n\n\n\"swallow\"\n\"media/birds10/01551.png\"\n\"train\"\n[-0.000212, -0.003448, … -0.058042]\n[-0.02476, -0.016369, … 0.003875]\n151.026155\n0.691566\n2.451371\n24.447943\n0.089684\n0.187341\n3.386081\n0.0\n0.027902\n68.718112\n\n\n\"swallow\"\n\"media/birds10/01552.png\"\n\"train\"\n[-0.012531, -0.006788, … -0.047077]\n[-0.022153, 0.00765, … -0.011152]\n156.078922\n0.185348\n11.820392\n5.369101\n0.053811\n0.159439\n64.845743\n0.0\n0.0\n17.566167\n\n\n\"swallow\"\n\"media/birds10/01553.png\"\n\"test\"\n[-0.007587, -0.053535, … -0.046395]\n[-0.005022, 0.00878, … -0.017564]\n118.326265\n0.007972\n0.239158\n0.159439\n0.819117\n0.185348\n5.624203\n0.0\n0.0\n92.964764\n\n\n\"swallow\"\n\"media/birds10/01554.png\"\n\"train\"\n[-0.01325, -0.032453, … -0.050751]\n[-0.015117, -0.00685, … -0.029756]\n156.02357\n1.052296\n20.938297\n0.187341\n0.155453\n0.075733\n0.277025\n0.0\n0.001993\n77.311862\n\n\n\n\n\n\nNow our dataset includes columns for each color category, enabling systematic analysis of color distributions across species. Let’s examine which images contain the most yellow.\n\n(\n    birds\n    .sort(c.yellow, descending=True)\n    .head(12)\n    .pipe(DSImage.plot_image_grid, ncol=4)\n)\n\n\n\n\n\n\n\n\nThe images with the highest yellow proportions include many hummingbirds photographed near yellow-green foliage, as well as the expected yellow birds. This demonstrates how color analysis captures not just the subject but the entire scene.\nWe can compare the yellow content across species systematically.\n\n(\n    birds\n    .pipe(ggplot, aes(\"reorder(label, yellow)\", \"yellow\"))\n    + geom_boxplot()\n)\n\n\n\n\n\n\n\n\nCanaries, as expected, have the highest yellow content on average. Their bright yellow plumage dominates the images. Other species show less yellow, with the distribution reflecting both the birds’ coloration and their typical photographic environments.\nPeacocks, known for their brilliant blue-green displays, should show high values in the cyan range.\n\n(\n    birds\n    .pipe(ggplot, aes(\"reorder(label, cyan)\", \"cyan\"))\n    + geom_boxplot()\n)\n\n\n\n\n\n\n\n\nIndeed, peacocks lead in cyan content, reflecting their characteristic iridescent feathers that shimmer between blue and green. Parrots also show substantial cyan, while species like robins and canaries have minimal cyan in their typical coloration.\nThese color features, while simple to compute, provide useful descriptors for organizing and exploring image collections. They can identify outliers (an unusually red canary might indicate a different subspecies or unusual lighting), reveal patterns across categories, and serve as features for downstream machine learning tasks.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "20_image_data.html#object-detection",
    "href": "20_image_data.html#object-detection",
    "title": "20  Image Data",
    "section": "20.7 Object Detection",
    "text": "20.7 Object Detection\nIn Chapter 15 we saw how to use pre-trained deep learning models for image-level predictions, classifying entire images into categories. Object detection extends this capability by identifying and locating multiple objects within a single image. Rather than asking “what is in this image?”, object detection asks “what objects are in this image, and where are they?”\nModern object detection models output a set of bounding boxes, each consisting of four coordinates that define a rectangle enclosing a detected object, along with a class label and a confidence score. The model might report: “there is a person at coordinates (100, 50) to (200, 300) with 95% confidence, and a dog at (250, 100) to (350, 250) with 87% confidence.”\nThe YOLO (You Only Look Once) family of models represents the state of the art in real-time object detection. These models process entire images in a single forward pass through the network, making them remarkably fast while maintaining high accuracy. The architecture divides the image into a grid, with each cell responsible for predicting objects centered in that cell. This design enables the model to reason globally about the image while making localized predictions.\nTraining an object detection model requires labeled data where humans have drawn bounding boxes around objects and assigned class labels. The loss function combines several components:\n\\[\n\\mathcal{L} = \\lambda_{\\text{coord}} \\mathcal{L}_{\\text{box}} + \\lambda_{\\text{obj}} \\mathcal{L}_{\\text{obj}} + \\lambda_{\\text{cls}} \\mathcal{L}_{\\text{cls}}\n\\]\nThe box loss \\(\\mathcal{L}_{\\text{box}}\\) penalizes errors in the predicted bounding box coordinates. Modern implementations often use Complete Intersection over Union (CIoU) loss, which considers the overlap between predicted and ground-truth boxes along with the distance between their centers and aspect ratio consistency:\n\\[\n\\mathcal{L}_{\\text{CIoU}} = 1 - \\text{IoU} + \\frac{\\rho^2(b, b^{gt})}{c^2} + \\alpha v\n\\]\nwhere IoU is the intersection over union of the boxes, \\(\\rho(b, b^{gt})\\) is the Euclidean distance between box centers, \\(c\\) is the diagonal length of the smallest enclosing box, and \\(v\\) measures aspect ratio consistency.\nThe objectness loss \\(\\mathcal{L}_{\\text{obj}}\\) trains the model to predict whether each grid cell contains an object. The classification loss \\(\\mathcal{L}_{\\text{cls}}\\) trains the model to correctly identify the class of detected objects. The \\(\\lambda\\) coefficients balance these components during training.\nLet’s load a pre-trained YOLO model and apply it to historical photographs.\n\nmodel = YOLO(\"yolo11n.pt\")\n\nFor this analysis, we will use a collection of color photographs from the FSA-OWI (Farm Security Administration - Office of War Information) project, a remarkable documentary photography initiative from the 1930s and 1940s. These images captured American life during the Great Depression and World War II.\n\nfsac\n\n\nshape: (500, 2)\n\n\n\nfilepath\nphotographer\n\n\nstr\nstr\n\n\n\n\n\"media/fsac/1a35266v.jpg\"\n\"Alfred T. Palmer\"\n\n\n\"media/fsac/1a34940v.jpg\"\n\"Howard R. Hollem\"\n\n\n\"media/fsac/1a34143v.jpg\"\n\"Russell Lee\"\n\n\n\"media/fsac/1a35375v.jpg\"\n\"Alfred T. Palmer\"\n\n\n\"media/fsac/1a34758v.jpg\"\n\"Jack Delano\"\n\n\n…\n…\n\n\n\"media/fsac/1a34359v.jpg\"\n\"Marion Post Wolcott\"\n\n\n\"media/fsac/1a34893v.jpg\"\n\"Howard R. Hollem\"\n\n\n\"media/fsac/1a34100v.jpg\"\n\"Russell Lee\"\n\n\n\"media/fsac/1a35045v.jpg\"\n\"Howard Liberman\"\n\n\n\"media/fsac/1a34863v.jpg\"\n\"Howard R. Hollem\"\n\n\n\n\n\n\nHere is an example of running object detection on one of these historical photographs.\n\npred = model.predict(\"media/fsac/1a35210v.jpg\")\n\n\nimage 1/1 /Users/admin/gh/fds-py/media/fsac/1a35210v.jpg: 544x640 3 persons, 33.7ms\nSpeed: 1.0ms preprocess, 33.7ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 640)\n\n\nThe model returns predictions that include bounding boxes, class labels, and confidence scores. We can visualize the detections by overlaying them on the original image.\n\nrgb = cv2.cvtColor(pred[0].plot(), cv2.COLOR_BGR2RGB)\npil_img = Image.fromarray(rgb)\npil_img\n\n\n\n\n\n\n\n\nThe visualization shows bounding boxes around detected objects, each labeled with the predicted class and confidence score. The model has been trained on the COCO dataset, which includes 80 common object categories such as person, car, dog, chair, and bicycle.\nLet’s systematically count the number of people detected in each image across our collection.\n\nresults = []\n\nfor row in fsac.iter_rows(named=True):\n    preds = model.predict(row[\"filepath\"], verbose=False)[0]\n\n    if preds.boxes is None:\n        results.append(0)\n        continue\n\n    cls_ids = preds.boxes.cls.tolist()\n    names = preds.names\n    results.append(sum(names[int(c)] == \"person\" for c in cls_ids))\n\n\nfsac = fsac.with_columns(people = pl.Series(results))\nfsac\n\n\nshape: (500, 3)\n\n\n\nfilepath\nphotographer\npeople\n\n\nstr\nstr\ni64\n\n\n\n\n\"media/fsac/1a35266v.jpg\"\n\"Alfred T. Palmer\"\n0\n\n\n\"media/fsac/1a34940v.jpg\"\n\"Howard R. Hollem\"\n0\n\n\n\"media/fsac/1a34143v.jpg\"\n\"Russell Lee\"\n10\n\n\n\"media/fsac/1a35375v.jpg\"\n\"Alfred T. Palmer\"\n1\n\n\n\"media/fsac/1a34758v.jpg\"\n\"Jack Delano\"\n0\n\n\n…\n…\n…\n\n\n\"media/fsac/1a34359v.jpg\"\n\"Marion Post Wolcott\"\n8\n\n\n\"media/fsac/1a34893v.jpg\"\n\"Howard R. Hollem\"\n1\n\n\n\"media/fsac/1a34100v.jpg\"\n\"Russell Lee\"\n1\n\n\n\"media/fsac/1a35045v.jpg\"\n\"Howard Liberman\"\n0\n\n\n\"media/fsac/1a34863v.jpg\"\n\"Howard R. Hollem\"\n0\n\n\n\n\n\n\nWe can examine the images with the most detected people.\n\n(\n    fsac\n    .sort(c.people, descending=True)\n    .head(8)\n    .pipe(DSImage.plot_image_grid, ncol=4, label_name=\"people\")\n)\n\n\n\n\n\n\n\n\nThe model successfully identifies crowd scenes, though the exact count becomes less reliable as the number of people increases. In dense crowds, overlapping individuals and partial occlusions make precise counting challenging. Nevertheless, the detection provides a useful proxy for the scale of human presence in each photograph.\nThe number of people varies considerably by photographer, reflecting different documentary styles and subject matter choices.\n\n(\n    fsac\n    .pipe(ggplot, aes(\"reorder(photographer, people)\", \"people\"))\n    + geom_boxplot()\n    + coord_flip()\n)\n\n\n\n\n\n\n\n\nSome photographers specialized in intimate portraits with one or two subjects, while others captured street scenes and public gatherings. These patterns emerge clearly from the automated analysis.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "20_image_data.html#segmentation",
    "href": "20_image_data.html#segmentation",
    "title": "20  Image Data",
    "section": "20.8 Segmentation",
    "text": "20.8 Segmentation\nObject detection tells us where objects are, but treats each detection as a simple rectangle. Instance segmentation goes further by identifying the exact pixels that belong to each object. Instead of a bounding box, segmentation produces a mask—a binary image where pixels belonging to the object are marked as 1 and background pixels as 0.\nSegmentation models build on object detection architectures by adding a mask prediction branch. For each detected object, the model predicts not just a bounding box but also a pixel-wise mask within that box. The training loss includes an additional component for mask accuracy:\n\\[\n\\mathcal{L}_{\\text{mask}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n\\]\nThis is the binary cross-entropy loss computed over all \\(N\\) pixels in the mask region, where \\(y_i\\) is the ground-truth label (1 if the pixel belongs to the object, 0 otherwise) and \\(\\hat{y}_i\\) is the predicted probability. The loss penalizes both false positives (predicting object when the pixel is background) and false negatives (predicting background when the pixel is object).\nYOLO includes segmentation variants that maintain real-time performance while producing pixel-accurate masks.\n\nmodel = YOLO(\"yolo11n-seg.pt\")\npred = model.predict(\"media/fsac/1a35210v.jpg\")\n\nrgb = cv2.cvtColor(pred[0].plot(), cv2.COLOR_BGR2RGB)\npil_img = Image.fromarray(rgb)\npil_img\n\n\nimage 1/1 /Users/admin/gh/fds-py/media/fsac/1a35210v.jpg: 544x640 3 persons, 45.7ms\nSpeed: 0.7ms preprocess, 45.7ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 640)\n\n\n\n\n\n\n\n\n\nThe visualization now shows colored masks overlaid on detected objects, precisely delineating their boundaries rather than just enclosing them in rectangles. This pixel-level precision enables more nuanced analysis of image content.\nWe can use segmentation to compute what percentage of each image is occupied by people, providing a measure of how prominently human figures feature in the composition.\n\nresults = []\nfor row in fsac.iter_rows(named=True):\n    preds = model.predict(row[\"filepath\"], verbose=False)[0]\n\n    cls_ids = preds.boxes.cls.to(\"cpu\").numpy().astype(int)\n    names = preds.names\n\n    person_idx = np.where(np.array([names[c] == \"person\" for c in cls_ids]))[0]\n    if person_idx.size == 0:\n        results.append(0.0)\n        continue\n\n    masks = preds.masks.data[person_idx]\n    union = masks.any(dim=0)\n\n    coverage = union.float().mean().item()\n    results.append(coverage * 100)\n\n\nfsac = fsac.with_columns(people_prop=pl.Series(results))\nfsac\n\n\nshape: (500, 4)\n\n\n\nfilepath\nphotographer\npeople\npeople_prop\n\n\nstr\nstr\ni64\nf64\n\n\n\n\n\"media/fsac/1a35266v.jpg\"\n\"Alfred T. Palmer\"\n0\n0.0\n\n\n\"media/fsac/1a34940v.jpg\"\n\"Howard R. Hollem\"\n0\n0.0\n\n\n\"media/fsac/1a34143v.jpg\"\n\"Russell Lee\"\n10\n3.520182\n\n\n\"media/fsac/1a35375v.jpg\"\n\"Alfred T. Palmer\"\n1\n19.758731\n\n\n\"media/fsac/1a34758v.jpg\"\n\"Jack Delano\"\n0\n0.0\n\n\n…\n…\n…\n…\n\n\n\"media/fsac/1a34359v.jpg\"\n\"Marion Post Wolcott\"\n8\n28.831056\n\n\n\"media/fsac/1a34893v.jpg\"\n\"Howard R. Hollem\"\n1\n8.354187\n\n\n\"media/fsac/1a34100v.jpg\"\n\"Russell Lee\"\n1\n3.764648\n\n\n\"media/fsac/1a35045v.jpg\"\n\"Howard Liberman\"\n0\n0.0\n\n\n\"media/fsac/1a34863v.jpg\"\n\"Howard R. Hollem\"\n0\n0.0\n\n\n\n\n\n\nThe people_prop column now contains the percentage of each image covered by detected people. Let’s see which images have the highest human coverage.\n\n(\n    fsac\n    .sort(c.people_prop, descending=True)\n    .head(8)\n    .pipe(DSImage.plot_image_grid, ncol=4, label_name=\"people_prop\")\n)\n\n\n\n\n\n\n\n\nThese tend to be closely framed portraits where the subject fills most of the frame. The segmentation-based measure captures a different aspect of photographic style than simple person counts: a single person in a tight portrait can have higher coverage than a dozen people in a wide street scene.\nThe relationship between person coverage and photographer reveals stylistic differences.\n\n(\n    fsac\n    .group_by(c.photographer)\n    .agg(avg_people = c.people_prop.mean())\n    .pipe(ggplot, aes(\"avg_people\", \"reorder(photographer, avg_people)\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nPhotographers with higher average coverage tended toward portrait work, while those with lower coverage may have focused on environmental scenes, architecture, or wide-angle documentary shots. The measure is not always inversely related to person count; some photographers captured groups in intimate settings with high coverage, while others documented single individuals in expansive landscapes.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "20_image_data.html#pose-detection",
    "href": "20_image_data.html#pose-detection",
    "title": "20  Image Data",
    "section": "20.9 Pose Detection",
    "text": "20.9 Pose Detection\nHuman pose estimation identifies the locations of body parts in an image, typically represented as a set of keypoints corresponding to joints like shoulders, elbows, wrists, hips, knees, and ankles. These keypoints, connected according to human anatomy, form a “skeleton” that describes body position and posture.\nPose estimation models predict coordinates for each keypoint along with confidence scores indicating detection reliability. The COCO pose format defines 17 keypoints: nose, left/right eyes, left/right ears, left/right shoulders, left/right elbows, left/right wrists, left/right hips, left/right knees, and left/right ankles.\nThe training loss for pose estimation typically combines localization accuracy with visibility prediction:\n\\[\n\\mathcal{L}_{\\text{pose}} = \\sum_{k=1}^{K} v_k \\cdot \\left\\| \\mathbf{p}_k - \\hat{\\mathbf{p}}_k \\right\\|^2\n\\]\nwhere \\(K\\) is the number of keypoints, \\(v_k\\) is a visibility flag (1 if the keypoint is visible, 0 otherwise), \\(\\mathbf{p}_k\\) is the ground-truth position, and \\(\\hat{\\mathbf{p}}_k\\) is the predicted position. This formulation only penalizes errors on visible keypoints, acknowledging that occluded body parts cannot be reliably localized.\nAdvanced pose estimation models use heatmap-based representations, predicting a probability distribution over possible keypoint locations:\n\\[\n\\mathcal{L}_{\\text{heatmap}} = \\sum_{k=1}^{K} \\sum_{(x,y)} \\left( H_k(x, y) - \\hat{H}_k(x, y) \\right)^2\n\\]\nwhere \\(H_k(x, y)\\) is the ground-truth heatmap (typically a 2D Gaussian centered on the keypoint) and \\(\\hat{H}_k(x, y)\\) is the predicted heatmap.\nLet’s apply pose detection to our historical photographs.\n\nmodel = YOLO(\"yolo11n-pose.pt\")\npred = model.predict(\"media/fsac/1a35210v.jpg\")\n\n\nrgb = cv2.cvtColor(pred[0].plot(), cv2.COLOR_BGR2RGB)\npil_img = Image.fromarray(rgb)\npil_img\n\n\nimage 1/1 /Users/admin/gh/fds-py/media/fsac/1a35210v.jpg: 544x640 3 persons, 35.9ms\nSpeed: 0.6ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 544, 640)\n\n\n\n\n\n\n\n\n\nThe visualization shows detected poses as skeleton overlays, with lines connecting keypoints according to anatomical structure. The colored points indicate individual keypoints, while the connecting lines show the body structure.\nWe can use pose information to analyze photographic composition in sophisticated ways. Let’s compute the proportion of image height occupied by the torso of the largest detected person. This metric indicates how prominently human figures are framed in each photograph.\n\nresults = []\n\nfor row in fsac.iter_rows(named=True):\n    preds = model.predict(row[\"filepath\"], verbose=False)[0]\n\n    if preds.keypoints is None:\n        results.append(0.0)\n        continue\n\n    img_h = preds.orig_img.shape[0]\n\n    xy = preds.keypoints.xy\n    conf = preds.keypoints.conf\n\n    if conf is None:\n        results.append(0.0)\n        continue\n\n    xy = xy.cpu().numpy()\n    conf = conf.cpu().numpy()\n\n    best_torso = 0.0\n\n    for i in range(xy.shape[0]):\n        kpt_confs = conf[i, [5, 6, 11, 12]]\n        if np.any(kpt_confs &lt; 0.5):\n            continue\n\n        mid_sh = xy[i, [5, 6], :].mean(axis=0)\n        mid_hip = xy[i, [11, 12], :].mean(axis=0)\n\n        torso_len = float(np.linalg.norm(mid_sh - mid_hip))\n\n        if torso_len &gt; best_torso:\n            best_torso = torso_len\n\n    torso_pct = (best_torso / img_h) * 100.0 if best_torso &gt; 0 else 0.0\n    results.append(torso_pct)\n\nfsac = fsac.with_columns(torso=pl.Series(results))\nfsac\n\n\nshape: (500, 5)\n\n\n\nfilepath\nphotographer\npeople\npeople_prop\ntorso\n\n\nstr\nstr\ni64\nf64\nf64\n\n\n\n\n\"media/fsac/1a35266v.jpg\"\n\"Alfred T. Palmer\"\n0\n0.0\n0.0\n\n\n\"media/fsac/1a34940v.jpg\"\n\"Howard R. Hollem\"\n0\n0.0\n0.0\n\n\n\"media/fsac/1a34143v.jpg\"\n\"Russell Lee\"\n10\n3.520182\n0.0\n\n\n\"media/fsac/1a35375v.jpg\"\n\"Alfred T. Palmer\"\n1\n19.758731\n47.362576\n\n\n\"media/fsac/1a34758v.jpg\"\n\"Jack Delano\"\n0\n0.0\n0.0\n\n\n…\n…\n…\n…\n…\n\n\n\"media/fsac/1a34359v.jpg\"\n\"Marion Post Wolcott\"\n8\n28.831056\n21.056602\n\n\n\"media/fsac/1a34893v.jpg\"\n\"Howard R. Hollem\"\n1\n8.354187\n24.024989\n\n\n\"media/fsac/1a34100v.jpg\"\n\"Russell Lee\"\n1\n3.764648\n11.063779\n\n\n\"media/fsac/1a35045v.jpg\"\n\"Howard Liberman\"\n0\n0.0\n0.0\n\n\n\"media/fsac/1a34863v.jpg\"\n\"Howard R. Hollem\"\n0\n0.0\n0.0\n\n\n\n\n\n\nThe computation identifies the shoulder and hip keypoints (indices 5, 6 for shoulders and 11, 12 for hips), calculates the midpoint of each pair, and measures the distance between them. This torso length, expressed as a percentage of image height, indicates how large human subjects appear in the frame.\n\n(\n    fsac\n    .sort(c.torso, descending=True)\n    .head(8)\n    .pipe(DSImage.plot_image_grid, ncol=4, label_name=\"torso\")\n)\n\n\n\n\n\n\n\n\nThe images with the highest torso proportions are medium shots of workers and subjects, framed tightly enough that the torso occupies a substantial portion of the vertical space. These are neither extreme close-ups (which might exclude the torso entirely) nor distant shots (where the full figure would be small).\nThis metric shows strong variation by photographer.\n\n(\n    fsac\n    .filter(c.torso &gt; 0)\n    .group_by(c.photographer)\n    .agg(torso_mean = c.torso.mean(), n = pl.len())\n    .filter(c.n &gt; 0)\n    .pipe(ggplot, aes(\"torso_mean\", \"reorder(photographer, torso_mean)\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nThe differences reflect distinct documentary styles: some photographers favored intimate working shots that captured subjects at medium distance, while others preferred wider environmental compositions or tighter facial portraits. Pose estimation enables these nuanced analyses of compositional choices that would be difficult to quantify manually.\nThis approach could be extended in many directions: analyzing face orientation to study where subjects are looking, comparing the poses of multiple people to identify group dynamics, or tracking limb positions to characterize types of physical activity depicted in photographs.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "20_image_data.html#training-yolo",
    "href": "20_image_data.html#training-yolo",
    "title": "20  Image Data",
    "section": "20.10 Training YOLO",
    "text": "20.10 Training YOLO\nThe pre-trained YOLO models we have used so far recognize objects from the COCO dataset: people, cars, dogs, and other common categories. But what if we want to detect objects specific to our domain? Training a custom YOLO model requires labeled data where humans have drawn bounding boxes around objects of interest and assigned class labels to each box.\nOur bird bounding box dataset contains exactly this information. Each row specifies an image filepath, a species label, and four coordinates defining the corners of a bounding box around the bird.\n\nbirds_bbox\n\n\nshape: (1_000, 7)\n\n\n\nlabel\nfilepath\nbbox_x0\nbbox_y0\nbbox_x1\nbbox_y1\nindex\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"Gray_Catbird\"\n\"media/birds_1000/00000.png\"\n15.0\n44.0\n480.0\n331.0\n\"train\"\n\n\n\"Sayornis\"\n\"media/birds_1000/00001.png\"\n131.0\n85.0\n488.0\n326.0\n\"train\"\n\n\n\"Tennessee_Warbler\"\n\"media/birds_1000/00002.png\"\n40.0\n5.0\n345.0\n239.0\n\"test\"\n\n\n\"White_throated_Sparrow\"\n\"media/birds_1000/00003.png\"\n99.0\n42.0\n448.0\n344.0\n\"train\"\n\n\n\"Ring_billed_Gull\"\n\"media/birds_1000/00004.png\"\n104.0\n32.0\n451.0\n284.0\n\"train\"\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"White_breasted_Kingfisher\"\n\"media/birds_1000/00995.png\"\n17.0\n105.0\n419.0\n336.0\n\"test\"\n\n\n\"Blue_Grosbeak\"\n\"media/birds_1000/00996.png\"\n96.0\n102.0\n361.0\n338.0\n\"test\"\n\n\n\"Yellow_headed_Blackbird\"\n\"media/birds_1000/00997.png\"\n53.0\n42.0\n424.0\n208.0\n\"train\"\n\n\n\"Tree_Sparrow\"\n\"media/birds_1000/00998.png\"\n47.0\n11.0\n450.0\n427.0\n\"train\"\n\n\n\"Cliff_Swallow\"\n\"media/birds_1000/00999.png\"\n158.0\n50.0\n352.0\n316.0\n\"train\"\n\n\n\n\n\n\nThe bbox_x0 and bbox_y0 columns give the coordinates of the upper-left corner, while bbox_x1 and bbox_y1 give the lower-right corner. The index column indicates whether each image belongs to the training set or the test set, a split we made beforehand to enable honest evaluation of model performance.\nYOLO expects training data in a specific directory structure with images and label files organized into train and validation folders. Our helper function DSImage.build_yolo_data converts our tabular format into the required layout, creating a YAML configuration file that tells YOLO where to find the data and what classes to recognize.\n\nDSImage.prepare_yolo_dataset(\n    birds_bbox, root=\"media/yolo_birds\", yaml_name=\"birds.yaml\"\n)\n\nWith the data prepared, training proceeds by loading a pre-trained model and calling its train method. We start from yolo11n.pt, a model already trained on COCO, and fine-tune it on our bird data. This transfer learning approach leverages features the model has already learned from millions of general images, adapting them to our specific task.\n\nmodel = YOLO(\"yolo11n.pt\")\nmodel.train(data=\"media/yolo_birds/birds.yaml\", epochs=50, imgsz=640)\nmodel.save(\"yolo_birds_final.pt\")\n\nThe epochs parameter controls how many complete passes through the training data the model makes, with each epoch updating the model weights based on the loss function described earlier. The imgsz parameter specifies the image size used during training; images are resized to 640×640 pixels regardless of their original dimensions.\nAfter training completes, we evaluate performance on the held-out test set using the val method. This computes standard object detection metrics that quantify how well the model’s predictions match ground-truth annotations.\n\nmetrics = model.val(data=\"media/yolo_birds/birds.yaml\", imgsz=640)\n\nThe primary metrics for object detection are variants of mean Average Precision (mAP), which measures how well the model balances finding all relevant objects (recall) with avoiding false detections (precision). To understand mAP, we first need to define when a predicted bounding box counts as a correct detection.\nA prediction is considered a true positive if its Intersection over Union (IoU) with a ground-truth box exceeds some threshold. Recall that IoU measures the overlap between two boxes:\n\\[\n\\text{IoU} = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}\n\\]\nAn IoU of 1.0 means perfect overlap, while 0 means no overlap at all. The choice of IoU threshold determines how strict we are about localization accuracy. For each class, we can compute precision and recall at various confidence thresholds, tracing out a precision-recall curve. Average Precision (AP) summarizes this curve as the area underneath it:\n\\[\n\\text{AP} = \\int_0^1 p(r) \\, dr\n\\]\nwhere \\(p(r)\\) is precision at recall level \\(r\\). In practice, this integral is approximated by interpolating the precision-recall curve at discrete points. The mAP50 metric uses an IoU threshold of 0.50, meaning a prediction counts as correct if it overlaps with the ground truth by at least 50%. This is a relatively lenient standard that rewards finding objects even if the bounding box is not perfectly aligned.\n\nmetrics.box.map50\n\nThe mAP50-95 averages performance across multiple IoU thresholds from 0.50 to 0.95 in steps of 0.05:\n\\[\n\\text{mAP50-95} = \\frac{1}{10} \\sum_{t \\in \\{0.50, 0.55, \\ldots, 0.95\\}} \\text{mAP}_t\n\\]\nThis stricter metric rewards precise localization. A model might achieve high mAP50 by finding objects with roughly correct boxes, but achieving high mAP50-95 requires tight alignment between predictions and ground truth.\n\nmetrics.box.map\n\nThe gap between these two metrics reveals how well the model localizes objects. A large gap suggests the model finds objects but draws imprecise boxes; a small gap indicates accurate localization.\nFinally, let’s see the trained model in action on a test image it has never seen during training.\n\npath = birds_bbox.filter(c.index == \"test\").select(c.filepath.last()).item()\npred = model.predict(path)\n\nrgb = cv2.cvtColor(pred[0].plot(), cv2.COLOR_BGR2RGB)\npil_img = Image.fromarray(rgb)\npil_img\n\n\nimage 1/1 /Users/admin/gh/fds-py/media/birds_1000/00996.png: 512x640 1 bird, 29.5ms\nSpeed: 0.9ms preprocess, 29.5ms inference, 0.3ms postprocess per image at shape (1, 3, 512, 640)\n\n\n\n\n\n\n\n\n\nAnd, at least in this one example, it does a very good job of boxing in the bird within the image.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "20_image_data.html#vision-language-models",
    "href": "20_image_data.html#vision-language-models",
    "title": "20  Image Data",
    "section": "20.11 Vision Language Models",
    "text": "20.11 Vision Language Models\nThe models we have explored so far each produce structured outputs: bounding boxes, segmentation masks, or keypoint coordinates. These representations are useful for quantitative analysis but do not capture the full richness of what an image depicts. A photograph of workers in a factory contains not just “3 persons detected” but a story about labor, industry, and daily life. Extracting such narrative content has traditionally required human interpretation.\nVision Language Models (VLMs) bridge this gap by combining visual understanding with natural language generation. These models can look at an image and produce free-form text descriptions, answer questions about visual content, or engage in dialogue about what they see. They represent a convergence of computer vision and large language models, trained on massive datasets of images paired with textual descriptions.\nThe architecture of a VLM typically consists of three components: a vision encoder that processes the image into a sequence of visual tokens, a projection layer that maps these tokens into the same embedding space used by the language model, and a language model that generates text conditioned on both the visual tokens and any text prompt. During training, the model learns to associate visual patterns with their linguistic descriptions, enabling it to describe novel images it has never seen before.\nVLMs open new possibilities for image analysis. Rather than counting objects or measuring pixel proportions, we can ask open-ended questions: “What activity is taking place in this photograph?” or “Describe the mood conveyed by this image.” The responses, while subjective and sometimes imperfect, capture aspects of visual content that structured outputs cannot represent.\nLet’s use the OpenAI API to get a description of one of our FSA photographs. We will send the image to a vision-capable model and ask it to describe what it sees.\n\nimport base64\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nimage_path = \"media/fsac/1a35210v.jpg\"\n\nwith open(image_path, \"rb\") as f:\n    image_data = base64.standard_b64encode(f.read()).decode(\"utf-8\")\n\ntext = (\"Provide a detailed plain-text description of the \"\n       \"objects, activities, people, background and/or composition \"\n       \"of this photograph\")\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-5-mini-2025-08-07\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": text\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{image_data}\"\n                    }\n                }\n            ]\n        }\n    ]\n)\n\nprint(response.choices[0].message.content)\n\n\n\n\nThe photograph shows a close, waist-up group portrait of three young men in\nmilitary work clothes standing in front of an armored vehicle. Each man wears\na one-piece coverall or boiler suit, belted at the waist, and a padded tank\nhelmet with ear flaps or built-in headphones. Their coveralls are dusty and\nstained with grease and dirt; their faces and hands also show grime,\nsuggesting recent hard work or field activity.  All three men are smiling\nbroadly and appear relaxed and friendly with one another. The central figure\nstands slightly forward with his arms linked to the two men beside him; the\nman on the left has one hand resting on his hip, and the man on the right has\na hand on his belt. Their posture and expressions convey camaraderie and a\nmoment of shared good spirits.  Behind them, a large tank or armored vehicle\ndominates the background. The vehicle’s turret and a long gun barrel run\ndiagonally across the upper right of the image. Parts of the tank’s hull and\ntracks are visible, showing a dusty, worn surface. In the upper left\nbackground, a pair of legs and a boot belonging to another crew member can\nbe seen standing on the vehicle, only partially included in the frame.\nThe setting is outdoors under a clear blue sky; a low horizon of open\ncountryside or field is visible in the far background. The photograph is\nin color with warm, natural daylight highlighting the men’s faces and the\ntextured surfaces of their clothing and the vehicle. The composition centers\nthe trio, creating a tight, informal group portrait against the larger\nmechanical backdrop, emphasizing both the human element and the military\nequipment. The image has the look of a candid, in-the-field moment rather\nthan a formal posed studio shot.\n\n\n\nThe model returns a natural language description that captures elements no structured detector could identify: the apparent era suggested by clothing and photographic style, the social context implied by the scene, and interpretive observations about mood or activity. This kind of output is inherently more subjective than counting bounding boxes, but it provides a different and complementary form of understanding.\nVLMs can also be used programmatically to generate structured data from images. By carefully crafting prompts, you can ask the model to output JSON with specific fields, effectively creating a flexible object detector that can identify whatever categories you specify without retraining. This approach trades some accuracy for remarkable flexibility: the same model can describe fashion items, identify architectural styles, or catalog the contents of historical photographs.\nThe combination of structured computer vision models and flexible VLMs provides a powerful toolkit. Use object detection when you need precise counts and locations. Use segmentation when pixel-level boundaries matter. Use pose estimation for body position analysis. And use VLMs when you need rich, contextual descriptions or want to extract information that no pre-trained detector was designed to find.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "20_image_data.html#conclusions",
    "href": "20_image_data.html#conclusions",
    "title": "20  Image Data",
    "section": "20.12 Conclusions",
    "text": "20.12 Conclusions\nThis chapter has explored a range of techniques for extracting information from images, progressing from simple pixel statistics to sophisticated deep learning models and vision language models. Each approach offers different tradeoffs between interpretability, computational cost, and semantic richness.\nBasic pixel-level features like brightness and color distributions are fast to compute and easy to understand. They capture global properties of images and can reveal interesting patterns across collections. However, these features cannot distinguish between a yellow bird and a yellow background, or between a dark subject and a dark photograph.\nDeep learning models for object detection, segmentation, and pose estimation offer dramatically more sophisticated understanding. These models identify meaningful objects, delineate their boundaries at pixel precision, and localize body parts in complex poses. They enable analyses that would be impossible with simple features: counting people in crowds, measuring how much of an image depicts human figures, or characterizing body positions.\nVision language models add yet another dimension by producing natural language descriptions of visual content. They capture contextual and interpretive aspects of images that structured outputs cannot represent, though their outputs require different analytical approaches than numerical features.\nBeyond the specific models explored here, remember that images can be converted to embeddings using transfer learning approaches (Chapter 15), enabling classification, clustering, and visualization with standard machine learning tools. This embedding-based approach is particularly valuable when your analytical categories don’t match the labels in pre-trained models.\nThe models we explored here—YOLO variants trained on the COCO dataset and VLMs trained on web-scale image-text pairs—represent just a sample of available approaches. The field of computer vision continues to advance rapidly, with new architectures and training techniques regularly improving accuracy and enabling new capabilities. The fundamental pattern, however, remains consistent: we extract information from images by combining low-level pixel data with learned representations that capture semantic meaning.\nFor data science applications, image analysis opens vast possibilities. Archives of historical photographs can be systematically analyzed to understand social patterns. Medical images can be screened for abnormalities. Satellite imagery can track environmental changes. Social media photographs can reveal trends in consumer behavior. Wherever images contain information, the techniques in this chapter provide tools to extract it.",
    "crumbs": [
      "Part IV: Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "21_notes.html",
    "href": "21_notes.html",
    "title": "21  Notes",
    "section": "",
    "text": "21.1 Setup\nLoad all of the modules and datasets needed for the chapter.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "21_notes.html#introduction",
    "href": "21_notes.html#introduction",
    "title": "21  Notes",
    "section": "21.2 Introduction",
    "text": "21.2 Introduction\nIn this chapter, we provide a list of the key functions that have been introduced throughout the book, with a particular focus on the general-purpose methods in the first two sections. We strongly recommend first using these notes when referencing a specific operation rather than the original text. The latter is designed for reading, whereas this is designed for quick reference and reduces the need to jump around across many different pages.\nIn the notes below we use the term df and df_r to be two DataFrame objects loaded into python. The terms col1, col2, and col3 are column names; when quotes are needed these are always added in the reference code. The values &lt;i64&gt;, &lt;f64&gt;, and &lt;str&gt; refer to integers, floats, and strings respectively. These are sometimes used multiple times in the same example; this does not indicate that they need to be the same number.\nNote that the examples often only give the core code that would need to be included within DataFrame transformation or visualization pipelines. A few complete examples are given to indicate how this should be formated and where it should go. When feasable, we have added comments after the # sign to gloss what the code does. These notes should not be included in your own code.",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "21_notes.html#loading-and-saving-data",
    "href": "21_notes.html#loading-and-saving-data",
    "title": "21  Notes",
    "section": "21.3 Loading and Saving Data",
    "text": "21.3 Loading and Saving Data\nHere is the standard way of loading a CSV dataset into Python and saving it as a DataFrame called df.\n\ndf = pl.read_csv(\"data/hans_roslin.csv\")\ndf\n\nIf you have messy data from another source, this is a helpful one-line of code that sanitises the column names. We usually will not need this for the datasets in this book, but it is very helpful to have when working on data from other sources.\n\ndf = df.rename(lambda c: c.lower().replace_all(r\"[^a-z0-9_]\", \"\"))\n\nFinally, to write the output as a CSV file, use the following code:\n\ndf.write_csv(\"output_name.csv\")\n\nOther input and output formats can be found in the Polars documentation.",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "21_notes.html#transforming-data",
    "href": "21_notes.html#transforming-data",
    "title": "21  Notes",
    "section": "21.4 Transforming Data",
    "text": "21.4 Transforming Data\nThese methods return a subset of the rows from the original dataset:\n\n.head(n=&lt;i64&gt;)          # take the first n rows from the top\n.tail(n=&lt;i64&gt;)          # take the last n rows from the bottom\n.sample(n=&lt;i64&gt;)        # take a random selection of n rows\n.sample(fraction=&lt;f64&gt;) # take a random selection of frac proportion of rows\n.drop_nulls()           # remove any rows that have missing data\n.drop_nans()            # remove any rows that have invalid numbers\n\nThe .filter method allows for selecting rows based on one or more conditions:\n\n.filter(c.col1 &gt; 0.1)        # select rows where col1 is bigger than 0.1 \n.filter(c[\"col1\"] &gt; 0.1)     # select rows where col1 is bigger than 0.1\n.filter(c.col1.is_in([\"A\", \"B\"]))   # select rows if col1 is \"A\" or \"B\"\n.filter(~c.col1.is_in([\"A\", \"B\"]))  # select if col1 is neither \"A\" nor \"B\"\n\nThese sort the rows of the dataset without changing their contents:\n\n.sort(c.col1)            # sort in ascending order by col1\n.sort(c.col1, c.col2)    # sort as above; break ties with col2\n.sort(c.col1, descending=True)  # sort in descending order by col1\n\nThe .with_columns method allows for creating or modifying one or more columns.\n\n.with_columns(\n    new = c.col1 + c.col2,     # create a new column as sum of col1 and col2\n    new2 = c.col3.sqrt(),      # create a new column from square root of col1\n)\n\nThe .over method on a column allows us to group by a column when creating or modifying another column.\n\n.with_columns(\n    new = c.col1 - c.col1.over(\"col2\").min(),    # diff to smallest value\n    new = c.col1 - c.col1.over(\"col2\").shift(-1) # rolling difference\n)\n\nThere is a special pl.when function that allows for using conditional statements when creating or modifying a column.\n\n.with_columns(\n    new = pl.when(c.col1 &gt; 0).then(c.col1).otherwise(0)\n)\n\nThere is also a special method called cast that converts data between different types. This is most useful for converting strings into numbers after string processing.\n\nc.col1.cast(pl.Int64)\nc.col1.cast(pl.Float64)\n\nWe can also group by one or more columns and then compute one or more aggregations of the columns (or transformations of them).\n\n.group_by(c.col1, c.col2)\n.agg(\n    col1_mean = c.col1.mean(),          # compute the average of col1\n    col1_1st = c.col1.first(),          # get the first value of col1\n)\n\nCommon aggregation methods include the following:\n\n.mean(), .median(), .min(), .max(), .sum(), .quantile(&lt;f64&gt;) - .first(), .last() - .any(), .all() - .n_unique(), .count(), .len() - pl.concat_str(c.col1, separator=\"&lt;str&gt;\")",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "21_notes.html#restructuring-data",
    "href": "21_notes.html#restructuring-data",
    "title": "21  Notes",
    "section": "21.5 Restructuring Data",
    "text": "21.5 Restructuring Data\nTo join on one or more keys between two tables, we can use the .join method. There are several options that control how it works, most of which can be combined with one another in various ways\n\n.join(df_r, on=\"col1\", how=\"left\")             # left join on col1\n.join(df_r, on=[\"col1\", \"col2\"], how=\"left\")   # left join on col1 and col2\n.join(df_r, left_on=\"col1\", right_on=\"col2\" how=\"left\") # join col1 == col2\n\n.join(df_r, on=\"col1\", how=\"right\")            # right join on col1\n.join(df_r, on=\"col1\", how=\"inner\")            # inner join on col1\n.join(df_r, on=\"col1\", how=\"full\")             # full join on col1\n.join(df_r, on=\"col1\", how=\"cross\")            # cross join on col1\n.join(df_r, on=\"col1\", how=\"semi\")             # semi join on col1\n.join(df_r, on=\"col1\", how=\"anti\")             # anti join on col1\n\nMore complex joins can use the .join_where method.\n\n.join_where(df_r, c.col1 &gt; c.col2)   # all combinations where col1 &gt; col2\n.join_where(                \n    df_r, c.col1 &gt; c.col2_right      # append suffix if col2 is in both tables \n)   \n\nWe can make a table wider using the .pivot method.\n\n.pivot(\n    values=\"col1\",\n    index=\"col2\",\n    columns=\"col3\",\n)\n\nOr, make it longer using the .unpivot method. We simply indicate which column(s) to use as the index in the output and all the other values becom unpivoted into the resulting rows.\n\n.unpivot(\n    index=\"col1\"\n)",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "21_notes.html#visualization",
    "href": "21_notes.html#visualization",
    "title": "21  Notes",
    "section": "21.6 Visualization",
    "text": "21.6 Visualization\nHere is an example of a simple scatter plot of two columns from the DataFrame df.\n\n(\n    df\n    .to_ggplot(aes(x=\"col1\", y=\"col2\"))\n    + geom_point()\n)\n\nGeometries can be modified by adding optional aesthetics such as color and size or an alternative DataFrame df_r. Any or all of these can be added and mixed with different geometries.\n\n+ geom_point(aes(color=\"col3\"), size=&lt;i64&gt;, data=df_r.to_pandas())\n\nHere are some of the most common geometries that were introduced in the text along with their additional required aesthetics beyond x and y. The second set at the bottom have statistics applied before creating the plot.\n\n+ geom_col()\n+ geom_line()\n+ geom_path()\n+ geom_barplot()\n+ geom_violin()\n+ geom_text(aes(label=\"col3\"))\n+ geom_segment(aes(xend=\"col3\", xend=\"col4\"))\n\n+ geom_smooth(method=\"lm\", se=False)\n+ geom_boxplot()\n+ geom_bar()\n+ geom_histogram(binwidth=&lt;f64&gt;, boundary=&lt;f64&gt;)   # has no y-aesthetic\n+ geom_histogram(bins=&lt;i64&gt;)                       # has no y-aesthetic\n+ geom_density(adjust=&lt;f64&gt;)                       # has no y-aesthetic\n\nScales can be added to the plot to control how visual elements are mapped from the specific values in the data. See the full examples in Chapter 3 for how to further customize these.\n\n+ scale_color_cmap()            # continuous color-blind friendly colors\n+ scale_color_cmap_d()          # discrete color-blind friendly colors\n+ scale_fill_cmap()             # continuous color-blind friendly colors\n+ scale_fill_cmap_d()           # discrete color-blind friendly colors\n+ scale_size_area(max_size=&lt;f64&gt;)  # makes zero size correspond to zero value\n+ scale_x_log10()               # set a log-scale for the x-axis\n+ scale_y_log10()               # set a log-scale for the y-axis\n+ scale_x_continuous(limits=[&lt;f64&gt;, &lt;f64&gt;])   # set range of the x axis\n+ scale_y_continuous(limits=[&lt;f64&gt;, &lt;f64&gt;])   # set range of the y axis\n\nWe can also add labels, facets, and coordinate systems to further modify the look of the plot.\n\n+ labs(\n    title=\"Main title\",\n    subtitle=\"Subtitle\",\n    caption=\"Label at the bottom\",\n    x=\"x-axis label\",\n    y=\"y-axis label\",\n    color=\"color legend title\"\n)\n\n\n+ facet_wrap(\"col1\")\n+ facet_grid(\"col1\", \"col2\")\n\n+ coord_flip()\n\nConsult the full plotnine documentation for the complete set of possible elements and options that can be used to create rich data visualizations in Python.",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "21_notes.html#strings",
    "href": "21_notes.html#strings",
    "title": "21  Notes",
    "section": "21.7 Strings",
    "text": "21.7 Strings\nThere are many different string operations supplied by polars that allow us to work with the content of the values within a string column. Many of these allow us to search for patterns of strings using regular expressions. In the examples below &lt;pattern&gt; describes a regular expression (described at the end of the section). Set literal=False to treat the search string as a literal value.\nThe method contains can be used inside of a .filter() method to select rows in a dataset.\n\nc.col1.str.contains(\"&lt;pattern&gt;\")\n\nThe .join function can be used within an aggregation to combine the values of a string column with a seperator value. It is often useful to use .unique() and/or .sort() before doing the joining.\n\nc.col1.str.join(\"&lt;str&gt;\")\n\nAll of the other string functions are used inside of with_columns to create or modify a column in the data. These are the functions that we use throughout the text, with notes for those that may not be clear from the context. They are grouped by the kind of data they return: the first set return a new string, the second an integer, and the final one a list.\n\nc.col1.str.extract(\"&lt;pattern&gt;\")\nc.col1.str.extract(\"&lt;pattern&gt;\", &lt;i64&gt;)   # extract group number &lt;i64&gt;\nc.col1.str.extract_all(\"&lt;pattern&gt;\")\nc.col1.str.replace(\"&lt;pattern&gt;\", \"&lt;str&gt;\")\nc.col1.str.replace_all(\"&lt;pattern&gt;\", \"&lt;str&gt;\")\nc.col1.str.slice(&lt;i64&gt;, &lt;i64&gt;)   # substring from the first index to the last\nc.col1.str.strip_chars()         # removes leading and trailing whitespace\nc.col1.str.to_lowercase()\nc.col1.str.to_uppercase()\nc.col1.str.to_titlecase()\n\nc.col1.str.count_matches(\"&lt;pattern&gt;\")\nc.col1.str.find(\"&lt;pattern&gt;\")             # returns starting index of pattern\nc.col1.str.len_chars()\n\nc.col1.str.split(\"&lt;str&gt;\")  # split by &lt;str&gt; into a list; use with .explode()\n\nThe polars library uses Rust-based regular expressions. The full documentation can be found on the regex syntax page. Here are examples of some of the most common patterns:\n\n. any character except new line\n[0-9] any digit\n[...] (where … is another expression) one element from ...\n(...) (where … is another expression) captures the value of ...\nx* zero or more values of x\nx+ one or more values of x\n^ start of a line\n$ end of a line\n\\w any word character\n\\W any non-word character\n\\s any whitespace\n\\W any non-whitespace\n\\p{Greek} a Unicode character class; here “Greek”\n\nNote that starting a string with the letter “r” (i.e., r\"\\d+\") makes a literal string that avoids treating the slash as an escape character. It is commonly used for regular expressions to make the code easier to read. It is, however, only the creation of the string that is different. There is no magical marker that this has been created as a regular expression.",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "21_notes.html#inference",
    "href": "21_notes.html#inference",
    "title": "21  Notes",
    "section": "21.8 Inference",
    "text": "21.8 Inference\nTo run statistical inference using the columns from a DataFrame, using the helper function fit_statsmodels as follows:\n\n.pipe(fit_statsmodels, \"ttest1\", \"col1 ~ 1\")     # col1 num.\n.pipe(fit_statsmodels, \"ttest2\", \"col1 ~ col2\")  # col1 num. ; col2 2 groups\n.pipe(fit_statsmodels, \"anova\", \"col1 ~ col2\")   # col1 num. ; col2 3+ groups\n.pipe(fit_statsmodels, \"chi2\", \"col1 ~ col2\")    # col1 + col2 categorical\n.pipe(fit_statsmodels, \"gtest\", \"col1 ~ col2\")   # col1 + col2 categorical\n.pipe(fit_statsmodels, \"ols\", \"col1 ~ col2\")     # col1 num.\n.pipe(fit_statsmodels, \"logit\", \"col1 ~ col2\")   # col1 is 0/1\n\nThe “ols” and “logit” models can accept any number of numeric and categorical variables on the right-hand side. They also accept the option raw=True to return the full statsmodels model object, which has methods such as .describe().",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "21_notes.html#sklearn-helpers",
    "href": "21_notes.html#sklearn-helpers",
    "title": "21  Notes",
    "section": "21.9 Sklearn Helpers",
    "text": "21.9 Sklearn Helpers\nThe DSSklearn class provides wrappers for fitting supervised, dimensionality reduction, and clustering models directly from a DataFrame. Supervised models split the data into train/test sets automatically.\n\nDSSklearn.linear_regression(df, target=c.col1, drop=c.id)\nDSSklearn.elastic_net(df, target=ccol1, features=[c.col2, c.col3])\nDSSklearn.elastic_net_cv(df, target=c.col1, drop=c.id)\nDSSklearn.gradient_boosting_regressor(df, target=c.col1, drop=c.id)\nDSSklearn.random_forest_regressor(df, target=c.col1, drop=c.id)\n\nDSSklearn.logistic_regression(df, target=c.col1, drop=c.id)\nDSSklearn.logistic_regression_cv(df, target=c.col1, drop=\"id)\nDSSklearn.gradient_boosting_classifier(df, target=c.col1, drop=c.id)\nDSSklearn.random_forest_classifier(df, target=c.col1, drop=c.id)\n\nDSSklearn.pca(df, drop=c.id, n_components=2)\nDSSklearn.tsne(df, drop=c.id, n_components=2)\nDSSklearn.umap(df, drop=c.id, n_components=2)\n\nDSSklearn.kmeans(df, drop=c.id, n_clusters=5)\nDSSklearn.dbscan(df, drop=c.id, eps=0.5)\n\nAll models accept test_size, random_state, and stratify options. The fitted model returns a wrapper object with the following methods:\n\nmodel.predict()              # returns DataFrame with target_, prediction_\nmodel.predict(full=True)     # includes original columns\nmodel.predict_proba()        # classifiers only; adds probability columns\nmodel.score()                # returns dict with train/test accuracy or RMSE\nmodel.coef()                 # linear/logistic; returns DataFrame of coefficients\nmodel.coef(raw=True)         # coefficients on original (unscaled) features\nmodel.importance()           # tree models; returns feature importances\nmodel.alpha()                # CV models; returns selected regularization\nmodel.confusion_matrix()     # classifiers; displays confusion matrix\n\nThe DSSklearnText class works similarly but builds a document-term matrix from token-level data:\n\nDSSklearnText.logistic_regression(\n    df, doc_id=\"doc_id\", term_id=\"lemma\", target=\"label\",\n    min_df=0.01, max_df=0.9, max_vocab_size=5000\n)\n\nIt supports all the same model types and methods as DSSklearn.",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "21_notes.html#pytorch-helpers",
    "href": "21_notes.html#pytorch-helpers",
    "title": "21  Notes",
    "section": "21.10 PyTorch Helpers",
    "text": "21.10 PyTorch Helpers\nThe DSTorch class provides utilities for loading data and training neural network classifiers.\n\nX, X_train, X_test, y, y_train, y_test, cn = DSTorch.load_image(\n    df, scale=True\n)\n\nX, X_train, X_test, y, y_train, y_test, cn = DSTorch.load_text(\n    df, model=w2v_model, tokens_expr=c.tokens, label_expr=c.label, max_length=200\n)\n\nDSTorch.train(model, optimizer, X_train, y_train, num_epochs=18, batch_size=32)\n\nDSTorch.score_image(model, X_test, y_test, cn)   # returns accuracy\nDSTorch.score_text(model, X_test, y_test)        # returns accuracy\nDSTorch.predict(model, X_test, y_test, cn)       # returns DataFrame\nDSTorch.predict_proba(model, X_test, y_test, cn) # includes probabilities\nDSTorch.confusion_matrix(model, X_test, y_test, cn)",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "21_notes.html#transformer-embedders",
    "href": "21_notes.html#transformer-embedders",
    "title": "21  Notes",
    "section": "21.11 Transformer Embedders",
    "text": "21.11 Transformer Embedders\nThree classes provide pre-trained embeddings from transformer models. Each returns a normalized numpy vector.\n\nvit = ViTEmbedder()\nvec = vit(\"path/to/image.jpg\")\n\nsiglip = SigLIPEmbedder()\nimg_vec = siglip.embed_image(\"path/to/image.jpg\")\ntxt_vec = siglip.embed_text(\"a photo of a cat\")\n\ne5 = E5TextEmbedder()\nvec = e5(\"some text to embed\")\nvecs = e5([\"text1\", \"text2\"])\n\nUse dot_product(c.vec1, c.vec2) to compute similarity between embedding columns.",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "21_notes.html#network-data",
    "href": "21_notes.html#network-data",
    "title": "21  Notes",
    "section": "21.12 Network Data",
    "text": "21.12 Network Data\nThe DSNetwork.process function converts an edge list into node and edge DataFrames suitable for visualization, along with centrality measures.\n\nnode_df, edge_df, G = DSNetwork.process(edges_df, directed=False)\n\nInput edges_df must have columns doc_id and doc_id2. The returned node_df contains: id, x, y (layout coordinates), component, cluster, degree (or degree_in/degree_out if directed), eigen, between, and close (closeness, undirected only). The edge_df contains x, y, xend, yend for plotting with geom_segment. The igraph object G is also returned for further analysis.",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "21_notes.html#text-data",
    "href": "21_notes.html#text-data",
    "title": "21  Notes",
    "section": "21.13 Text Data",
    "text": "21.13 Text Data\nThe DSText.process function applies a spaCy NLP pipeline to a document DataFrame and returns a token-level DataFrame.\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\ntokens_df = DSText.process(docs_df, nlp)\n\nInput docs_df must have columns doc_id and text. The output contains one row per token with columns: doc_id, sid (sentence id), tid (token id), token, token_with_ws, lemma, upos, tag, is_alpha, is_stop, is_punct, dep, head_idx, ent_type, ent_iob.",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "21_notes.html#image-data",
    "href": "21_notes.html#image-data",
    "title": "21  Notes",
    "section": "21.14 Image Data",
    "text": "21.14 Image Data\nThe DSImage class provides utilities for working with image files.\n\nDSImage.plot_image_grid(df, ncol=10, label_name=\"label\", filepath=\"filepath\", limit=100)\n\nThe compute_colors method returns color percentages from an HSV image array (used internally for color analysis).",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes</span>"
    ]
  },
  {
    "objectID": "22_datasets.html",
    "href": "22_datasets.html",
    "title": "22  Datasets",
    "section": "",
    "text": "22.1 Setup\nLoad all of the modules and datasets needed for the chapter. In each of the sections below we briefly present the datasets used in this text and the supplemental materials. The glimpse method is used to show all of the column names, data types, and the first few rows of the dataset.\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#countries",
    "href": "22_datasets.html#countries",
    "title": "22  Datasets",
    "section": "22.2 Countries",
    "text": "22.2 Countries\nSourced from GapMinder and WikiData, the countries dataset provides a snapshot of 135 nations, identifying each by its full standard name and three-letter ISO code. Geographically, entries are categorized into broad regions and specific subregions, accompanied by precise latitude and longitude coordinates. The data captures essential socioeconomic health through metrics such as total population (in millions), life expectancy, and the Human Development Index (HDI). Economic conditions are represented by GDP figures and the Gini coefficient, which measures income inequality, while broader well-being is gauged via a happiness index. Additionally, the dataset includes infrastructure and cultural details, specifically tracking cellphone adoption rates, the percentage of the population with access to improved water sources, and the primary languages spoken.\n\ncountries = pl.read_csv(\"data/countries.csv\")\ncountries.glimpse()\n\nRows: 135\nColumns: 15\n$ iso          &lt;str&gt; 'SEN', 'VEN', 'FIN', 'USA', 'LKA', 'DOM', 'SGP', 'GAB', 'BGR', 'TZA'\n$ full_name    &lt;str&gt; 'Senegal', 'Venezuela, Bolivarian Republic of', 'Finland', 'United States of America', 'Sri Lanka', 'Dominican Republic', 'Singapore', 'Gabon', 'Bulgaria', 'Tanzania, United Republic of'\n$ region       &lt;str&gt; 'Africa', 'Americas', 'Europe', 'Americas', 'Asia', 'Americas', 'Asia', 'Africa', 'Europe', 'Africa'\n$ subregion    &lt;str&gt; 'Western Africa', 'South America', 'Northern Europe', 'Northern America', 'Southern Asia', 'Caribbean', 'South-eastern Asia', 'Middle Africa', 'Eastern Europe', 'Eastern Africa'\n$ pop          &lt;f64&gt; 18.932, 28.517, 5.623, 347.276, 23.229, 11.52, 5.871, 2.593, 6.715, 70.546\n$ lexp         &lt;f64&gt; 70.43, 76.18, 82.84, 79.83, 78.51, 74.35, 85.63, 68.68, 74.33, 68.59\n$ lat          &lt;f64&gt; 14.366667, 8.0, 65.0, 39.828175, 7.0, 18.8, 1.3, -0.683330555, 42.75, -6.306944444\n$ lon          &lt;f64&gt; -14.283333, -67.0, 27.0, -98.5795, 81.0, -70.2, 103.8, 11.5, 25.5, 34.853888888\n$ hdi          &lt;f64&gt; 0.53, 0.709, 0.948, 0.938, 0.776, 0.776, 0.946, 0.733, 0.845, 0.555\n$ gdp          &lt;i64&gt; 4871, 8899, 57574, 78389, 14380, 25663, 137906, 19543, 36211, 3924\n$ gini         &lt;f64&gt; 38.1, 44.8, 27.7, 47.7, 39.3, 39.6, null, 38.0, 40.3, 40.5\n$ happy        &lt;f64&gt; 50.93, 57.65, 76.99, 65.21, 36.02, 59.21, 66.54, 51.04, 55.9, 40.42\n$ cellphone    &lt;f64&gt; 66.0, 96.8, 156.4, 91.7, 83.1, 90.6, 145.5, 93.6, 137.1, 46.9\n$ water_access &lt;f64&gt; 54.93987, 95.66913, 99.44798, 99.72235, 90.77437, 86.1939, 100.0, 49.20331, 86.00395, 26.78297\n$ lang         &lt;str&gt; 'pbp|fra|wol', 'spa|vsl', 'fin|swe', 'eng', 'sin|sin|tam|tam', 'spa', 'eng|msa|cmn|tam', 'fra', 'bul', 'eng|swa'\n\n\n\nAlso sourced from GapMinder, the cellphone dataset is a longitudinal record containing 3,480 observations that track the adoption of mobile technology over time. Unlike the previous cross-sectional dataset, this table uses a time-series format, recording data for specific nations identified by their three-letter iso codes across multiple years. The primary metric, cell, quantifies mobile phone subscriptions (expressed per 100 people), allowing for the analysis of growth trends and technological saturation within different countries over the recorded period.\n\ncellphone = pl.read_csv(\"data/countries_cellphone.csv\")\ncellphone.glimpse()\n\nRows: 3480\nColumns: 3\n$ iso  &lt;str&gt; 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AFG'\n$ year &lt;i64&gt; 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012\n$ cell &lt;f64&gt; 0.87978, 2.54662, 4.91711, 9.9133, 18.0167, 29.8268, 38.2289, 36.1187, 47.0152, 50.1967\n\n\n\nSourced from Wikidata, the borders dataset provides a relational map of international boundaries, containing 829 entries that define connections between nations. Each row represents a single land border, linking a primary country (iso) to one of its adjacent neighbors (iso_neighbor) using their three-letter ISO codes. Because a single country often shares borders with multiple neighbors, the iso column contains repeated values, effectively creating an adjacency list that allows for the analysis of geographic clustering, continent connectivity, and geopolitical relationships.\n\nborders = pl.read_csv(\"data/countries_borders.csv\")\nborders.glimpse()\n\nRows: 829\nColumns: 2\n$ iso          &lt;str&gt; 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AGO', 'AGO', 'AGO', 'AGO'\n$ iso_neighbor &lt;str&gt; 'IRN', 'PAK', 'CHN', 'TJK', 'TKM', 'UZB', 'COD', 'GAB', 'NAM', 'COG'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#food-items",
    "href": "22_datasets.html#food-items",
    "title": "22  Datasets",
    "section": "22.3 Food Items",
    "text": "22.3 Food Items\nThe food dataset profiles 61 common culinary items, providing a comprehensive nutritional and descriptive breakdown for each. It categorizes items into broad food_group classifications (such as fruits, vegetables, grains, and meats) and details their dietary composition through macronutrients—including total and saturated fats, carbohydrates, sugar, fiber, and protein—as well as cholesterol and calorie counts. The dataset also tracks micronutrient content, specifically sodium, iron, and vitamins A and C. Beyond nutritional metrics, the table includes metadata sourced from WikiData, such as a URL slug (wiki), a textual description defining the item, and its primary visual color.\n\nfood = pl.read_csv(\"data/food.csv\")\nfood.glimpse()\n\nRows: 61\nColumns: 17\n$ item        &lt;str&gt; 'Apple', 'Asparagus', 'Avocado', 'Banana', 'Chickpea', 'String Bean', 'Beef', 'Bell Pepper', 'Crab', 'Broccoli'\n$ food_group  &lt;str&gt; 'fruit', 'vegetable', 'fruit', 'fruit', 'grains', 'vegetable', 'meat', 'vegetable', 'fish', 'vegetable'\n$ calories    &lt;i64&gt; 52, 20, 160, 89, 180, 31, 288, 26, 87, 34\n$ total_fat   &lt;f64&gt; 0.1, 0.1, 14.6, 0.3, 2.9, 0.1, 19.5, 0.0, 1.0, 0.3\n$ sat_fat     &lt;f64&gt; 0.028, 0.046, 2.126, 0.112, 0.309, 0.026, 7.731, 0.059, 0.222, 0.039\n$ cholesterol &lt;i64&gt; 0, 0, 0, 0, 0, 0, 87, 0, 78, 0\n$ sodium      &lt;i64&gt; 1, 2, 7, 1, 243, 6, 384, 2, 293, 33\n$ carbs       &lt;f64&gt; 13.81, 3.88, 8.53, 22.84, 29.98, 7.13, 0.0, 6.03, 0.04, 6.64\n$ fiber       &lt;f64&gt; 2.4, 2.1, 6.7, 2.6, 8.6, 3.4, 0.0, 2.0, 0.0, 2.6\n$ sugar       &lt;f64&gt; 10.39, 1.88, 0.66, 12.23, 5.29, 1.4, 0.0, 4.2, 0.0, 1.7\n$ protein     &lt;f64&gt; 0.26, 2.2, 2.0, 1.09, 9.54, 1.82, 26.33, 0.99, 18.06, 2.82\n$ iron        &lt;i64&gt; 1, 12, 3, 1, 17, 6, 15, 2, 4, 4\n$ vitamin_a   &lt;i64&gt; 1, 15, 3, 1, 0, 14, 0, 63, 0, 12\n$ vitamin_c   &lt;i64&gt; 8, 9, 17, 15, 3, 27, 0, 317, 5, 149\n$ wiki        &lt;str&gt; 'apple', 'asparagus', 'avocado', 'banana', 'chickpea', 'green_bean', 'beef', 'bell_pepper', 'callinectes_sapidus', 'broccoli'\n$ description &lt;str&gt; 'A common, round fruit produced by the tree &lt;i&gt;Malus domestica&lt;/i&gt;, cultivated in temperate climates.', 'Any of various perennial plants of the genus &lt;i&gt;Asparagus&lt;/i&gt; having leaflike stems, scalelike leaves, and small flowers.', 'The large, usually yellowish-green or black, pulpy fruit of the avocado tree.', 'An elongated curved tropical fruit that grows in bunches and has a creamy flesh and a smooth skin.', 'An annual Asian plant (&lt;i&gt;Cicer arietinum&lt;/i&gt;) in the pea family, widely cultivated for the edible seeds in its short inflated pods.', 'A long, slender variety of green bean.', 'The meat from a cow, bull or other bovine.', '&lt;i&gt;Capsicum annuum&lt;/i&gt;, an edible spicy-sweet fruit, originating in the New World.', 'A crustacean of the infraorder &lt;i&gt;Brachyura&lt;/i&gt;, having five pairs of legs, the foremost of which are in the form of claws, and a carapace.', 'A plant, &lt;i&gt;Brassica oleracea var. italica&lt;/i&gt;, of the cabbage family, Brassicaceae; especially, the tree-shaped flower and stalk that are eaten as a vegetable.'\n$ color       &lt;str&gt; 'red', 'green', 'green', 'yellow', 'brown', 'green', 'red', 'green', 'red', 'green'\n\n\n\nThe diet dataset is a small reference table containing 6 rows that define dietary compliance for major food groups. It links broad food_group categories (such as fruit, vegetable, grains, meat, fish, and dairy) to specific restrictive diets. Boolean-style columns (yes/no) indicate whether each group is permissible within vegan, vegetarian, and pescatarian lifestyles, effectively serving as a lookup table for filtering food items based on dietary restrictions.\n\ndiet = pl.read_csv(\"data/food_diet_restrictions.csv\")\ndiet.glimpse()\n\nRows: 6\nColumns: 4\n$ food_group  &lt;str&gt; 'fruit', 'vegetable', 'grains', 'meat', 'fish', 'dairy'\n$ vegan       &lt;str&gt; 'yes', 'yes', 'yes', 'no', 'no', 'no'\n$ vegetarian  &lt;str&gt; 'yes', 'yes', 'yes', 'no', 'no', 'yes'\n$ pescatarian &lt;str&gt; 'yes', 'yes', 'yes', 'no', 'yes', 'yes'\n\n\n\nThe recipe dataset provides a structural breakdown of culinary dishes, listing the specific components required to prepare them. Organized in a “long” format, each row represents a single ingredient for a given recipe, rather than a single row per dish. This means complex recipes like “Pot Roast” or “Guacamole” appear across multiple lines, each detailing a constituent item and its corresponding amount (in grams). This granular structure facilitates the aggregation of nutritional data by allowing individual ingredients to be linked back to detailed food profiles.\n\nrecipe = pl.read_csv(\"data/food_recipes.csv\")\nrecipe.glimpse()\n\nRows: 10\nColumns: 3\n$ recipe     &lt;str&gt; 'Pot Roast', 'Pot Roast', 'Pot Roast', 'Pot Roast', 'Pot Roast', 'Pot Roast', 'Guacamole', 'Guacamole', 'Guacamole', 'Guacamole'\n$ ingredient &lt;str&gt; 'Beef', 'Carrot', 'Potato', 'Onion', 'Tomato', 'Bay Leaf', 'Avocado', 'Onion', 'Tomato', 'Lime'\n$ amount     &lt;i64&gt; 1200, 400, 1000, 500, 200, 5, 1000, 500, 500, 150",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#majors-and-salary",
    "href": "22_datasets.html#majors-and-salary",
    "title": "22  Datasets",
    "section": "22.4 Majors and Salary",
    "text": "22.4 Majors and Salary\nSourced from the U.S. Bureau of Labor Statistics, the major dataset offers a high-resolution profile of the earnings distribution for various undergraduate fields of study. Unlike summary tables that report only a median income, this dataset uses a long-format structure to trace the entire salary curve, containing 8,316 rows that correspond to 99 percentile ranks for roughly 84 distinct majors. For each major, the data lists the percentile (from 1 to 99) and the associated earnings value at that rank. This granular approach allows for a deeper analysis of financial outcomes, enabling comparisons of income inequality within fields and assessing the risk-reward profiles—such as the reliable “floor” versus the potential “ceiling” of wages—across different career paths.\n\nmajor = pl.read_csv(\"data/majors.csv\")\nmajor.glimpse()\n\nRows: 8316\nColumns: 3\n$ major      &lt;str&gt; 'Accounting', 'Accounting', 'Accounting', 'Accounting', 'Accounting', 'Accounting', 'Accounting', 'Accounting', 'Accounting', 'Accounting'\n$ percentile &lt;i64&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n$ earnings   &lt;f64&gt; 1.538733, 1.673857, 1.760265, 1.816639, 1.872135, 1.933803, 1.980822, 2.019701, 2.059812, 2.090352",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#criterion-films",
    "href": "22_datasets.html#criterion-films",
    "title": "22  Datasets",
    "section": "22.5 Criterion Films",
    "text": "22.5 Criterion Films\nThe film dataset contains 1,479 entries from the Criterion Collection, a prestigious home-video distribution company dedicated to preserving and publishing “important classic and contemporary films” from around the world. Often regarded as a canon of cinema as an art form, the collection includes technically restored and historically significant works.\nThe dataset identifies each film by its standard title, release year, and unique imdb_id. It captures the creative backbone of each work through columns for directors, writers, and genre classifications, alongside production details like the country of origin, primary languages, and runtime. Critical reception is well-documented with aggregated scores from IMDb (including vote counts), Rotten Tomatoes, and Metacritic. Additionally, the table is enriched with encyclopedic context via Wikipedia extracts and descriptions, and occasionally includes financial metrics like production budgets and box office returns.\n\nfilm = pl.read_csv(\"data/criterion.csv\")\nfilm.glimpse()\n\nRows: 1479\nColumns: 18\n$ imdb_id               &lt;str&gt; 'tt0012349', 'tt0012364', 'tt0013257', 'tt0014429', 'tt0014624', 'tt0015634', 'tt0015768', 'tt0015841', 'tt0016142', 'tt0017075'\n$ title                 &lt;str&gt; 'The Kid', 'The Phantom Carriage', 'Häxan', 'Safety Last!', 'A Woman of Paris: A Drama of Fate', 'Body and Soul', 'Master of the House', 'The Freshman', 'The Mystic', 'The Lodger: A Story of the London Fog'\n$ year                  &lt;i64&gt; 1921, 1921, 1922, 1923, 1923, 1925, 1925, 1925, 1925, 1927\n$ language              &lt;str&gt; 'None|English', 'None|Swedish', 'Swedish|Danish', 'English', 'English', 'English', 'Danish', 'None|English', 'None|English', 'None'\n$ genre                 &lt;str&gt; 'Comedy|Drama|Family', 'Drama|Fantasy|Horror', 'Documentary|Fantasy|Horror', 'Action|Comedy|Thriller', 'Drama|Romance', 'Crime|Drama|Thriller', 'Comedy|Drama', 'Comedy|Family|Romance', 'Drama', 'Crime|Drama|Mystery'\n$ director              &lt;str&gt; 'Charles Chaplin', 'Victor Sjöström', 'Benjamin Christensen', 'Fred C. Newmeyer|Sam Taylor', 'Charles Chaplin', 'Oscar Micheaux', 'Carl Theodor Dreyer', 'Fred C. Newmeyer|Sam Taylor', 'Tod Browning', 'Alfred Hitchcock'\n$ writer                &lt;str&gt; 'Charles Chaplin', 'Selma Lagerlöf|Victor Sjöström', 'Benjamin Christensen', 'Hal Roach|Sam Taylor|Tim Whelan', 'Charles Chaplin', 'Oscar Micheaux', 'Carl Theodor Dreyer|Svend Rindom', 'Sam Taylor|Ted Wilde|John Grey', 'Tod Browning|Waldemar Young', 'Marie Belloc Lowndes|Eliot Stannard|Alfred Hitchcock'\n$ country               &lt;str&gt; 'United States', 'Sweden', 'Sweden|Denmark', 'United States', 'United States', 'United States', 'Denmark', 'United States', 'United States', 'United Kingdom'\n$ imdb_votes            &lt;i64&gt; 142797, 15311, 18391, 23503, 6548, 1221, 2506, 6373, 489, 14371\n$ rating_imdb           &lt;f64&gt; 8.2, 8.0, 7.6, 8.1, 6.9, 6.2, 7.0, 7.5, 6.7, 7.3\n$ rating_rt             &lt;i64&gt; 100, 100, 93, 97, 94, null, 100, 95, null, 96\n$ rating_mc             &lt;i64&gt; null, null, null, null, 76, null, null, null, null, 82\n$ runtime_raw           &lt;i64&gt; 68, 106, 107, 73, 84, 102, 107, 76, 70, 70\n$ wikipedia_pageid      &lt;i64&gt; 1346905, 7329426, 3644898, 76313, 546663, 1506585, 11072916, 3831825, 17325678, 287408\n$ wikipedia_description &lt;str&gt; '1921 silent film by Charlie Chaplin', '1921 film by Victor Sjöström', 'Swedish 1922 silent horror essay film', '1923 American silent romantic comedy film', '1923 drama film by Charlie Chaplin', '1925 film directed by Oscar Micheaux', '1925 film by Carl Theodor Dreyer', '1925 film', '1925 film', '1927 silent film by Alfred Hitchcock'\n$ wikipedia_extract     &lt;str&gt; \"The Kid is a 1921 American silent comedy-drama film written, produced, directed by and starring Charlie Chaplin, and features Jackie Coogan as his foundling baby, adopted son and sidekick. This was Chaplin's first full-length film as a director. It was a huge success and was the second-highest-grossing film in 1921. Now considered one of the greatest films of the silent era, it was selected for preservation in the United States National Film Registry by the Library of Congress in 2011.\", \"The Phantom Carriage is a 1921 Swedish silent film directed by and starring Victor Sjöström, based on the 1912 novel Thy Soul Shall Bear Witness! (Körkarlen) by Swedish author Selma Lagerlöf. In the film, Sjöström plays a drunkard named David Holm who, on the night of New Year's Eve, is compelled by the ghostly driver of Death's carriage to reflect on his past mistakes. Alongside Sjöström, the film's cast includes Hilda Borgström, Tore Svennberg, and Astrid Holm.\", \"Häxan is a 1922 Swedish-Danish silent horror essay film written and directed by Benjamin Christensen. Consisting partly of documentary-style storytelling as well as dramatized narrative sequences, the film purports to chart the historical roots and superstitions surrounding witchcraft, beginning in the Middle Ages through the 20th century. Based partly on Christensen's own study of the Malleus Maleficarum, a 15th-century German guide for inquisitors, Häxan proposes that such witch-hunts may have stemmed from misunderstandings of mental or neurological disorders, triggering mass hysteria.\", \"Safety Last! is a 1923 American silent romantic-comedy film starring Harold Lloyd. It includes one of the most famous images from the silent-film era: Lloyd clutching the hands of a large clock as he dangles from the outside of a skyscraper above moving traffic. The film was highly successful and critically hailed, and it cemented Lloyd's status as a major figure in early motion pictures. It is still popular at revivals, and it is viewed today as one of the great film comedies.\", 'A Woman of Paris is a 1923 silent drama film written, produced, and directed by Charlie Chaplin. It stars Edna Purviance as the title character, along with Clarence Geldart, Carl Miller, Lydia Knott, Charles K. French and Adolphe Menjou. A United Artists production, the film was an atypical dramatic work for Chaplin.', 'Body and Soul is a 1925 race film produced, written, directed, and distributed by Oscar Micheaux and starring Paul Robeson in his motion picture debut. In 2019, the film was selected by the Library of Congress for inclusion in the National Film Registry for being \"culturally, historically, or aesthetically significant\".', 'Master of the House is a 1925 Danish silent drama film directed and written by acclaimed filmmaker Carl Theodor Dreyer. The film marked the debut of Karin Nellemose, and it is regarded by many as a classic of Danish cinema.', \"The Freshman is a 1925 American silent comedy film that tells the story of a college freshman trying to become popular by joining the school football team. It stars Harold Lloyd, Jobyna Ralston, Brooks Benedict, and James Anderson. It remains one of Lloyd's most successful and enduring films. When the film opened on September 20 at the B.S. Moss Colony Theater on Broadway, Broderick & Felsen's production of Campus Capers was the opening act which was engaged for the full ten weeks of the film's run.\", \"The Mystic is a 1925 American MGM silent drama film directed by Tod Browning, who also co-wrote it with Waldemar Young. It is the only one of nine silent MGM films directed by Browning from 1925 to 1929 that does not star Lon Chaney. The film costars Aileen Pringle and Conway Tearle. Aileen Pringle's gowns in the film were by already famous Romain de Tirtoff. A print of the film exists.\", \"The Lodger: A Story of the London Fog is a 1927 British silent thriller film directed by Alfred Hitchcock and starring Marie Ault, Arthur Chesney, June Tripp, Malcolm Keen and Ivor Novello. Hitchcock's third feature film, it was released on 14 February 1927 in London and on 10 June 1928 in New York City. The film is based on the 1913 novel The Lodger by Marie Belloc Lowndes and the play Who Is He? co-written by Belloc Lowndes. Its plot concerns the hunt for a Jack the Ripper-like serial killer in London.\"\n$ budget_raw            &lt;i64&gt; 250000, null, 2000000, 121000, 351000, null, null, 301681, null, 12000\n$ box_office_raw        &lt;i64&gt; null, null, null, null, 634000, null, null, null, null, null",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#fifty-years-of-movies",
    "href": "22_datasets.html#fifty-years-of-movies",
    "title": "22  Datasets",
    "section": "22.6 Fifty Years of Movies",
    "text": "22.6 Fifty Years of Movies\nThe movie dataset serves as the central hub, containing 5,000 observations that represent the top-100 grossing U.S. films for each year from 1970 to 2020. It captures essential metadata such as the film’s title, release year, MPA rating, and runtime, alongside measures of commercial and critical success like gross revenue, IMDb user ratings/vote counts, and Metacritic scores. Uniquely, it also includes computer vision metrics derived from the film’s promotional poster, quantifying visual attributes such as poster_brightness, saturation, and edgeness (a measure of visual complexity).\n\nmovie = pl.read_csv(\"data/movies_50_years.csv\")\nmovie.glimpse()\n\nRows: 5000\nColumns: 12\n$ year              &lt;i64&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970\n$ title             &lt;str&gt; 'Love Story', 'Airport', 'MASH', 'Patton', 'The AristoCats', 'Little Big Man', 'Tora! Tora! Tora!', 'Catch-22', 'The Owl and the Pussycat', 'Joe'\n$ mpa               &lt;str&gt; 'PG', 'G', 'R', 'GP', 'G', 'PG-13', 'G', 'R', 'PG', 'R'\n$ runtime           &lt;str&gt; '100', '137', '116', '172', '78', '139', '144', '122', '95', '107'\n$ gross             &lt;str&gt; '106.4', '100.49', '81.6', '61.7', '37.68', '31.56', '29.55', '24.91', '23.68', '19.32'\n$ rating_count      &lt;i64&gt; 28330, 16512, 64989, 90461, 87551, 31412, 30347, 20997, 3107, 2633\n$ rating            &lt;str&gt; '6.9', '6.6', '7.5', '7.9', '7.1', '7.6', '7.5', '7.2', '6.5', '6.8'\n$ metacritic        &lt;str&gt; 'NA', '42', 'NA', 'NA', 'NA', 'NA', '46', 'NA', 'NA', 'NA'\n$ poster_brightness &lt;str&gt; '79.039734052134', '70.73515993593905', '74.5400023238925', '83.12899118937443', '79.79474571281945', '67.96583791250038', '39.79528775599128', '62.28054483453459', '67.22113941912305', '31.826858122949158'\n$ poster_saturation &lt;str&gt; '8.029792248510015', '29.28457189363516', '40.103629182765395', '17.433849565817365', '12.481991945072151', '9.016387405954426', '48.60645939534094', '35.96620937559402', '10.24255420931898', '27.578054400938452'\n$ poster_edgeness   &lt;str&gt; '4.586166444178613', '4.954734636760736', '3.5102847848915624', '3.657573618987647', '4.400358220849864', '5.359519670438313', '2.1121361608919753', '3.6546805917391145', '4.894551155673808', '4.229543347907548'\n$ description       &lt;str&gt; 'A boy and a girl from different backgrounds fall in love regardless of their upbringing - and then tragedy strikes.', 'A bomber on board an airplane, an airport almost closed by snow, and various personal problems of the people involved.', 'The staff of a Korean War field hospital use humor and high jinks to keep their sanity in the face of the horror of war.', 'The World War II phase of the career of controversial American general George S. Patton.', 'With the help of a smooth talking tomcat, a family of Parisian felines set to inherit a fortune from their owner try to make it back home after a jealous butler kidnaps them and leaves them in the country.', 'Jack Crabb, looking back from extreme old age, tells of his life being raised by Native Americans and fighting with General Custer.', 'In 1941, following months of economic embargo, Japan prepares to open its war against the United States with a surprise attack on the US naval base at Pearl Harbor.', 'A man is trying desperately to be certified insane during World War II, so he can stop flying missions.', 'A stuffy author enters into an explosive relationship with his neighbor, a foul-mouthed, freewheeling prostitute.', \"Two men, Bill, a wealthy conservative, and Joe, a far-right factory worker, form a dangerous bond after Bill confesses to murdering his daughter's drug dealer boyfriend to Joe.\"\n\n\n\nThe color dataset provides a detailed breakdown of the color palettes used in the film posters. Structured in a long format, it links each movie to multiple rows representing specific color categories—spanning hues like “red” or “blue” and greyscale tones like “black” or “white.” The percentage column quantifies the dominance of each color, enabling the analysis of visual trends in movie marketing over the last half-century (such as the rise of darker or more saturated poster designs).\n\ncolor = pl.read_csv(\"data/movies_50_years_color.csv\")\ncolor.glimpse()\n\nRows: 46980\nColumns: 5\n$ year       &lt;i64&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970\n$ title      &lt;str&gt; 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story'\n$ color_type &lt;str&gt; 'hue', 'hue', 'hue', 'hue', 'hue', 'hue', 'greyscale', 'greyscale', 'greyscale', 'hue'\n$ color      &lt;str&gt; 'red', 'orange', 'yellow', 'green', 'blue', 'violet', 'black', 'grey', 'white', 'other'\n$ percentage &lt;f64&gt; 2.6356547746208077, 3.0933561204870754, 0.05420850245674002, 0.2360606707968383, 0.31937620166631064, 0.0005340739158299509, 10.963736381115147, 6.9082461012604135, 75.78882717368084, 0.0\n\n\n\nThe genre dataset acts as a mapping table to handle the one-to-many relationship between films and their narrative categories. Since a single movie often fits into multiple classifications (e.g., a film that is both “Action” and “Sci-Fi”), this table lists each genre tag on a separate row. This structure allows for precise filtering and aggregation, facilitating analysis of how genre popularity—like the decline of Westerns or the rise of Superhero films—has shifted over the 50-year period.\n\ngenre = pl.read_csv(\"data/movies_50_years_genre.csv\")\ngenre.glimpse()\n\nRows: 11887\nColumns: 3\n$ year  &lt;i64&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970\n$ title &lt;str&gt; 'Love Story', 'Love Story', 'Airport', 'Airport', 'Airport', 'MASH', 'MASH', 'MASH', 'Patton', 'Patton'\n$ genre &lt;str&gt; 'Drama', 'Romance', 'Action', 'Drama', 'Thriller', 'Comedy', 'Drama', 'War', 'Biography', 'Drama'\n\n\n\nThe people dataset details the key creative talent behind each film, listing the director and the top four billed actors (“starring”) ranked by prominence. Beyond simply naming the individuals, this table enriches the data with demographic inference: it includes predicted gender classifications and a confidence score (gender_conf) for each name. These predictions are derived from U.S. Social Security name data, allowing for longitudinal studies of gender representation in top-tier Hollywood productions.\n\npeople = pl.read_csv(\"data/movies_50_years_people.csv\")\npeople.glimpse()\n\nRows: 24648\nColumns: 7\n$ year        &lt;i64&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970\n$ title       &lt;str&gt; 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Airport', 'Airport', 'Airport', 'Airport', 'Airport'\n$ role        &lt;str&gt; 'director', 'starring', 'starring', 'starring', 'starring', 'director', 'director', 'starring', 'starring', 'starring'\n$ rank        &lt;i64&gt; 1, 1, 2, 3, 4, 1, 2, 1, 2, 3\n$ person      &lt;str&gt; 'Arthur Hiller', 'Ali MacGraw', \"Ryan O'Neal\", 'John Marley', 'Ray Milland', 'George Seaton', 'Henry Hathaway', 'Burt Lancaster', 'Dean Martin', 'George Kennedy'\n$ gender      &lt;str&gt; 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male'\n$ gender_conf &lt;str&gt; '0.9937', '0.6877', '0.9768', '0.9961', '0.984', '0.9932', '0.9935', '1', '0.9875', '0.9932'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#rva-flights",
    "href": "22_datasets.html#rva-flights",
    "title": "22  Datasets",
    "section": "22.7 RVA Flights",
    "text": "22.7 RVA Flights\nSourced from the U.S. Bureau of Transportation, these five datasets provide a comprehensive record of commercial aviation activity departing from Richmond International Airport (RIC) during its record-breaking year of 2019.\nThe rva dataset is the central fact table, containing 24,808 rows that represent the complete set of commercial departures from Richmond for the year. It captures the pulse of daily operations, logging scheduling data (planned vs. actual departure/arrival times), delays, and routing information (origin to dest). It also serves as the connector for the other tables, linking to them via keys like carrier, tailnum, and time_hour.\n\nrva = pl.read_csv(\"data/flightsrva_flights.csv.gz\", null_values=[\"NA\"])\nrva.glimpse()\n\nRows: 24808\nColumns: 19\n$ year           &lt;i64&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019\n$ month          &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ day            &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ dep_time       &lt;i64&gt; 548, 552, 558, 630, 639, 641, 648, 655, 717, 734\n$ sched_dep_time &lt;i64&gt; 550, 600, 600, 630, 645, 645, 654, 700, 725, 730\n$ dep_delay      &lt;i64&gt; -2, -8, -2, 0, -6, -4, -6, -5, -8, 4\n$ arr_time       &lt;i64&gt; 728, 814, 817, 713, 748, 827, 910, 933, 828, 903\n$ sched_arr_time &lt;i64&gt; 740, 824, 810, 729, 824, 847, 924, 945, 855, 850\n$ arr_delay      &lt;i64&gt; -12, -10, 7, -16, -36, -20, -14, -12, -27, 13\n$ carrier        &lt;str&gt; 'WN', 'B6', 'YX', 'YV', 'AA', 'DL', 'YX', 'AA', '9E', 'UA'\n$ flight         &lt;i64&gt; 25, 33, 135, 145, 58, 28, 117, 62, 79, 29\n$ tailnum        &lt;str&gt; 'N485WN', 'N624JB', 'N818MD', 'N88327', 'N680AW', 'N960DN', 'N443YX', 'N930AU', 'N8943A', 'N29717'\n$ origin         &lt;str&gt; 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC'\n$ dest           &lt;str&gt; 'ATL', 'FLL', 'MSP', 'IAD', 'CLT', 'ATL', 'MIA', 'DFW', 'JFK', 'ORD'\n$ air_time       &lt;i64&gt; 85, 115, 161, 26, 52, 85, 119, 194, 54, 121\n$ distance       &lt;i64&gt; 481, 805, 970, 100, 257, 481, 825, 1158, 288, 642\n$ hour           &lt;i64&gt; 5, 6, 6, 6, 6, 6, 6, 7, 7, 7\n$ minute         &lt;i64&gt; 50, 0, 0, 30, 45, 45, 54, 0, 25, 30\n$ time_hour      &lt;str&gt; '2019-01-01T05:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T07:00:00Z', '2019-01-01T07:00:00Z', '2019-01-01T07:00:00Z'\n\n\n\nThe weather dataset offers an hourly meteorological log for the airport, containing 8,735 observations. It tracks environmental conditions—such as wind speed, visibility, and humidity—that are critical for analyzing flight delays. The time_hour column allows this data to be precisely joined with flight departures to assess the impact of weather on airport performance.\n\nweather = pl.read_csv(\"data/flightsrva_weather.csv.gz\", null_values=[\"NA\"])\nweather.glimpse()\n\nRows: 8735\nColumns: 15\n$ origin     &lt;str&gt; 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC'\n$ year       &lt;i64&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019\n$ month      &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ day        &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ hour       &lt;i64&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n$ temp       &lt;str&gt; null, null, null, null, null, null, null, null, null, null\n$ dewp       &lt;str&gt; null, null, null, null, null, null, null, null, null, null\n$ humid      &lt;str&gt; null, null, null, null, null, null, null, null, null, null\n$ wind_dir   &lt;i64&gt; 180, 180, 180, 180, 190, 200, 200, 210, 220, 210\n$ wind_speed &lt;f64&gt; 8.05546, 12.658579999999999, 13.809359999999998, 13.809359999999998, 12.658579999999999, 19.56326, 17.261699999999998, 18.41248, 16.11092, 16.11092\n$ wind_gust  &lt;f64&gt; 9.2700622588, 14.567240692399997, 15.891535300799996, 15.891535300799996, 14.567240692399997, 22.5130083428, 19.864419125999994, 21.188713734399997, 18.5401245176, 18.5401245176\n$ precip     &lt;f64&gt; null, null, null, null, null, null, null, null, null, null\n$ pressure   &lt;str&gt; null, null, null, null, null, null, null, null, null, null\n$ visib      &lt;f64&gt; 1.0, 1.5, 1.5, 4.0, 9.0, 10.0, 10.0, 10.0, 10.0, 10.0\n$ time_hour  &lt;str&gt; '2019-01-01T05:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T07:00:00Z', '2019-01-01T08:00:00Z', '2019-01-01T09:00:00Z', '2019-01-01T10:00:00Z', '2019-01-01T11:00:00Z', '2019-01-01T12:00:00Z', '2019-01-01T13:00:00Z', '2019-01-01T14:00:00Z'\n\n\n\nThe airport dataset acts as a geospatial lookup table for the 1,251 US destinations accessible from Richmond. It maps three-letter FAA codes (like “ATL” or “ORD”) to their full names, time zones, and exact latitude/longitude coordinates, enabling the mapping of flight paths and the calculation of distance-based metrics.\n\nairport = pl.read_csv(\"data/flightsrva_airports.csv.gz\", null_values=[\"NA\"])\nairport.glimpse()\n\nRows: 1251\nColumns: 8\n$ faa   &lt;str&gt; 'AAF', 'AAP', 'ABE', 'ABI', 'ABL', 'ABQ', 'ABR', 'ABY', 'ACK', 'ACT'\n$ name  &lt;str&gt; 'Apalachicola Regional Airport', 'Andrau Airpark', 'Lehigh Valley International Airport', 'Abilene Regional Airport', 'Ambler Airport', 'Albuquerque International Sunport', 'Aberdeen Regional Airport', 'Southwest Georgia Regional Airport', 'Nantucket Memorial Airport', 'Waco Regional Airport'\n$ lat   &lt;f64&gt; 29.72750092, 29.7224998474, 40.652099609375, 32.4113006592, 67.1063, 35.040199, 45.449100494384766, 31.535499572753903, 41.25310135, 31.611299514770508\n$ lon   &lt;f64&gt; -85.02749634, -95.5883026123, -75.44080352783203, -99.6819000244, -157.856989, -106.609001, -98.42179870605467, -84.19450378417969, -70.06020355, -97.23049926757812\n$ alt   &lt;i64&gt; 20, 79, 393, 1791, 334, 5355, 1302, 197, 47, 516\n$ tz    &lt;i64&gt; -5, -6, -5, -6, -9, -7, -6, -5, -5, -6\n$ dst   &lt;str&gt; 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A'\n$ tzone &lt;str&gt; 'America/New_York', 'America/Chicago', 'America/New_York', 'America/Chicago', 'America/Anchorage', 'America/Denver', 'America/Chicago', 'America/New_York', 'America/New_York', 'America/Chicago'\n\n\n\nThe airline dataset is a small reference table linking the 14 unique two-letter carrier codes found in the flight logs (e.g., “AA”, “DL”) to their full corporate names (e.g., “American Airlines Inc.”, “Delta Air Lines Inc.”).\n\nairline = pl.read_csv(\"data/flightsrva_airlines.csv.gz\", null_values=[\"NA\"])\nairline.glimpse()\n\nRows: 14\nColumns: 2\n$ carrier &lt;str&gt; '9E', 'AA', 'B6', 'DL', 'EV', 'G4', 'MQ', 'NK', 'OH', 'OO'\n$ name    &lt;str&gt; 'Endeavor Air Inc.', 'American Airlines Inc.', 'JetBlue Airways', 'Delta Air Lines Inc.', 'ExpressJet Airlines LLC d/b/a aha!', 'Allegiant Air', 'Envoy Air', 'Spirit Air Lines', 'PSA Airlines Inc.', 'SkyWest Airlines Inc.'\n\n\n\nThe plane dataset provides a technical registry for the aircraft used in these flights. Indexed by unique tail numbers, it details the hardware specifications for 3,120 individual planes, including their manufacturer, model year, engine type, and seating capacity, allowing for analysis of fleet modernization and equipment usage.\n\nplane = pl.read_csv(\"data/flightsrva_planes.csv.gz\", null_values=[\"NA\"])\nplane.glimpse()\n\nRows: 3120\nColumns: 9\n$ tailnum      &lt;str&gt; 'N101HQ', 'N102HQ', 'N102UW', 'N103HQ', 'N103SY', 'N103US', 'N104HQ', 'N104UW', 'N10575', 'N105HQ'\n$ year         &lt;i64&gt; 2007, 2007, 1998, 2007, 2014, 1999, 2007, 1999, 2002, 2007\n$ type         &lt;str&gt; 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine'\n$ manufacturer &lt;str&gt; 'EMBRAER-EMPRESA BRASILEIRA DE', 'EMBRAER-EMPRESA BRASILEIRA DE', 'AIRBUS INDUSTRIE', 'EMBRAER-EMPRESA BRASILEIRA DE', 'EMBRAER S A', 'AIRBUS INDUSTRIE', 'EMBRAER-EMPRESA BRASILEIRA DE', 'AIRBUS INDUSTRIE', 'EMBRAER', 'EMBRAER'\n$ model        &lt;str&gt; 'ERJ 170-200 LR', 'ERJ 170-200 LR', 'A320-214', 'ERJ 170-200 LR', 'ERJ 170-200 LR', 'A320-214', 'ERJ 170-200 LR', 'A320-214', 'EMB-145LR', 'ERJ 170-200 LR'\n$ engines      &lt;i64&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n$ seats        &lt;i64&gt; 80, 80, 182, 80, 88, 182, 80, 182, 55, 88\n$ speed        &lt;i64&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ engine       &lt;str&gt; 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#what-we-eat-in-america",
    "href": "22_datasets.html#what-we-eat-in-america",
    "title": "22  Datasets",
    "section": "22.8 What We Eat in America",
    "text": "22.8 What We Eat in America\nThe wweia dataset serves as a granular log of dietary intake events, containing over 173,000 observations where each row represents a specific food item consumed by a participant. It captures the “what, when, and where” of eating habits: identifying the item via a standard food_code, pinpointing the occasion with temporal markers (time, day_of_week, meal_name), and noting the origin (food_source) and location (at_home). Crucially, this transactional table details the nutritional impact of each specific portion, recording the mass in grams and providing a breakdown of energy (kcal), macronutrients (protein, carbs, fat, sugar), and other constituents like caffeine and alcohol.\n\nwweia = pl.read_csv(\"data/wweia_food.csv\", ignore_errors=True)\nwweia.glimpse()\n\nRows: 173174\nColumns: 15\n$ id          &lt;i64&gt; 109263, 109263, 109263, 109263, 109263, 109263, 109263, 109263, 109263, 109263\n$ food_code   &lt;i64&gt; 28320300, 91746110, 58106210, 64104010, 11710801, 54304020, 57124200, 94000100, 11710801, 94000100\n$ day_of_week &lt;i64&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5\n$ time        &lt;i64&gt; 19, 18, 12, 16, 19, 14, 16, 8, 8, 14\n$ meal_name   &lt;str&gt; 'Dinner', 'Snack', 'Lunch', 'Snack', 'Dinner', 'Snack', 'Snack', 'Extended consumption', 'Breakfast', 'Snack'\n$ food_source &lt;str&gt; 'Store - grocery/supermarket', 'Child/Adult care center', 'Child/Adult care center', 'Store - grocery/supermarket', 'Store - grocery/supermarket', 'Child/Adult care center', 'Store - grocery/supermarket', 'NA', 'Store - grocery/supermarket', 'NA'\n$ at_home     &lt;str&gt; 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No'\n$ grams       &lt;f64&gt; 199.5, 20.0, 238.0, 209.0, 124.0, 10.0, 11.67, 105.0, 130.2, 105.0\n$ kcal        &lt;i64&gt; 114, 101, 633, 99, 123, 49, 45, 0, 129, 0\n$ protein     &lt;f64&gt; 12.11, 2.2, 27.11, 0.19, 3.55, 1.09, 0.63, 0.0, 3.72, 0.0\n$ carbs       &lt;f64&gt; 5.07, 12.03, 79.33, 23.71, 13.83, 5.94, 9.45, 0.0, 14.52, 0.0\n$ sugar       &lt;f64&gt; 2.13, 10.4, 8.52, 21.17, 13.02, 0.45, 4.02, 0.0, 13.67, 0.0\n$ fat         &lt;f64&gt; 4.95, 5.09, 23.06, 0.52, 5.92, 2.27, 0.58, 0.0, 6.21, 0.0\n$ caffeine    &lt;i64&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 0\n$ alcohol     &lt;i64&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n\n\n\nThe demo dataset provides the socioeconomic and demographic context for the 13,724 survey participants, linked to the food log by a unique id. It constructs a profile for each individual, tracking fundamental attributes such as age, gender, and race, alongside indicators of social status like education level (edu_level) and family structure. Economic wellbeing is quantified by the ratio_to_poverty (the ratio of family income to the federal poverty threshold), allowing researchers to analyze how diet quality varies across different income brackets and population segments.\n\ndemo = pl.read_csv(\"data/wweia_demo.csv\")\ndemo.glimpse()\n\nRows: 13724\nColumns: 8\n$ id               &lt;i64&gt; 109263, 109264, 109265, 109266, 109269, 109270, 109271, 109272, 109273, 109274\n$ age              &lt;i64&gt; 2, 13, 2, 29, 2, 11, 49, 0, 36, 68\n$ gender           &lt;str&gt; 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male'\n$ edu_level        &lt;str&gt; 'NA', 'NA', 'NA', '5', 'NA', 'NA', '2', 'NA', '4', '4'\n$ race             &lt;str&gt; 'Other', 'Mexican American', 'White', 'Other', 'Other Hispanic', 'Black', 'White', 'Mexican American', 'White', 'Missing'\n$ family_status    &lt;str&gt; 'NA', 'NA', 'NA', 'Other', 'NA', 'NA', 'Other', 'NA', 'Other', 'Other'\n$ ratio_to_poverty &lt;str&gt; '4.66', '0.83', '3.06', '5', '0.96', '1.88', 'NA', '0.73', '0.83', '1.2'\n$ lang_interview   &lt;str&gt; 'English', 'English', 'English', 'English', 'English', 'English', 'English', 'English', 'English', 'English'\n\n\n\nThe meta dataset acts as the definitive taxonomy for the survey, containing 7,444 entries that map the numeric food_code found in the consumption logs to human-readable definitions. It organizes the vast array of food items into a hierarchical structure, linking specific descriptions (e.g., “Milk, low sodium, whole”) to broader category_descriptions (e.g., “Milk, whole”) and high-level food_group classifications (e.g., “Milk and Dairy”). This reference table is essential for aggregating granular food data into meaningful dietary patterns consistent with nutritional guidelines.\n\nmeta = pl.read_csv(\"data/wweia_meta.csv\")\nmeta.glimpse()\n\nRows: 7444\nColumns: 7\n$ food_code             &lt;i64&gt; 11000000, 11100000, 11111000, 11111100, 11111150, 11111160, 11111170, 11112110, 11112120, 11112130\n$ food_code_description &lt;str&gt; 'Milk, human', 'Milk, NFS', 'Milk, whole', 'Milk, low sodium, whole', 'Milk, calcium fortified, whole', 'Milk, calcium fortified, low fat (1%)', 'Milk, calcium fortified, fat free (skim)', 'Milk, reduced fat (2%)', 'Milk, acidophilus, low fat (1%)', 'Milk, acidophilus, reduced fat (2%)'\n$ category_number       &lt;i64&gt; 9602, 1004, 1002, 1002, 1002, 1006, 1008, 1004, 1006, 1004\n$ category_description  &lt;str&gt; 'Human milk', 'Milk, reduced fat', 'Milk, whole', 'Milk, whole', 'Milk, whole', 'Milk, lowfat', 'Milk, nonfat', 'Milk, reduced fat', 'Milk, lowfat', 'Milk, reduced fat'\n$ meta_number           &lt;i64&gt; 96, 10, 10, 10, 10, 10, 10, 10, 10, 10\n$ meta_name             &lt;str&gt; 'Human Milk', 'Milk', 'Milk', 'Milk', 'Milk', 'Milk', 'Milk', 'Milk', 'Milk', 'Milk'\n$ food_group            &lt;str&gt; 'Baby Foods', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#inference-data",
    "href": "22_datasets.html#inference-data",
    "title": "22  Datasets",
    "section": "22.9 Inference Data",
    "text": "22.9 Inference Data\nDerived from the CDC’s 2010 National Survey of Family Growth (NSFG), the marriage dataset is a focused univariate collection regarding family formation trends. It consists of a single column, age, which records the age in years at which 5,534 U.S. women entered into their first marriage. This simple numeric vector serves as a foundational sample for estimating population parameters—such as the median age of first marriage—and analyzing shifts in nuptiality over time.\n\nmarriage = pl.read_csv(\"data/inference_age_at_mar.csv\")\nmarriage.glimpse()\n\nRows: 5534\nColumns: 1\n$ age &lt;i64&gt; 32, 25, 24, 26, 32, 29, 23, 23, 29, 27\n\n\n\nOriginating from a study in rural New South Wales, Australia, the absent dataset investigates the factors influencing school attendance among 146 primary school students. The target variable, days, counts the total days a student was absent during the school year. These figures are contextualized by categorical demographic and academic indicators, including ethnicity (eth), gender (sex), age group (age), and lrn, a classification of the student’s learning status.\n\nabsent = pl.read_csv(\"data/inference_absenteeism.csv\")\nabsent.glimpse()\n\nRows: 146\nColumns: 5\n$ eth  &lt;str&gt; 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A'\n$ sex  &lt;str&gt; 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M'\n$ age  &lt;str&gt; 'F0', 'F0', 'F0', 'F0', 'F0', 'F0', 'F0', 'F0', 'F1', 'F1'\n$ lrn  &lt;str&gt; 'SL', 'SL', 'SL', 'AL', 'AL', 'AL', 'AL', 'AL', 'SL', 'SL'\n$ days &lt;i64&gt; 2, 11, 14, 5, 5, 13, 20, 22, 6, 6\n\n\n\nThe sulph dataset captures the results of a clinical control trial testing the efficacy of the drug sulphinpyrazone in post-heart attack care. It tracks 1,475 patients, dividing them into experimental and placebo arms via the group column. The primary endpoint is recorded in the binary outcome column (“lived” or “died”), creating a classic contingency structure used to calculate odds ratios and determine if the drug provides a statistically significant survival benefit compared to the control.\n\nsulph = pl.read_csv(\"data/inference_sulphinpyrazone.csv\")\nsulph.glimpse()\n\nRows: 1475\nColumns: 2\n$ group   &lt;str&gt; 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control'\n$ outcome &lt;str&gt; 'died', 'died', 'died', 'died', 'died', 'died', 'died', 'died', 'died', 'died'\n\n\n\nSourced from a survey of 1,325 UCLA students, the speed dataset combines physiological metrics with self-reported risk behavior. It logs the student’s sex and height (in inches), alongside a behavioral metric: speed, representing the fastest speed the student has ever driven a vehicle (presumably in mph). This combination allows for inference tasks such as testing for gender-based differences in driving habits or exploring correlations between physical stature and risk-taking.\n\nspeed = pl.read_csv(\"data/inference_speed_sex_height.csv\")\nspeed.glimpse()\n\nRows: 1279\nColumns: 3\n$ speed  &lt;i64&gt; 85, 40, 87, 110, 110, 120, 90, 90, 80, 95\n$ sex    &lt;str&gt; 'female', 'male', 'female', 'female', 'male', 'female', 'female', 'female', 'female', 'male'\n$ height &lt;f64&gt; 69.0, 71.0, 64.0, 60.0, 70.0, 61.0, 65.0, 65.0, 61.0, 69.0\n\n\n\nThe possum dataset provides a morphometric profile of 104 brushtail possums captured across Australia and New Guinea. Aside from sex and age estimates, the data tracks geographic provenance through site codes and population regions (pop, e.g., “Vic” for Victoria). The dataset is defined by its precise biological measurements in millimeters—specifically head_l (head length), skull_w (skull width), total_l (total body length), and tail_l (tail length)—which are often used to classify subspecies or study regional physical variations.\n\npossum = pl.read_csv(\"data/inference_possum.csv\")\npossum.glimpse()\n\nRows: 104\nColumns: 8\n$ site    &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ pop     &lt;str&gt; 'Vic', 'Vic', 'Vic', 'Vic', 'Vic', 'Vic', 'Vic', 'Vic', 'Vic', 'Vic'\n$ sex     &lt;str&gt; 'm', 'f', 'f', 'f', 'f', 'f', 'm', 'f', 'f', 'f'\n$ age     &lt;str&gt; '8', '6', '6', '6', '2', '1', '2', '6', '9', '6'\n$ head_l  &lt;f64&gt; 94.1, 92.5, 94.0, 93.2, 91.5, 93.1, 95.3, 94.8, 93.4, 91.8\n$ skull_w &lt;f64&gt; 60.4, 57.6, 60.0, 57.1, 56.3, 54.8, 58.2, 57.6, 56.3, 58.0\n$ total_l &lt;f64&gt; 89.0, 91.5, 95.5, 92.0, 85.5, 90.5, 89.5, 91.0, 91.5, 89.5\n$ tail_l  &lt;f64&gt; 36.0, 36.5, 39.0, 38.0, 36.0, 35.5, 36.0, 37.0, 37.0, 37.5",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#keylogging",
    "href": "22_datasets.html#keylogging",
    "title": "22  Datasets",
    "section": "22.10 Keylogging",
    "text": "22.10 Keylogging\nThe klog dataset is a high-resolution behavioral log capturing the precise keystroke dynamics of students writing in English. With over 1.1 million observations, each row represents a single key press or input event. The data records the temporal flow of writing through timestamps (t0 for press, t1 for release) and calculated durations (dur), offering insight into motor processing and cognitive hesitation. The input and code columns differentiate between the resulting character (e.g., “I”) and the physical key actuated (e.g., “KeyI” or “Space”), allowing for the reconstruction of the text and the analysis of editing behaviors like backspacing or pausing.\n\nklog = pl.read_csv(\"data/keylog.csv.gz\")\nklog.glimpse()\n\nRows: 1145051\nColumns: 7\n$ id        &lt;str&gt; 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP'\n$ t0        &lt;f64&gt; 20914.10000000009, 21146.20000000018, 21234.30000000028, 22074.10000000009, 22306.20000000018, 23674.30000000028, 23818.39999999991, 24043.70000000018, 25066.30000000028, 25170.20000000018\n$ t1        &lt;f64&gt; 20978.5, 21226.30000000028, 21290.10000000009, 22154.30000000028, 22394.39999999991, 23738.60000000009, 23874.5, 24090.30000000028, 25130.30000000028, 25250.0\n$ dur       &lt;f64&gt; 64.3999999999069, 80.1000000000968, 55.7999999998137, 80.2000000001863, 88.1999999997242, 64.2999999998137, 56.1000000000931, 46.6000000000968, 64.0, 79.7999999998174\n$ dur_after &lt;f64&gt; 80.1000000000968, 55.7999999998137, 80.2000000001863, 88.1999999997242, 64.2999999998137, 56.1000000000931, 46.6000000000968, 64.0, 79.7999999998174, 72.2999999998174\n$ input     &lt;str&gt; 'I', 'f', null, 'I', null, 'c', 'o', 'u', 'l', 'd'\n$ code      &lt;str&gt; 'KeyI', 'KeyF', 'Space', 'KeyI', 'Space', 'KeyC', 'KeyO', 'KeyU', 'KeyL', 'KeyD'\n\n\n\nThe meta dataset provides the demographic and linguistic context for the 823 participants tracked in the keylogs. It links each unique session id to the writer’s age and, crucially, their native language background (lang). The dataset also includes a cefr rating (Common European Framework of Reference for Languages) , which categorizes their English proficiency into standard levels such as “B1/B2” (independent user) or “C1/C2” (proficient user). This metadata enables comparative analysis of how L1 background and L2 proficiency manifest in low-level typing patterns.\n\nmeta = pl.read_csv(\"data/keylog-meta.csv.gz\")\nmeta.glimpse()\n\nRows: 823\nColumns: 4\n$ id   &lt;str&gt; 'R_2EGIsZARLydD3Uc', 'R_1obCaysaZCWZXoG', 'R_3fqTek829k38iCk', 'R_brxD7Q5ZnPW8Gn7', 'R_1k1RE78cBbZyZMA', 'R_1NwuZMzRkVIR0WT', 'R_2t8LOS9nQDBQPA8', 'R_239Q0X5YLwB7U6Z', 'R_10xbkjEmnsusfb1', 'R_10CbLBzAnYKgWxB'\n$ age  &lt;i64&gt; 25, 22, 22, 43, 23, 32, 24, 28, 32, 21\n$ lang &lt;str&gt; 'Italian', 'Spanish', 'Polish', 'English', 'Polish', 'English', 'Spanish', 'English', 'Polish', 'Polish'\n$ cefr &lt;str&gt; 'C1/C2', 'B1/B2', 'B1/B2', 'C1/C2', 'B1/B2', 'C1/C2', 'C1/C2', 'C1/C2', 'B1/B2', 'B1/B2'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#paris-metro",
    "href": "22_datasets.html#paris-metro",
    "title": "22  Datasets",
    "section": "22.11 Paris Metro ",
    "text": "22.11 Paris Metro \nThe pmetro dataset captures the geospatial layout of the Paris Métro system, containing 371 entries that represent individual station stops or track segments. Each row identifies a station by name and links it to its specific line number and official branding line_color (provided as a hex code). Uniquely, the dataset is structured to facilitate network visualization rather than just point plotting: in addition to the station’s own coordinates (lat, lon), it includes lat_end and lon_end columns. This “start-to-end” structure effectively defines the edges between stations, allowing for the reconstruction of the connected path of each subway line.\n\npmetro = pl.read_csv(\"data/paris_metro_stops.csv\")\npmetro.glimpse()\n\nRows: 371\nColumns: 7\n$ name       &lt;str&gt; 'La Defense - Grande Arche', 'Esplanade de la Defense', 'Pont de Neuilly (Avenue de Madrid)', \"Les Sablons (Jardin d'acclimatation)\", 'Argentine', 'Charles De Gaulle-Etoile', 'George-V', 'Franklin D.Roosevelt', 'Champs-Elysees-Clemenceau', 'Concorde'\n$ line       &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ line_color &lt;str&gt; '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00'\n$ lon        &lt;f64&gt; 2.237018056395013, 2.247932435324861, 2.260515077888117, 2.271686721050983, 2.289322589613773, 2.295904906076514, 2.300560451248796, 2.30747079344783, 2.313545549946741, 2.322943412243541\n$ lat        &lt;f64&gt; 48.892187076449495, 48.88863121777117, 48.884708201322525, 48.88119152058607, 48.87559404986666, 48.87514981973562, 48.872023809500426, 48.86980822019895, 48.86790534489708, 48.86628580458387\n$ lon_end    &lt;str&gt; '2.247932435324861', '2.260515077888117', '2.271686721050983', '2.289322589613773', '2.295904906076514', '2.300560451248796', '2.30747079344783', '2.313545549946741', '2.322943412243541', '2.330129877112861'\n$ lat_end    &lt;str&gt; '48.88863121777117', '48.884708201322525', '48.88119152058607', '48.87559404986666', '48.87514981973562', '48.872023809500426', '48.86980822019895', '48.86790534489708', '48.86628580458387', '48.864343778733904'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#us-city-population",
    "href": "22_datasets.html#us-city-population",
    "title": "22  Datasets",
    "section": "22.12 US City Population ",
    "text": "22.12 US City Population \nThe us_pop dataset traces the demographic evolution of the United States through a historical record of urban growth. Spanning from the first census in 1790 through 2010, it logs the population (measured in thousands) for distinct cities identified by name and state. The data appears to track modern cities backward in time, showing values of 0.0 for years prior to a city’s founding or incorporation (e.g., Anchorage in 1790). Enriched with geospatial coordinates (lat and lon), this longitudinal collection facilitates the analysis of urbanization patterns, capturing the country’s westward expansion and the explosive growth of metropolitan hubs over two centuries.\n\nus_pop = pl.read_csv(\"data/us_city_population.csv\")\nus_pop.glimpse()\n\nRows: 6900\nColumns: 6\n$ city       &lt;str&gt; 'Anchorage, AK', 'Birmingham, AL', 'Huntsville, AL', 'Mobile, AL', 'Montgomery, AL', 'Little Rock, AR', 'Chandler, AZ', 'Gilbert, AZ', 'Glendale, AZ', 'Mesa, AZ'\n$ state      &lt;str&gt; 'AK', 'AL', 'AL', 'AL', 'AL', 'AR', 'AZ', 'AZ', 'AZ', 'AZ'\n$ year       &lt;i64&gt; 1790, 1790, 1790, 1790, 1790, 1790, 1790, 1790, 1790, 1790\n$ population &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n$ lon        &lt;f64&gt; -149.2743541, -86.799047, -86.5389964, -88.1002261, -86.2685927, -92.358556, -111.8549429, -111.7421907, -112.1899006, -111.7173787\n$ lat        &lt;f64&gt; 61.177549, 33.5274441, 34.7842707, 30.668426, 32.3462512, 34.7254318, 33.2828736, 33.3102088, 33.5331113, 33.4019259",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#us-metropolitan-regions",
    "href": "22_datasets.html#us-metropolitan-regions",
    "title": "22  Datasets",
    "section": "22.13 US Metropolitan Regions ",
    "text": "22.13 US Metropolitan Regions \nSourced from the Census Bureau’s American Community Survey, this collection of datasets provides a multi-dimensional view of U.S. demographics and economics, centered on Metropolitan Statistical Areas (CBSAs).\nThe metro dataset is the primary analytic table, profiling 934 metropolitan areas identified by a unique geoid. It aggregates key socioeconomic indicators, including population size (pop), population density, and the median age of residents. Economic health is captured through median household income and housing metrics like home ownership rates (percent_own) and the median cost of a one-bedroom rental. Geographically, each metro is assigned to a broad census region (quad) and division, and is precisely located via latitude/longitude coordinates. The accompanying metro_geo dataset provides the corresponding polygon geometries for these areas, enabling choropleth mapping and spatial analysis.\n\nmetro = pl.read_csv(\"data/acs_cbsa.csv\")\nmetro.glimpse()\n\nRows: 934\nColumns: 13\n$ name             &lt;str&gt; 'New York', 'Los Angeles', 'Chicago', 'Dallas', 'Houston', 'Washington', 'Philadelphia', 'Miami', 'Atlanta', 'Boston'\n$ geoid            &lt;i64&gt; 35620, 31080, 16980, 19100, 26420, 47900, 37980, 33100, 12060, 14460\n$ quad             &lt;str&gt; 'NE', 'W', 'NC', 'S', 'S', 'S', 'NE', 'S', 'S', 'NE'\n$ lon              &lt;f64&gt; -74.10105570561859, -118.1487215689162, -87.95881973164443, -96.97050780978928, -95.40157389770467, -77.51307477160977, -75.30263491667849, -80.50630736521515, -84.39956676469873, -71.0999121719376\n$ lat              &lt;f64&gt; 40.768770318020096, 34.219405716738684, 41.70060516046628, 32.84947968570761, 29.78708316635632, 38.812483836818316, 39.90521296481641, 26.15536900531196, 33.691787081105744, 42.555193833166065\n$ pop              &lt;f64&gt; 20.011812, 13.202558, 9.607711, 7.54334, 7.048954, 6.332069, 6.215222, 6.105897, 6.026734, 4.91203\n$ density          &lt;f64&gt; 1051.3064674555676, 1040.6472811000378, 508.62940568377377, 323.1814035995303, 316.5435135006062, 363.73268864646764, 506.06813036177755, 430.1031618191652, 263.27582111238434, 517.8277024367719\n$ age_median       &lt;f64&gt; 42.9, 41.6, 41.9, 41.3, 41.0, 42.4, 42.6, 43.9, 41.9, 42.2\n$ hh_income_median &lt;i64&gt; 86445, 81652, 78790, 76916, 72551, 111252, 79070, 62870, 75267, 99039\n$ percent_own      &lt;f64&gt; 55.3, 51.3, 68.9, 64.1, 65.1, 67.4, 71.1, 60.8, 67.3, 66.4\n$ rent_1br_median  &lt;i64&gt; 1430, 1468, 1060, 1106, 997, 1601, 1083, 1230, 1181, 1390\n$ rent_perc_income &lt;f64&gt; 31.0, 33.6, 29.0, 29.1, 30.0, 28.8, 30.0, 36.8, 30.3, 29.5\n$ division         &lt;str&gt; 'Middle Atlantic', 'Pacific', 'East North Central', 'West South Central', 'West South Central', 'South Atlantic', 'Middle Atlantic', 'South Atlantic', 'South Atlantic', 'New England'\n\n\n\nThe next several datasets handle the higher-level state geography. The state table serves as a reference for the 50 U.S. states, providing names, abbreviations, and total populations, while state_geo contains their boundaries.\n\nstate = pl.read_csv(\"data/acs_state.csv\")\nstate.glimpse()\n\nRows: 50\nColumns: 3\n$ state &lt;str&gt; 'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia'\n$ abb   &lt;str&gt; 'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA'\n$ pop   &lt;f64&gt; 4.997675, 0.735951, 7.079203, 3.006309, 39.455353, 5.723176, 3.60533, 0.981892, 21.339762, 10.625615\n\n\n\nThe metro_cw (crosswalk) table bridges the two geographic levels, handling the complexity of metropolitan areas that span multiple state lines (e.g., the New York metro area covering parts of NY, NJ, and PA). It uses the prop column to indicate what fraction of a metro area’s footprint or population falls within a specific state.\n\nmetro_cw = pl.read_csv(\"data/acs_cbsa_to_state.csv\")\nmetro_cw.glimpse()\n\nRows: 996\nColumns: 3\n$ geoid &lt;i64&gt; 35620, 35620, 35620, 31080, 16980, 16980, 16980, 19100, 26420, 47900\n$ state &lt;str&gt; 'NY', 'NJ', 'PA', 'CA', 'IL', 'IN', 'WI', 'TX', 'TX', 'VA'\n$ prop  &lt;f64&gt; 0.6483270833289084, 0.3486404584192905, 0.0030324582518010926, 1.0, 0.9065905299185738, 0.07571270790662321, 0.017696762174803024, 1.0, 1.0, 0.48420377548328675\n\n\n\nThe transit dataset provides a breakdown of transportation modes, listing the percentage of the population that commutes via car, public transportation, bicycle, or other means, as well as those who work from home.\n\ntransit = pl.read_csv(\"data/acs_cbsa_commute_type.csv\")\ntransit.glimpse()\n\nRows: 939\nColumns: 11\n$ name                  &lt;str&gt; 'New York', 'Los Angeles', 'Chicago', 'Dallas', 'Houston', 'Washington', 'Philadelphia', 'Miami', 'Atlanta', 'Boston'\n$ geoid                 &lt;i64&gt; 35620, 31080, 16980, 19100, 26420, 47900, 37980, 33100, 12060, 14460\n$ pop                   &lt;f64&gt; 20.011812, 13.202558, 9.607711, 7.54334, 7.048954, 6.332069, 6.215222, 6.105897, 6.026734, 4.91203\n$ car                   &lt;f64&gt; 53.51, 80.13, 74.23, 85.27, 86.49, 69.21, 75.33, 84.17, 81.38, 68.81\n$ public_transportation &lt;f64&gt; 27.77, 4.07, 10.03, 1.04, 1.85, 10.12, 7.87, 2.62, 2.39, 10.75\n$ taxicab               &lt;f64&gt; 0.78, 0.27, 0.38, 0.11, 0.14, 0.45, 0.25, 0.47, 0.45, 0.32\n$ motorcycle            &lt;f64&gt; 0.05, 0.21, 0.05, 0.1, 0.09, 0.11, 0.06, 0.16, 0.09, 0.05\n$ bicycle               &lt;f64&gt; 0.71, 0.61, 0.61, 0.14, 0.26, 0.74, 0.6, 0.49, 0.15, 0.94\n$ walked                &lt;f64&gt; 5.53, 2.28, 2.78, 1.22, 1.18, 2.88, 3.22, 1.46, 1.17, 5.05\n$ other_means           &lt;f64&gt; 1.06, 1.18, 1.01, 0.97, 1.3, 1.09, 0.96, 1.43, 1.14, 1.07\n$ worked_from_home      &lt;f64&gt; 10.59, 11.25, 10.91, 11.15, 8.69, 15.41, 11.71, 9.21, 13.22, 13.02\n\n\n\nComplementing this, the commute dataset uses a “long” format to capture the distribution of travel times. Instead of a single average, it breaks commute durations into specific time bins (defined by time_min and time_max), with the per column indicating the percentage of commuters falling into each interval.\n\ncommute = pl.read_csv(\"data/acs_cbsa_commute_time.csv\")\ncommute.glimpse()\n\nRows: 13146\nColumns: 6\n$ name     &lt;str&gt; 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York'\n$ geoid    &lt;i64&gt; 35620, 35620, 35620, 35620, 35620, 35620, 35620, 35620, 35620, 35620\n$ pop      &lt;f64&gt; 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812\n$ per      &lt;f64&gt; 3.43, 2.97, 3.32, 7.66, 7.72, 14.4, 10.74, 15.69, 7.97, 9.56\n$ time_min &lt;f64&gt; 0.0, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0\n$ time_max &lt;f64&gt; 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 10.0\n\n\n\nThe hh dataset offers a granular look at economic disparity by mapping the full distribution of household income for each metro area. Like the commute data, it is structured in a long format, where each row represents a specific income bracket (bounded by band_min and band_max). The per column quantifies the share of households within that bracket, allowing for a more nuanced analysis of wealth distribution—such as identifying the “middle class” squeeze or poverty rates—than a simple median value could provide.\n\nhh = pl.read_csv(\"data/acs_cbsa_hh_income.csv\")\nhh.glimpse()\n\nRows: 15024\nColumns: 6\n$ name     &lt;str&gt; 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York'\n$ geoid    &lt;i64&gt; 35620, 35620, 35620, 35620, 35620, 35620, 35620, 35620, 35620, 35620\n$ pop      &lt;f64&gt; 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812\n$ per      &lt;f64&gt; 5.66, 3.97, 3.27, 3.31, 3.2, 3.12, 2.92, 3.06, 2.66, 5.63\n$ band_min &lt;i64&gt; 0, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000\n$ band_max &lt;str&gt; '10000', '14999', '19999', '24999', '29999', '34999', '39999', '44999', '49999', '59999'\n\n\n\nA geographic file that contains the polygons for each of the US states. There are keys to join to the other structured datasets above.\n\nstate_geo = DSGeo.read_file(\"data/acs_state.geojson\")\nstate_geo.drop(c.geometry).glimpse()\n\nRows: 50\nColumns: 3\n$ name &lt;str&gt; 'Maine', 'New Hampshire', 'Delaware', 'South Carolina', 'Nebraska', 'Washington', 'New Mexico', 'South Dakota', 'Texas', 'California'\n$ abb  &lt;str&gt; 'ME', 'NH', 'DE', 'SC', 'NE', 'WA', 'NM', 'SD', 'TX', 'CA'\n$ fips &lt;str&gt; '23', '33', '10', '45', '31', '53', '35', '46', '48', '06'\n\n\n\nAnd another geographic file that contains the polygons for each of the metro regions. There are keys to join to the other structured datasets above.\n\nmetro_geo = DSGeo.read_file(\"data/acs_cbsa_geo.geojson\")\nmetro_geo.drop(c.geometry).glimpse()\n\nRows: 939\nColumns: 3\n$ geoid &lt;f64&gt; 35620.0, 31080.0, 16980.0, 19100.0, 26420.0, 47900.0, 37980.0, 33100.0, 12060.0, 14460.0\n$ quad  &lt;str&gt; 'NE', 'W', 'NC', 'S', 'S', 'S', 'NE', 'S', 'S', 'NE'\n$ pop   &lt;f64&gt; 20.011812, 13.202558, 9.607711, 7.54334, 7.048954, 6.332069, 6.215222, 6.105897, 6.026734, 4.91203",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#covid",
    "href": "22_datasets.html#covid",
    "title": "22  Datasets",
    "section": "22.14 COVID ",
    "text": "22.14 COVID \nThe covid dataset is a longitudinal record tracking the daily impact of the COVID-19 pandemic across France’s administrative departments. It uses a time-series structure where each row represents the status of a specific department (departement) on a given date. The metrics capture the strain on the healthcare system and the severity of the outbreak, recording cumulative statistics for deceased patients and recovered cases, as well as real-time snapshots of patients currently hospitalised or in intensive care (reanimation). Additionally, some columns track daily flows, such as new hospital admissions (hospitalised_new), allowing for analysis of infection waves and healthcare capacity over time.\n\ncovid = pl.read_csv(\"data/france_departement_covid.csv\")\ncovid.glimpse()\n\nRows: 19998\nColumns: 9\n$ date             &lt;str&gt; '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18'\n$ departement      &lt;str&gt; '01', '02', '03', '04', '05', '06', '07', '08', '09', '10'\n$ departement_name &lt;str&gt; 'Ain', 'Aisne', 'Allier', 'Alpes-de-Haute-Provence', 'Hautes-Alpes', 'Alpes-Maritimes', 'Ardèche', 'Ardennes', 'Ariège', 'Aube'\n$ deceased         &lt;i64&gt; 0, 9, 0, 0, 0, 2, 0, 0, 0, 0\n$ hospitalised     &lt;i64&gt; 2, 0, 0, 3, 8, 25, 0, 0, 1, 5\n$ reanimation      &lt;i64&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 0\n$ recovered        &lt;i64&gt; 1, 0, 0, 2, 9, 47, 0, 1, 2, 0\n$ hospitalised_new &lt;str&gt; 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA'\n$ reanimation_new  &lt;str&gt; 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA'\n\n\n\nThe pop dataset is a concise demographic reference table listing the total resident population for each of the 101 French departments. Indexed by the standard two-character departement code (e.g., “01” for Ain, “75” for Paris), this table serves as a critical normalization tool. It allows researchers to convert raw counts from the COVID-19 or economic datasets into standardized rates (such as cases per 100,000 inhabitants), enabling fair comparisons between densely populated urban areas and rural regions.\n\npop = pl.read_csv(\"data/france_departement_population.csv\")\npop.glimpse()\n\nRows: 101\nColumns: 2\n$ departement &lt;str&gt; '01', '02', '03', '04', '05', '06', '07', '08', '09', '10'\n$ population  &lt;i64&gt; 643350, 534490, 337988, 163915, 141284, 1083310, 325712, 273579, 153153, 310020\n\n\n\nThe fr_city dataset focuses on the country’s major urban centers, providing geospatial and demographic details for 58 significant cities. It identifies each city by name and links it to its broader administrative region (admin_name), such as “Ile-de-France” or “Nouvelle-Aquitaine.” The data includes precise lat and lon coordinates for mapping and a population figure that represents the broader urban or metropolitan area (agglomeration) rather than just the municipal limits. This dataset allows for spatial analysis of city-level hubs distinct from the broader departmental data.\n\nfr_city = pl.read_csv(\"data/france_cities.csv\")\nfr_city.glimpse()\n\nRows: 58\nColumns: 5\n$ city       &lt;str&gt; 'Paris', 'Lyon', 'Marseille', 'Lille', 'Nice', 'Toulouse', 'Bordeaux', 'Rouen', 'Strasbourg', 'Nantes'\n$ lat        &lt;f64&gt; 48.8667, 45.77, 43.29, 50.65, 43.715, 43.62, 44.85, 49.4304, 48.58, 47.2104\n$ lon        &lt;f64&gt; 2.3333, 4.83, 5.375, 3.08, 7.265, 1.4499, -0.595, 1.08, 7.75, -1.59\n$ population &lt;i64&gt; 9904000, 1423000, 1400000, 1044000, 927000, 847000, 803000, 532559, 439972, 438537\n$ admin_name &lt;str&gt; 'Ile-de-France', 'Auvergne-Rhone-Alpes', \"Provence-Alpes-Cote d'Azur\", 'Hauts-de-France', \"Provence-Alpes-Cote d'Azur\", 'Occitanie', 'Nouvelle-Aquitaine', 'Normandie', 'Grand Est', 'Pays de la Loire'\n\n\n\nThe gdp dataset provides an economic profile of the country at the departmental level. It maps each departement (identified by both code and name) to its Gross Domestic Product (GDP) per capita in Euros. This metric serves as a proxy for regional standard of living and economic productivity, allowing for correlations with health outcomes or infrastructure availability.\n\ngdp = pl.read_csv(\"data/france_departement_gdp.csv\")\ngdp.glimpse()\n\nRows: 96\nColumns: 3\n$ departement      &lt;str&gt; '01', '02', '03', '04', '05', '06', '07', '08', '09', '10'\n$ departement_name &lt;str&gt; 'Ain', 'Aisne', 'Allier', 'Alpes-de-Haute-Provence', 'Hautes-Alpes', 'Alpes-Maritimes', 'Ardèche', 'Ardennes', 'Ariège', 'Aube'\n$ gdp_eur          &lt;i64&gt; 28296, 25556, 27657, 28149, 28672, 38488, 24720, 26816, 23534, 31098\n\n\n\nThe dep dataset serves as the geospatial backbone for mapping French administrative divisions. It contains the boundaries for the 101 departments, linking the standard departement codes and names to a hidden geometry column (polygons). By joining this spatial file with the covid, pop, or gdp tables, users can visualize data through choropleth maps, revealing geographic patterns such as regional economic clusters or the spatial spread of the pandemic.\n\ndep = DSGeo.read_file(\"data/france_departement_sml.geojson\")\ndep.drop(c.geometry).glimpse()\n\nRows: 101\nColumns: 2\n$ departement      &lt;str&gt; '01', '02', '03', '04', '05', '06', '07', '08', '09', '10'\n$ departement_name &lt;str&gt; 'Ain', 'Aisne', 'Allier', 'Alpes-de-Haute-Provence', 'Hautes-Alpes', 'Alpes-Maritimes', 'Ardèche', 'Ardennes', 'Ariège', 'Aube'\n\n\n\nWe have a second covid dataset consisting of a granular time-series record tracking the spread of the virus across Italy’s administrative landscape. Unlike national or regional summaries, this table drills down to the provincial level (roughly equivalent to U.S. counties), providing a daily count of total cases for each province and its parent region. With over 68,000 observations, it allows for the analysis of local outbreaks and the specific trajectory of the pandemic within distinct geographic pockets from February 2020 onwards.\n\ncovid = pl.read_csv(\"data/it_province_covid.csv\")\ncovid.glimpse()\n\nRows: 68694\nColumns: 4\n$ date     &lt;str&gt; '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24'\n$ region   &lt;str&gt; 'Abruzzo', 'Abruzzo', 'Abruzzo', 'Abruzzo', 'Basilicata', 'Basilicata', 'Calabria', 'Calabria', 'Calabria', 'Calabria'\n$ province &lt;str&gt; \"L'Aquila\", 'Teramo', 'Pescara', 'Chieti', 'Potenza', 'Matera', 'Cosenza', 'Catanzaro', 'Reggio di Calabria', 'Crotone'\n$ cases    &lt;i64&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n\n\n\nThe it_city dataset serves as a geospatial reference for 388 distinct Italian urban centers. It identifies cities by name—ranging from major metropolises like Rome and Milan to smaller regional hubs—and provides their precise latitude (lat) and longitude (lon) coordinates. Additionally, the pop column lists the resident population for each city, enabling analysis that correlates population density or urban size with other socioeconomic or health indicators.\n\nit_city = pl.read_csv(\"data/it_cities.csv\")\nit_city.glimpse()\n\nRows: 388\nColumns: 4\n$ city_name &lt;str&gt; 'Rome', 'Milan', 'Naples', 'Turin', 'Palermo', 'Genoa', 'Bologna', 'Florence', 'Catania', 'Bari'\n$ lon       &lt;f64&gt; 12.51133, 9.18951, 14.26811, 7.68682, 13.33561, 8.94439, 11.33875, 11.24626, 15.07041, 16.8554\n$ lat       &lt;f64&gt; 41.89193, 45.46427, 40.85216, 45.07049, 38.13205, 44.40478, 44.49381, 43.77925, 37.49223, 41.11148\n$ pop       &lt;i64&gt; 2318895, 1236837, 959470, 870456, 648260, 580223, 366133, 349296, 290927, 277387\n\n\n\nThe prov dataset provides the essential spatial geometry required to map the provincial data. It contains the administrative boundaries for Italy’s 107 provinces, identifying each by its standard name (e.g., “Torino”, “Firenze”). By linking this geospatial file with the covid dataset via the province column, users can construct choropleth maps to visualize the spatial distribution and evolution of case counts across the peninsula.\n\nprov = DSGeo.read_file(\"data/it_province.geojson\")\nprov.drop(c.geometry).glimpse()\n\nRows: 107\nColumns: 1\n$ province &lt;str&gt; 'Torino', 'Vercelli', 'Novara', 'Cuneo', 'Asti', 'Alessandria', 'Biella', 'Verbano-Cusio-Ossola', \"Valle d'Aosta/Vallée d'Aoste\", 'Varese'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#u.s.-storms",
    "href": "22_datasets.html#u.s.-storms",
    "title": "22  Datasets",
    "section": "22.15 U.S. Storms ",
    "text": "22.15 U.S. Storms \nSourced from NOAA’s National Hurricane Center, these three datasets provide a historical record of North Atlantic tropical cyclones. The storm dataset records the comprehensive trajectory and intensity history of tropical cyclones since 1950. It contains over 25,000 timestamped observations, logging the status of a storm every six hours. The data captures the storm’s precise geographic position (lat, lon), its maximum sustained wind speed (wind) in knots, and its Saffir-Simpson category (ranging from 0 for tropical storms/depressions to 5 for catastrophic hurricanes).\n\nstorm = pl.read_csv(\"data/storms.csv\")\nstorm.glimpse()\n\nRows: 25112\nColumns: 12\n$ year     &lt;i64&gt; 1950, 1950, 1950, 1950, 1950, 1950, 1950, 1950, 1950, 1950\n$ month    &lt;i64&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8\n$ day      &lt;i64&gt; 12, 12, 12, 12, 13, 13, 13, 13, 14, 14\n$ hour     &lt;i64&gt; 0, 6, 12, 18, 0, 6, 12, 18, 0, 6\n$ name     &lt;str&gt; 'Able', 'Able', 'Able', 'Able', 'Able', 'Able', 'Able', 'Able', 'Able', 'Able'\n$ letter   &lt;str&gt; 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A'\n$ doy      &lt;i64&gt; 224, 224, 224, 224, 225, 225, 225, 225, 226, 226\n$ lon      &lt;f64&gt; -55.5, -56.3, -57.4, -58.6, -60.0, -61.1, -62.2, -63.2, -63.8, -64.6\n$ lat      &lt;f64&gt; 17.1, 17.7, 18.2, 19.0, 20.0, 20.7, 21.3, 22.0, 22.7, 23.1\n$ status   &lt;str&gt; 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS'\n$ category &lt;i64&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ wind     &lt;i64&gt; 35, 40, 45, 50, 50, 50, 55, 55, 60, 60\n\n\n\nThe gender dataset classifies 262 distinct storm names by gender. It provides the name, the assigned gender (male or female), and a prob score reflecting the confidence of that assignment based on U.S. naming conventions. This table supports behavioral research, such as investigations into whether the gender of a storm’s name psychologically impacts public risk perception and preparedness levels.\n\ngender = pl.read_csv(\"data/storm_gender.csv\")\ngender.glimpse()\n\nRows: 262\nColumns: 3\n$ name   &lt;str&gt; 'Abby', 'Able', 'Agnes', 'Alberto', 'Alex', 'Alice', 'Alicia', 'Allen', 'Allison', 'Alma'\n$ gender &lt;str&gt; 'female', 'male', 'female', 'male', 'male', 'female', 'female', 'male', 'female', 'female'\n$ prob   &lt;f64&gt; 1.0, 1.0, 1.0, 1.0, 0.9634, 1.0, 1.0, 0.9936, 0.9987, 0.9867\n\n\n\nThe storm_type dataset acts as a lookup table for the meteorological classifications found in the main tracking data. It maps the two-letter status codes (like ‘HU’ or ‘TD’) to their full descriptions (e.g., ‘hurricane’ or ‘tropical depression’), clarifying the specific developmental stage or physical nature of the cyclone at each observation point.\n\nstorm_type = pl.read_csv(\"data/storm_codes.csv\")\nstorm_type.glimpse()\n\nRows: 9\nColumns: 2\n$ status      &lt;str&gt; 'TD', 'TS', 'HU', 'EX', 'SD', 'SS', 'LO', 'WV', 'DB'\n$ status_name &lt;str&gt; 'tropical depression', 'tropical storm', 'hurricane', 'extratropical cyclone', 'subtropical depression', 'subtropical storm', 'low', 'tropical wave', 'disturbance'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#shakespeare-plays",
    "href": "22_datasets.html#shakespeare-plays",
    "title": "22  Datasets",
    "section": "22.16 Shakespeare Plays ",
    "text": "22.16 Shakespeare Plays \nA catalog of Shakespeare’s 37 plays, capturing each work’s identity and publication metadata. This serves as the metadata reference table, with each play assigned a short identifier (like “ham” for Hamlet) that links to all other tables. The data comes from the Folger Shakespeare Library’s scholarly editions.\n\nplay = pl.read_csv(\"data/shakespeare_plays.csv\")\nplay.glimpse()\n\nRows: 37\nColumns: 4\n$ play_id     &lt;str&gt; 'mnd', 'aww', 'ant', 'ayl', 'cor', 'cym', 'ham', '1h4', '2h4', 'h5'\n$ title       &lt;str&gt; 'A Midsummer Night’s Dream', 'All’s Well That Ends Well', 'Antony and Cleopatra', 'As You Like It', 'Coriolanus', 'Cymbeline', 'Hamlet', 'Henry IV, Part I', 'Henry IV, Part II', 'Henry V'\n$ author      &lt;str&gt; 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare'\n$ short_title &lt;str&gt; 'MND', 'AWW', 'Ant', 'AYL', 'Cor', 'Cym', 'Ham', '1H4', '2H4', 'H5'\n\n\n\nThe dramatis personae across all plays—1,126 characters from Hamlet to Puck to Lady Macbeth. Each character is tied to their play and includes role descriptions where available (e.g., “duke of Athens” for Theseus, “father to Hermia” for Egeus). This table enables analysis of character networks, naming patterns, and the social structures Shakespeare constructed.\n\nchar = pl.read_csv(\"data/shakespeare_characters.csv\")\nchar.glimpse()\n\nRows: 1126\nColumns: 4\n$ character_id     &lt;str&gt; 'Hermia_MND', 'Lysander_MND', 'Helena_MND', 'Demetrius_MND', 'Theseus_MND', 'Hippolyta_MND', 'Egeus_MND', 'Philostrate_MND', 'Bottom_MND', 'Quince_MND'\n$ play_id          &lt;str&gt; 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd'\n$ name             &lt;str&gt; 'Hermia', 'Lysander', 'Helena', 'Demetrius', 'Theseus', 'Hippolyta', 'Egeus', 'Philostrate', 'Nick Bottom', 'Peter Quince'\n$ role_description &lt;str&gt; null, null, null, null, 'duke of Athens', 'queen of the Amazons', 'father to Hermia', 'master of the revels to Theseus', 'weaver', 'carpenter'\n\n\n\nThe spoken text of the plays, segmented into 80,592 individual lines of verse or prose. Each line is attributed to a specific character and located within the play’s structure (act, scene, line number). This is the core table for literary analysis—studying who speaks, how much, in what style (verse vs. prose), and how dialogue flows through each scene.\n\nline = pl.read_csv(\"data/shakespeare_lines.csv.gz\")\nline.glimpse()\n\nRows: 80592\nColumns: 8\n$ line_id      &lt;str&gt; 'mnd_line_000001', 'mnd_line_000002', 'mnd_line_000003', 'mnd_line_000004', 'mnd_line_000005', 'mnd_line_000006', 'mnd_line_000007', 'mnd_line_000008', 'mnd_line_000009', 'mnd_line_000010'\n$ play_id      &lt;str&gt; 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd'\n$ character_id &lt;str&gt; 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Hippolyta_MND', 'Hippolyta_MND', 'Hippolyta_MND', 'Hippolyta_MND'\n$ act          &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ scene        &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ line_number  &lt;str&gt; '1.1.1', '1.1.2', '1.1.3', '1.1.4', '1.1.5', '1.1.6', '1.1.7', '1.1.8', '1.1.9', '1.1.10'\n$ line_type    &lt;str&gt; 'verse', 'verse', 'verse', 'verse', 'verse', 'verse', 'verse', 'verse', 'verse', 'verse'\n$ text         &lt;str&gt; 'Now, fair Hippolyta, our nuptial hour', 'Draws on apace. Four happy days bring in', 'Another moon. But, O, methinks how slow', 'This old moon wanes! She lingers my desires', 'Like to a stepdame or a dowager', 'Long withering out a young man’s revenue.', 'Four days will quickly steep themselves in night;', 'Four nights will quickly dream away the time;', 'And then the moon, like to a silver bow', 'New-bent in heaven, shall behold the night'\n\n\n\nA granular, linguistically-annotated word table with nearly 600,000 tokens. Each word is linked back to its line, character, and play, and includes its lemma (base form) and part-of-speech tag. This enables computational stylistics: vocabulary richness, word frequency analysis, grammatical patterns, and comparative studies of how different characters or plays use language.\n\nword = pl.read_csv(\"data/shakespeare_words.csv.gz\")\nword.glimpse()\n\nRows: 599864\nColumns: 8\n$ word_id      &lt;str&gt; 'mnd_word_00000001', 'mnd_word_00000002', 'mnd_word_00000003', 'mnd_word_00000004', 'mnd_word_00000005', 'mnd_word_00000006', 'mnd_word_00000007', 'mnd_word_00000008', 'mnd_word_00000009', 'mnd_word_00000010'\n$ play_id      &lt;str&gt; 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd'\n$ character_id &lt;str&gt; 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND'\n$ line_id      &lt;str&gt; 'mnd_line_000001', 'mnd_line_000001', 'mnd_line_000001', 'mnd_line_000001', 'mnd_line_000001', 'mnd_line_000001', 'mnd_line_000002', 'mnd_line_000002', 'mnd_line_000002', 'mnd_line_000002'\n$ word         &lt;str&gt; 'Now', 'fair', 'Hippolyta', 'our', 'nuptial', 'hour', 'Draws', 'on', 'apace', 'Four'\n$ lemma        &lt;str&gt; 'now', 'fair', 'Hippolyta', 'our', 'nuptial', 'hour', 'draw', 'on', 'apace', 'four'\n$ pos          &lt;str&gt; 'av', 'j', 'n1-nn', 'po', 'j', 'n1', 'vvz', 'acp-p', 'av', 'crd'\n$ location     &lt;str&gt; '1.1.1', '1.1.1', '1.1.1', '1.1.1', '1.1.1', '1.1.1', '1.1.2', '1.1.2', '1.1.2', '1.1.2'\n\n\n\nThere are also versions with other early-modern British playwrights: emed_plays.csv, emed_characters.csv, emed_lines.csv and emed_words.csv.",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#wikipedia-authors",
    "href": "22_datasets.html#wikipedia-authors",
    "title": "22  Datasets",
    "section": "22.17 Wikipedia Authors ",
    "text": "22.17 Wikipedia Authors \nSourced from Wikipedia, these six datasets provide a multi-faceted view of 75 prominent British authors, ranging from medieval poets like Chaucer to modern literary figures.\nThe wiki dataset serves as the biographical master table for the collection. It anchors each author (doc_id) with key vital statistics, including their years of birth and death, assigned literary era (e.g., “Early”, “Sixteenth C”), and gender. To facilitate linking with the other tables, it includes the URL slug for their Wikipedia page (link) and a shortened name (short) for cleaner visualization.\n\nwiki = pl.read_csv(\"data/wiki_uk_meta.csv.gz\", ignore_errors=True)\nwiki.glimpse()\n\nRows: 75\nColumns: 7\n$ doc_id &lt;str&gt; 'Marie de France', 'Geoffrey Chaucer', 'John Gower', 'William Langland', 'Margery Kempe', 'Thomas Malory', 'Thomas More', 'Edmund Spenser', 'Walter Raleigh', 'Philip Sidney'\n$ born   &lt;i64&gt; 1160, 1343, 1330, 1332, 1373, 1405, 1478, 1552, 1552, 1554\n$ died   &lt;i64&gt; 1215, 1400, 1408, 1386, 1438, 1471, 1535, 1599, 1618, 1586\n$ era    &lt;str&gt; 'Early', 'Early', 'Early', 'Early', 'Early', 'Early', 'Sixteenth C', 'Sixteenth C', 'Sixteenth C', 'Sixteenth C'\n$ gender &lt;str&gt; 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male'\n$ link   &lt;str&gt; 'Marie_de_France', 'Geoffrey_Chaucer', 'John_Gower', 'William_Langland', 'Margery_Kempe', 'Thomas_Malory', 'Thomas_More', 'Edmund_Spenser', 'Walter_Raleigh', 'Philip_Sidney'\n$ short  &lt;str&gt; 'Marie d. F.', 'Chaucer', 'Gower', 'Langland', 'Kempe', 'Malory', 'More', 'Spenser', 'Raleigh', 'Sidney'\n\n\n\nThe ptext dataset contains the full text of each of the Wikipedia pages for each of the authors.\n\nptext = pl.read_csv(\"data/wiki_uk_authors_text.csv\", ignore_errors=True)\nptext.drop(c.text).glimpse()\n\nRows: 75\nColumns: 1\n$ doc_id &lt;str&gt; 'Marie de France', 'Geoffrey Chaucer', 'John Gower', 'William Langland', 'Margery Kempe', 'Thomas Malory', 'Thomas More', 'Edmund Spenser', 'Walter Raleigh', 'Philip Sidney'\n\n\n\nThe anno dataset is a large, token-level linguistic corpus derived from the text of the authors’ Wikipedia biographies. With over 400,000 rows, it breaks down every sentence into individual words (token), providing deep grammatical analysis including the lemma, part-of-speech tags (upos, xpos), and morphological features (feats). It also captures dependency parsing relationships (relation, tid_source), enabling syntactic analysis of how these figures are described in encyclopedic text.\n\nanno = pl.read_csv(\"data/wiki_uk_authors_anno.csv.gz\", ignore_errors=True)\nanno.glimpse()\n\nRows: 407698\nColumns: 11\n$ doc_id        &lt;str&gt; 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France'\n$ sid           &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ tid           &lt;i64&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n$ token         &lt;str&gt; 'Marie', 'de', 'France', 'was', 'a', 'poet', 'possibly', 'born', 'in', 'what'\n$ token_with_ws &lt;str&gt; 'Marie ', 'de ', 'France ', 'was ', 'a ', 'poet ', 'possibly ', 'born ', 'in ', 'what '\n$ lemma         &lt;str&gt; 'Marie', 'de', 'France', 'be', 'a', 'poet', 'possibly', 'bear', 'in', 'what'\n$ upos          &lt;str&gt; 'PROPN', 'PROPN', 'PROPN', 'AUX', 'DET', 'NOUN', 'ADV', 'VERB', 'SCONJ', 'PRON'\n$ xpos          &lt;str&gt; 'NNP', 'NNP', 'NNP', 'VBD', 'DT', 'NN', 'RB', 'VBN', 'IN', 'WP'\n$ feats         &lt;str&gt; 'Number=Sing', 'Number=Sing', 'Number=Sing', 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin', 'Definite=Ind|PronType=Art', 'Number=Sing', 'NA', 'Tense=Past|VerbForm=Part|Voice=Pass', 'NA', 'PronType=Int'\n$ tid_source    &lt;i64&gt; 6, 1, 1, 6, 6, 0, 8, 6, 13, 13\n$ relation      &lt;str&gt; 'nsubj', 'flat', 'flat', 'cop', 'det', 'root', 'advmod', 'acl', 'mark', 'nsubj'\n\n\n\nThe cite dataset maps the direct “knowledge graph” between these authors within Wikipedia. It functions as a directed adjacency list, where each row represents a hyperlink connecting one author’s page (doc_id) to another’s (doc_id2). This structure allows researchers to construct a citation network, revealing which authors are explicitly referenced in the biographies of their peers—for example, showing how Geoffrey Chaucer is linked to later writers like Charles Dickens or William Shakespeare.\n\ncite = pl.read_csv(\"data/wiki_uk_citations.csv\", ignore_errors=True)\ncite.glimpse()\n\nRows: 377\nColumns: 2\n$ doc_id  &lt;str&gt; 'Marie de France', 'Geoffrey Chaucer', 'Geoffrey Chaucer', 'Geoffrey Chaucer', 'Geoffrey Chaucer', 'Geoffrey Chaucer', 'Geoffrey Chaucer', 'John Gower', 'John Gower', 'John Gower'\n$ doc_id2 &lt;str&gt; 'Geoffrey Chaucer', 'William Langland', 'John Gower', 'John Dryden', 'Philip Sidney', 'Charles Dickens', 'William Shakespeare', 'William Langland', 'Geoffrey Chaucer', 'C. S. Lewis'\n\n\n\nThe cocite dataset captures latent connections between authors by tracking how often they appear together in other Wikipedia articles. Rather than direct links, it counts the co-occurrences (count) between a primary author (doc_id) and another figure (doc_id_out).\n\ncocite = pl.read_csv(\"data/wiki_uk_cocitations.csv\", ignore_errors=True)\ncocite.glimpse()\n\nRows: 2114\nColumns: 3\n$ doc_id     &lt;str&gt; 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France'\n$ doc_id_out &lt;str&gt; 'Matthew Arnold', 'Oscar Wilde', 'Samuel Pepys', 'T. S. Eliot', 'Thomas Malory', 'W. H. Auden', 'Walter Raleigh', 'William Blake', 'William Langland', 'William Shakespeare'\n$ count      &lt;i64&gt; 1, 2, 1, 1, 1, 1, 1, 1, 1, 2\n\n\n\nThe rev dataset provides a longitudinal history of the Wikipedia articles themselves, logging over 35,000 individual edits. Each row records a specific revision, detailing the user who made the change, the timestamp (datetime), and the resulting file size. It also captures the editor’s comment, offering a window into the community collaboration and content disputes that shape the public narrative around these literary figures.\n\nrev = pl.read_csv(\"data/wiki_uk_page_revisions.csv\", ignore_errors=True)\nrev.glimpse()\n\nRows: 35470\nColumns: 5\n$ doc_id   &lt;str&gt; 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France'\n$ user     &lt;str&gt; 'YurikBot', 'YurikBot', 'Ccarroll', '63.231.20.88', 'ExplicitImplicity', 'ExplicitImplicity', 'Adam keller', 'Adam keller', '209.174.134.1', '74.71.36.72'\n$ datetime &lt;str&gt; '2006-07-06T23:24:54Z', '2006-07-24T18:39:08Z', '2006-09-10T13:36:44Z', '2006-09-18T04:59:53Z', '2006-09-23T22:27:20Z', '2006-09-23T22:31:03Z', '2006-10-03T18:18:09Z', '2006-10-03T18:19:32Z', '2006-10-10T18:48:26Z', '2006-10-11T05:29:32Z'\n$ size     &lt;i64&gt; 3061, 3085, 3085, 3105, 3104, 3132, 3140, 3132, 3160, 3132\n$ comment  &lt;str&gt; 'robot  Adding: [[it:Maria di Francia]]', 'robot  Adding: [[pt:Maria de França]]', null, null, 'i believe it is stupid to translate one word, but not the other. revert if you think otherwise.', null, null, null, null, null\n\n\n\nThe views dataset tracks the public interest in these authors through daily page view statistics. It records the number of visits (views) to each author’s Wikipedia page for every day in August 2023. This time-series data allows for the analysis of current popularity trends, revealing how historical figures maintain relevance or experience spikes in attention due to news, anniversaries, or cultural events.\n\nviews = pl.read_csv(\"data/wiki_uk_page_views.csv\", ignore_errors=True)\nviews.glimpse()\n\nRows: 4490\nColumns: 5\n$ doc_id &lt;str&gt; 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France'\n$ year   &lt;i64&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023\n$ month  &lt;i64&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8\n$ day    &lt;i64&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n$ views  &lt;i64&gt; 121, 138, 138, 129, 104, 139, 107, 109, 127, 117\n\n\n\nWe also have a dataset ptext_fr, which is a version of the French Wikipedia pages for the same authors. Here are the texts.\n\nptext_fr = pl.read_csv(\"data/wiki_uk_authors_text_fr.csv\", ignore_errors=True)\nptext_fr.drop(c.text).glimpse()\n\nRows: 73\nColumns: 1\n$ doc_id &lt;str&gt; 'Marie de France', 'Geoffrey Chaucer', 'John Gower', 'William Langland', 'Margery Kempe', 'Thomas Malory', 'Thomas More', 'Edmund Spenser', 'Walter Raleigh', 'Philip Sidney'\n\n\n\nAnd likewise, anno_fr captures the French-language specific annotations.\n\nanno_fr = pl.read_csv(\"data/wiki_uk_authors_anno_fr.csv.gz\", ignore_errors=True)\nanno_fr.glimpse()\n\nRows: 215735\nColumns: 11\n$ doc_id        &lt;str&gt; 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France'\n$ sid           &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ tid           &lt;str&gt; '1', '2', '3', '4', '5', '6', '7', '8', '9', '10-11'\n$ token         &lt;str&gt; 'Marie', 'de', 'France', 'est', 'une', 'poétesse', 'de', 'la', 'Renaissance', 'du'\n$ token_with_ws &lt;str&gt; 'Marie ', 'de ', 'France ', 'est ', 'une ', 'poétesse ', 'de ', 'la ', 'Renaissance ', 'du '\n$ lemma         &lt;str&gt; 'Marie', 'de', 'France', 'être', 'un', 'poétesse', 'de', 'le', 'Renaissance', 'NA'\n$ upos          &lt;str&gt; 'NOUN', 'ADP', 'PROPN', 'AUX', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'NA'\n$ xpos          &lt;str&gt; 'S', 'E', 'SP', 'V', 'RI', 'S', 'E', 'RD', 'S', 'NA'\n$ feats         &lt;str&gt; 'Gender=Fem|Number=Sing', 'NA', 'NA', 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin', 'Definite=Ind|Gender=Fem|Number=Sing|PronType=Art', 'Gender=Fem|Number=Sing', 'NA', 'Definite=Def|Gender=Fem|Number=Sing|PronType=Art', 'Gender=Fem|Number=Sing', 'NA'\n$ tid_source    &lt;str&gt; '6', '3', '1', '6', '6', '0', '9', '9', '6', 'NA'\n$ relation      &lt;str&gt; 'nsubj', 'case', 'nmod', 'cop', 'det', 'root', 'case', 'det', 'nmod', 'NA'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#imdb-reviews",
    "href": "22_datasets.html#imdb-reviews",
    "title": "22  Datasets",
    "section": "22.18 IMDb Reviews ",
    "text": "22.18 IMDb Reviews \nThe imdb5k dataset is a curated, 5,000-observation subset of the classic IMDb movie review corpus, designed for rapid prototyping and lightweight sentiment analysis experiments. Each row represents a single review, categorized by a binary sentiment label (“positive” or “negative”) and assigned to a specific data partition (index, e.g., “train” or “test”). Although hidden from the preview for brevity, the dataset is fully equipped for modern NLP tasks: it includes the raw text of the critique and e5, a pre-computed 1024-dimensional embedding vector.\n\nimdb5k = pl.read_parquet(\"data/imdb5k_pca.parquet\")\nimdb5k.drop(c.e5, c.text).glimpse()\n\nRows: 5000\nColumns: 3\n$ id    &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'\n$ label &lt;str&gt; 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive'\n$ index &lt;str&gt; 'test', 'test', 'test', 'train', 'test', 'train', 'train', 'test', 'train', 'train'\n\n\n\nThe imdb dataset represents the complete, 50,000-row standard benchmark for binary sentiment classification. Balanced between positive and negative polarities, it provides the robust volume of data necessary for training deep learning models. Like the smaller version, it contains metadata for experimental reproducibility (id and index). Crucially, it pairs the unstructured content of the reviews (text) with high-fidelity, 1024-dimensional vector representations (e5), enabling advanced machine learning applications—such as semantic search or clustering—straight out of the box.\n\nimdb = pl.read_parquet(\"data/imdb_pca.parquet\")\nimdb.drop(c.e5, c.text).glimpse()\n\nRows: 50000\nColumns: 3\n$ id    &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'\n$ label &lt;str&gt; 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative'\n$ index &lt;str&gt; 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#ag-news",
    "href": "22_datasets.html#ag-news",
    "title": "22  Datasets",
    "section": "22.19 AG News ",
    "text": "22.19 AG News \nThe agnews dataset is a widely used benchmark for multiclass topic classification, comprising 127,600 news articles harvested from more than 2,000 sources. Unlike the binary sentiment datasets, this corpus challenges models to distinguish between four distinct thematic categories: World, Sports, Business, and Sci/Tech. Each entry is assigned a specific label and partitioned into training or testing sets via the index column. While the preview displays only the metadata, the dataset is fully enriched with the raw article text and pre-computed, 1024-dimensional e5 embeddings, streamlining the workflow for developing and evaluating advanced NLP models.\n\nagnews = pl.read_parquet(\"data/agnews_pca.parquet\")\nagnews.drop(c.e5, c.text).glimpse()\n\nRows: 127600\nColumns: 3\n$ id    &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'\n$ label &lt;str&gt; 'Business', 'Business', 'Business', 'Business', 'Business', 'Business', 'Business', 'Business', 'Business', 'Business'\n$ index &lt;str&gt; 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#amazon-reviews",
    "href": "22_datasets.html#amazon-reviews",
    "title": "22  Datasets",
    "section": "22.20 Amazon Reviews ",
    "text": "22.20 Amazon Reviews \nThe amazon dataset is a specialized subset of the massive Amazon Product Reviews corpus, containing 10,000 observations selected for the task of product category classification. Unlike standard sentiment analysis datasets, the challenge here is to predict the correct product department (e.g., “All Beauty”) based solely on the content of the review. The dataset includes standard metadata columns—a unique id, the target label, and an index assigning rows to training or testing partitions. Like the previous NLP collections, it comes fully prepared with both the raw text and pre-computed, 1024-dimensional e5 embeddings, enabling immediate experimentation with dense vector-based classification models.\n\namazon = pl.read_parquet(\"data/amazon_pca.parquet\")\namazon.drop(c.e5, c.text).glimpse()\n\nRows: 10000\nColumns: 3\n$ id    &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'\n$ label &lt;str&gt; 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty'\n$ index &lt;str&gt; 'train', 'test', 'train', 'train', 'test', 'train', 'train', 'train', 'train', 'train'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#bbc-headlines",
    "href": "22_datasets.html#bbc-headlines",
    "title": "22  Datasets",
    "section": "22.21 BBC Headlines ",
    "text": "22.21 BBC Headlines \nThe bbc dataset is a concise benchmark for multiclass document classification, derived from a collection of 2,225 articles published by BBC News. It organizes content into five distinct thematic categories: business, entertainment, politics, sport, and tech. Each row represents a single article, tagged with its ground-truth label and assigned to a specific partition (index) for training or evaluation. Designed for modern NLP workflows, the dataset comes enriched with both the raw text and pre-computed, 1024-dimensional e5 embeddings, allowing researchers to immediately apply vector-based machine learning techniques like clustering or topic modeling.\n\nbbc = pl.read_parquet(\"data/bbc_pca.parquet\")\nbbc.drop(c.e5, c.text).glimpse()\n\nRows: 2225\nColumns: 3\n$ id    &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'\n$ label &lt;str&gt; 'politics', 'entertainment', 'sport', 'entertainment', 'business', 'tech', 'sport', 'sport', 'tech', 'entertainment'\n$ index &lt;str&gt; 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#sentiment-treebank",
    "href": "22_datasets.html#sentiment-treebank",
    "title": "22  Datasets",
    "section": "22.22 Sentiment Treebank ",
    "text": "22.22 Sentiment Treebank \nThe sst dataset contains the Stanford Sentiment Treebank (SST-5), a premier benchmark for fine-grained sentiment analysis. Unlike binary classification tasks, this corpus of 11,855 movie review excerpts challenges models to discern subtle emotional gradations across a five-point scale: “very negative”, “negative”, “neutral”, “positive”, and “very positive”. The dataset includes standard metadata columns for row identification (id) and data partitioning (index). Consistent with the other NLP collections in this series, it is provided with both the raw text and pre-computed, 1024-dimensional e5 embeddings, enabling nuanced research into how vector models capture intensity and neutrality in language.\n\nsst = pl.read_parquet(\"data/sst5_pca.parquet\")\nsst.drop(c.e5, c.text).glimpse()\n\nRows: 11855\nColumns: 3\n$ id    &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'\n$ label &lt;str&gt; 'very positive', 'negative', 'negative', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'negative', 'very positive'\n$ index &lt;str&gt; 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#goemotions",
    "href": "22_datasets.html#goemotions",
    "title": "22  Datasets",
    "section": "22.23 GoEmotions ",
    "text": "22.23 GoEmotions \nThe goemo dataset represents GoEmotions, a large-scale, fine-grained corpus designed to capture the complexity of human emotional expression. Sourced from Reddit comments, this collection of over 54,000 observations moves far beyond simple positive/negative sentiment, categorizing text into a rich taxonomy of 27 distinct emotions—such as “admiration,” “remorse,” “curiosity,” and “confusion”—plus a “neutral” state. The data is structured for multi-label classification, with separate columns indicating the presence or absence of each specific emotion. As with the other NLP datasets in this series, it comes enriched with both the raw text and pre-computed, 1024-dimensional e5 embeddings, facilitating advanced research into the vector-space relationships between subtle emotional concepts.\n\ngoemo = pl.read_parquet(\"data/goemotions_pca.parquet\")\ngoemo.drop(c.e5, c.text).glimpse()\n\nRows: 54263\nColumns: 30\n$ id             &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'\n$ index          &lt;str&gt; 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'\n$ admiration     &lt;str&gt; 'no admiration', 'no admiration', 'no admiration', 'no admiration', 'no admiration', 'no admiration', 'no admiration', 'no admiration', 'admiration', 'no admiration'\n$ amusement      &lt;str&gt; 'no amusement', 'no amusement', 'no amusement', 'no amusement', 'no amusement', 'no amusement', 'no amusement', 'no amusement', 'no amusement', 'no amusement'\n$ anger          &lt;str&gt; 'no anger', 'no anger', 'anger', 'no anger', 'no anger', 'no anger', 'no anger', 'no anger', 'no anger', 'no anger'\n$ annoyance      &lt;str&gt; 'no annoyance', 'no annoyance', 'no annoyance', 'no annoyance', 'annoyance', 'no annoyance', 'no annoyance', 'no annoyance', 'no annoyance', 'no annoyance'\n$ approval       &lt;str&gt; 'no approval', 'no approval', 'no approval', 'no approval', 'no approval', 'no approval', 'no approval', 'no approval', 'no approval', 'no approval'\n$ caring         &lt;str&gt; 'no caring', 'no caring', 'no caring', 'no caring', 'no caring', 'no caring', 'no caring', 'no caring', 'no caring', 'no caring'\n$ confusion      &lt;str&gt; 'no confusion', 'no confusion', 'no confusion', 'no confusion', 'no confusion', 'no confusion', 'no confusion', 'no confusion', 'no confusion', 'no confusion'\n$ curiosity      &lt;str&gt; 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity'\n$ desire         &lt;str&gt; 'no desire', 'no desire', 'no desire', 'no desire', 'no desire', 'no desire', 'no desire', 'desire', 'no desire', 'no desire'\n$ disappointment &lt;str&gt; 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment'\n$ disapproval    &lt;str&gt; 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval'\n$ disgust        &lt;str&gt; 'no disgust', 'no disgust', 'no disgust', 'no disgust', 'no disgust', 'no disgust', 'no disgust', 'no disgust', 'no disgust', 'no disgust'\n$ embarrassment  &lt;str&gt; 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment'\n$ excitement     &lt;str&gt; 'no excitement', 'no excitement', 'no excitement', 'no excitement', 'no excitement', 'no excitement', 'no excitement', 'no excitement', 'no excitement', 'no excitement'\n$ fear           &lt;str&gt; 'no fear', 'no fear', 'no fear', 'fear', 'no fear', 'no fear', 'no fear', 'no fear', 'no fear', 'no fear'\n$ gratitude      &lt;str&gt; 'no gratitude', 'no gratitude', 'no gratitude', 'no gratitude', 'no gratitude', 'no gratitude', 'gratitude', 'no gratitude', 'no gratitude', 'no gratitude'\n$ grief          &lt;str&gt; 'no grief', 'no grief', 'no grief', 'no grief', 'no grief', 'no grief', 'no grief', 'no grief', 'no grief', 'no grief'\n$ joy            &lt;str&gt; 'no joy', 'no joy', 'no joy', 'no joy', 'no joy', 'no joy', 'no joy', 'no joy', 'no joy', 'no joy'\n$ love           &lt;str&gt; 'no love', 'no love', 'no love', 'no love', 'no love', 'no love', 'no love', 'no love', 'no love', 'no love'\n$ nervousness    &lt;str&gt; 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness'\n$ optimism       &lt;str&gt; 'no optimism', 'no optimism', 'no optimism', 'no optimism', 'no optimism', 'no optimism', 'no optimism', 'optimism', 'no optimism', 'no optimism'\n$ pride          &lt;str&gt; 'no pride', 'no pride', 'no pride', 'no pride', 'no pride', 'no pride', 'no pride', 'no pride', 'no pride', 'no pride'\n$ realization    &lt;str&gt; 'no realization', 'no realization', 'no realization', 'no realization', 'no realization', 'no realization', 'no realization', 'no realization', 'no realization', 'no realization'\n$ relief         &lt;str&gt; 'no relief', 'no relief', 'no relief', 'no relief', 'no relief', 'no relief', 'no relief', 'no relief', 'no relief', 'no relief'\n$ remorse        &lt;str&gt; 'no remorse', 'no remorse', 'no remorse', 'no remorse', 'no remorse', 'no remorse', 'no remorse', 'no remorse', 'no remorse', 'no remorse'\n$ sadness        &lt;str&gt; 'no sadness', 'no sadness', 'no sadness', 'no sadness', 'no sadness', 'no sadness', 'no sadness', 'no sadness', 'no sadness', 'no sadness'\n$ surprise       &lt;str&gt; 'no surprise', 'no surprise', 'no surprise', 'no surprise', 'no surprise', 'surprise', 'no surprise', 'no surprise', 'no surprise', 'no surprise'\n$ neutral        &lt;str&gt; 'neutral', 'neutral', 'no neutral', 'no neutral', 'no neutral', 'no neutral', 'no neutral', 'no neutral', 'no neutral', 'neutral'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#fsa-owi-color-images",
    "href": "22_datasets.html#fsa-owi-color-images",
    "title": "22  Datasets",
    "section": "22.24 FSA-OWI Color Images ",
    "text": "22.24 FSA-OWI Color Images \nSourced from the Library of Congress, the fsac dataset contains a sample of 500 color photographs from the Farm Security Administration - Office of War Information (FSA-OWI) collection. This historic government project was originally established to document rural poverty during the Great Depression and later expanded to capture American mobilization for World War II. While the collection is famous for its iconic black-and-white imagery, this dataset highlights the less common, vibrant color work shot on early Kodachrome film. Each row represents a single photograph, accessible via the filepath, and includes rich provenance details such as the photographer (e.g., Russell Lee, Jack Delano) and a descriptive caption. The data is geocoded with city, state, and lat/lon coordinates.\n\nfsac = pl.read_csv(\"data/fsac.csv\")\nfsac.glimpse()\n\nRows: 500\nColumns: 11\n$ filepath     &lt;str&gt; 'media/fsac/1a35266v.jpg', 'media/fsac/1a34940v.jpg', 'media/fsac/1a34143v.jpg', 'media/fsac/1a35375v.jpg', 'media/fsac/1a34758v.jpg', 'media/fsac/1a34718v.jpg', 'media/fsac/1a34400v.jpg', 'media/fsac/1a34371v.jpg', 'media/fsac/1a34230v.jpg', 'media/fsac/1a34505v.jpg'\n$ call_number  &lt;str&gt; 'LC-DIG-fsac-1a35266', 'LC-DIG-fsac-1a34940', 'LC-DIG-fsac-1a34143', 'LC-DIG-fsac-1a35375', 'LC-DIG-fsac-1a34758', 'LC-DIG-fsac-1a34718', 'LC-DIG-fsac-1a34400', 'LC-DIG-fsac-1a34371', 'LC-DIG-fsac-1a34230', 'LC-DIG-fsac-1a34505'\n$ photographer &lt;str&gt; 'Alfred T. Palmer', 'Howard R. Hollem', 'Russell Lee', 'Alfred T. Palmer', 'Jack Delano', 'Jack Delano', 'Marion Post Wolcott', 'Marion Post Wolcott', 'Russell Lee', 'John Collier'\n$ caption      &lt;str&gt; 'Eight generator units in the generator room of a new addition to TVA&#39;s hydroelectric plant at Wilson Dam, Sheffield vicinity, Ala. Located 260 miles above the mouth of the Tennessee River, the dam has an authorized power installation of 288,000 kw., which can be increased to a possible ultimate of 444,000 kw. The reservoir at the dam adds 377,000 acre-feet of water to controlled storage on the Tennessee River system', 'Metal tubing at the Consolidated Aircraft Corp. plant, Fort Worth, Texas', 'The school at Pie Town, New Mexico is held in the Farm Bureau building, which was constructed by cooperative effort', 'Drilling horizontal stabilizers: operating a hand drill, this woman worker at Vultee-Nashville is shown working on the horizontal stabilizer for a Vultee &quot;Vengeance&quot; dive bomber, Tennessee. The &quot;Vengeance&quot; (A-31) was originally designed for the French. It was later adopted by the R.A.F. and still later by the U.S. Army Air Forces. It is a single-engine, low-wing plane, carrying a crew of two men and having six machine guns of varying calibers', 'Santa Fe R.R. trains going through Cajon Pass in the San Bernardino Mountains, Cajon, Calif. On the right, streamliner &quot;Chief&quot; going west; in the background, on the left, a freight train with a helper engine, going east. Santa Fe trip', 'General view of the city and the Atchison, Topeka, and Santa Fe Railroad, Amarillo, Texas. Santa Fe R.R. trip', 'Houses which have been condemned by the Board of Health but are still occupied by Negro migratory workers, Belle Glade, Fla.', 'Burley tobacco is placed on sticks to wilt after cutting, before it is taken into the barn for drying and curing on the Russell Spears&#39; farm, vicinity of Lexington, Ky.', 'Shasta dam under construction, California', 'Trampas, New Mexico'\n$ year         &lt;i64&gt; 1942, 1942, 1940, 1943, 1943, 1943, 1941, 1940, 1942, 1943\n$ month        &lt;i64&gt; 6, 10, 10, 2, 3, 3, 1, 9, 6, 1\n$ state        &lt;str&gt; 'Alabama', 'Texas', 'New Mexico', 'Tennessee', 'California', 'Texas', 'Florida', 'Kentucky', 'California', 'New Mexico'\n$ city         &lt;str&gt; 'Sheffield', 'Fort Worth', 'Pie Town', 'Nashville', 'Cajon', 'Amarillo', 'Belle Glade', 'Lexington', 'Redding', 'Trampas'\n$ county       &lt;str&gt; 'Colbert County', 'Tarrant County', 'Catron County', 'Davidson County', 'San County', 'Potter County', 'Palm Beach', 'Fayette County', 'Shasta County', 'Taos County'\n$ longitude    &lt;f64&gt; -87.6986407, -97.3208496, -108.1347836, -86.7844432, -116.9625269, -101.8312969, -80.6675577, -84.4777153, -122.3916754, -105.7589053\n$ latitude     &lt;f64&gt; 34.7650887, 32.725409, 34.2983884, 36.1658899, 32.7947731, 35.2219971, 26.6845104, 37.9886892, 40.5865396, 36.1311359",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#mnist",
    "href": "22_datasets.html#mnist",
    "title": "22  Datasets",
    "section": "22.25 MNIST ",
    "text": "22.25 MNIST \nThe mnist dataset is a 1,000-sample subset of the classic MNIST database, widely considered the “Hello World” of computer vision. It consists of grayscale images of handwritten digits ranging from 0 to 9. The table links the ground-truth numeric label to the corresponding image file via the filepath column and assigns each observation to a specific data partition (index) for training or testing purposes.\n\nmnist = pl.read_csv(\"data/mnist_1000.csv\")\nmnist.glimpse()\n\nRows: 1000\nColumns: 3\n$ label    &lt;i64&gt; 3, 9, 9, 8, 8, 5, 9, 7, 3, 2\n$ filepath &lt;str&gt; 'media/mnist_1000/00000.png', 'media/mnist_1000/00001.png', 'media/mnist_1000/00002.png', 'media/mnist_1000/00003.png', 'media/mnist_1000/00004.png', 'media/mnist_1000/00005.png', 'media/mnist_1000/00006.png', 'media/mnist_1000/00007.png', 'media/mnist_1000/00008.png', 'media/mnist_1000/00009.png'\n$ index    &lt;str&gt; 'test', 'test', 'train', 'test', 'train', 'train', 'train', 'train', 'train', 'train'\n\n\n\nThe emnist dataset is a 10,000-sample subset of EMNIST (Extended MNIST), a more challenging dataset that expands the original digit recognition task to include handwritten letters. Unlike the standard MNIST, the label column here is alphanumeric, containing a mix of digits and both uppercase and lowercase characters (e.g., ‘g’, ‘P’, ‘4’), reflecting the greater complexity of the classification problem.\n\nemnist = pl.read_csv(\"data/emnist_10000.csv\")\nemnist.glimpse()\n\nRows: 10000\nColumns: 3\n$ label    &lt;str&gt; 'g', 'b', 'P', '9', 't', '4', 'M', 'b', 'e', 'A'\n$ filepath &lt;str&gt; 'media/emnist_10000/00000.png', 'media/emnist_10000/00001.png', 'media/emnist_10000/00002.png', 'media/emnist_10000/00003.png', 'media/emnist_10000/00004.png', 'media/emnist_10000/00005.png', 'media/emnist_10000/00006.png', 'media/emnist_10000/00007.png', 'media/emnist_10000/00008.png', 'media/emnist_10000/00009.png'\n$ index    &lt;str&gt; 'train', 'train', 'train', 'train', 'test', 'train', 'train', 'train', 'train', 'train'\n\n\n\nThe fmnist dataset represents a 10,000-sample subset of Fashion-MNIST, designed by Zalando Research as a modern, more difficult drop-in replacement for the original digit dataset. Instead of abstract numbers, the images depict distinct articles of clothing and accessories. The label column provides the text description of the class—such as “dress,” “pullover,” or “ankle boot”—making it intuitive to interpret model errors (e.g., confusing a “shirt” with a “t-shirt”).\n\nfmnist = pl.read_csv(\"data/fashionmnist_10000.csv\")\nfmnist.glimpse()\n\nRows: 10000\nColumns: 3\n$ label    &lt;str&gt; 'dress', 'shirt', 'pullover', 'pullover', 'ankle boot', 'pullover', 'trouser', 'ankle boot', 'ankle boot', 'pullover'\n$ filepath &lt;str&gt; 'media/fashionmnist_10000/00000.png', 'media/fashionmnist_10000/00001.png', 'media/fashionmnist_10000/00002.png', 'media/fashionmnist_10000/00003.png', 'media/fashionmnist_10000/00004.png', 'media/fashionmnist_10000/00005.png', 'media/fashionmnist_10000/00006.png', 'media/fashionmnist_10000/00007.png', 'media/fashionmnist_10000/00008.png', 'media/fashionmnist_10000/00009.png'\n$ index    &lt;str&gt; 'test', 'test', 'train', 'test', 'train', 'train', 'train', 'train', 'train', 'test'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#imagenet",
    "href": "22_datasets.html#imagenet",
    "title": "22  Datasets",
    "section": "22.26 ImageNet ",
    "text": "22.26 ImageNet \nThe inet dataset is a 1,000-sample subset of Imagenette, a corpus derived from the massive ImageNet database but restricted to ten distinct, easily distinguishable classes (e.g., “church,” “gas_pump,” “garbage_truck”). Designed as a lightweight benchmark for rapid prototyping, it links each ground-truth label to its source image via filepath. Crucially, this version comes pre-packaged with two state-of-the-art embedding vectors (vit and siglip), allowing researchers to immediately compare how different vision transformer architectures represent these visually distinct objects without needing to run heavy inference tasks.\n\ninet = pl.read_parquet(\"data/imagenette_1000.parquet\")\ninet.drop(c.vit, c.siglip).glimpse()\n\nRows: 1000\nColumns: 3\n$ label    &lt;str&gt; 'church', 'tench', 'church', 'gas_pump', 'tench', 'garbage_truck', 'tench', 'gas_pump', 'gas_pump', 'garbage_truck'\n$ filepath &lt;str&gt; 'media/imagenette_1000/00000.png', 'media/imagenette_1000/00001.png', 'media/imagenette_1000/00002.png', 'media/imagenette_1000/00003.png', 'media/imagenette_1000/00004.png', 'media/imagenette_1000/00005.png', 'media/imagenette_1000/00006.png', 'media/imagenette_1000/00007.png', 'media/imagenette_1000/00008.png', 'media/imagenette_1000/00009.png'\n$ index    &lt;str&gt; 'train', 'train', 'test', 'train', 'train', 'train', 'test', 'test', 'train', 'train'\n\n\n\nThe woof dataset is a 1,000-sample subset of Imagewoof, a significantly harder classification challenge also drawn from ImageNet. Unlike the broad categories in Imagenette, this dataset focuses exclusively on ten specific dog breeds (e.g., “shih_tzu,” “border_terrier,” “dingo”), forcing models to discern fine-grained features rather than gross structural differences. Like its sibling dataset, it includes standard metadata (label, filepath, index) and is enriched with pre-computed vit and siglip embeddings, making it an ideal testbed for evaluating the discriminative power of modern vision models on subtle, fine-grained tasks.\n\nwoof = pl.read_parquet(\"data/imagewoof_1000.parquet\")\nwoof.drop(c.vit, c.siglip).glimpse()\n\nRows: 1000\nColumns: 3\n$ label    &lt;str&gt; 'shih_tzu', 'border_terrier', 'australian_terrier', 'golden_retriever', 'dingo', 'english_foxhound', 'border_terrier', 'old_english_sheepdog', 'rhodesian_ridgeback', 'rhodesian_ridgeback'\n$ filepath &lt;str&gt; 'media/imagewoof_1000/00000.png', 'media/imagewoof_1000/00001.png', 'media/imagewoof_1000/00002.png', 'media/imagewoof_1000/00003.png', 'media/imagewoof_1000/00004.png', 'media/imagewoof_1000/00005.png', 'media/imagewoof_1000/00006.png', 'media/imagewoof_1000/00007.png', 'media/imagewoof_1000/00008.png', 'media/imagewoof_1000/00009.png'\n$ index    &lt;str&gt; 'test', 'train', 'train', 'test', 'test', 'test', 'train', 'train', 'train', 'train'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#oxford-flowers",
    "href": "22_datasets.html#oxford-flowers",
    "title": "22  Datasets",
    "section": "22.27 Oxford Flowers ",
    "text": "22.27 Oxford Flowers \nThe flowers dataset is a 1,000-sample subset of the Oxford Flowers dataset, a standard benchmark for fine-grained image classification. Unlike general object recognition, this dataset focuses on distinguishing between closely related floral species, with label entries such as “foxglove,” “clematis,” and “bishop of llandaff.” Each row links the specific flower class to its image via filepath and assigns it to a training or testing partition (index). Consistent with the previous vision datasets, it is enriched with pre-computed vit and siglip embedding vectors, enabling researchers to immediately apply clustering or similarity search to explore how different models encode botanical features.\n\nflowers = pl.read_parquet(\"data/flowers_1000.parquet\")\nflowers.drop(c.vit, c.siglip).glimpse()\n\nRows: 1000\nColumns: 3\n$ label    &lt;str&gt; 'foxglove', 'clematis', 'tree mallow', 'bishop of llandaff', 'daffodil', 'hippeastrum', 'bougainvillea', 'rose', 'bougainvillea', 'azalea'\n$ filepath &lt;str&gt; 'media/flowers_1000/00000.png', 'media/flowers_1000/00001.png', 'media/flowers_1000/00002.png', 'media/flowers_1000/00003.png', 'media/flowers_1000/00004.png', 'media/flowers_1000/00005.png', 'media/flowers_1000/00006.png', 'media/flowers_1000/00007.png', 'media/flowers_1000/00008.png', 'media/flowers_1000/00009.png'\n$ index    &lt;str&gt; 'test', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "22_datasets.html#caltech-ucsd-birds",
    "href": "22_datasets.html#caltech-ucsd-birds",
    "title": "22  Datasets",
    "section": "22.28 Caltech-UCSD Birds ",
    "text": "22.28 Caltech-UCSD Birds \nThe birds dataset is a curated subset of the Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset, a renowned benchmark for fine-grained visual categorization. While the full collection covers 200 species, this version (birds10) likely restricts the task to 10 distinct classes (e.g., “canary”), making it more accessible for rapid model testing. Each entry links the species label to its image filepath and assigns it to a data partition (index). Like the other classification datasets in this series, it comes ready-to-use with pre-computed vit and siglip embeddings, allowing for immediate exploration of how transformer models distinguish between subtle avian features.\n\nbirds = pl.read_parquet(\"data/birds10.parquet\")\nbirds.drop(c.vit, c.siglip).glimpse()\n\nRows: 1555\nColumns: 3\n$ label    &lt;str&gt; 'canary', 'canary', 'canary', 'canary', 'canary', 'canary', 'canary', 'canary', 'canary', 'canary'\n$ filepath &lt;str&gt; 'media/birds10/00000.png', 'media/birds10/00001.png', 'media/birds10/00002.png', 'media/birds10/00003.png', 'media/birds10/00004.png', 'media/birds10/00005.png', 'media/birds10/00006.png', 'media/birds10/00007.png', 'media/birds10/00008.png', 'media/birds10/00009.png'\n$ index    &lt;str&gt; 'test', 'train', 'train', 'train', 'train', 'test', 'train', 'train', 'train', 'test'\n\n\n\nThe birds_bbox dataset complements the classification data by focusing on object detection and localization. Also derived from the CUB-200-2011 collection, it contains 1,000 observations where the primary task is not just naming the bird, but locating it within the frame. In addition to the label and filepath, this dataset provides precise coordinates for a bounding box: bbox_x0 and bbox_y0 define the top-left corner, while bbox_x1 and bbox_y1 mark the bottom-right. This granular spatial data is essential for training models to separate the subject from complex natural backgrounds.\n\nbirds_bbox = pl.read_csv(\"data/birds_1000.csv\")\nbirds_bbox.glimpse()\n\nRows: 1000\nColumns: 7\n$ label    &lt;str&gt; 'Gray_Catbird', 'Sayornis', 'Tennessee_Warbler', 'White_throated_Sparrow', 'Ring_billed_Gull', 'Tree_Swallow', 'Florida_Jay', 'Yellow_breasted_Chat', 'Rusty_Blackbird', 'House_Sparrow'\n$ filepath &lt;str&gt; 'media/birds_1000/00000.png', 'media/birds_1000/00001.png', 'media/birds_1000/00002.png', 'media/birds_1000/00003.png', 'media/birds_1000/00004.png', 'media/birds_1000/00005.png', 'media/birds_1000/00006.png', 'media/birds_1000/00007.png', 'media/birds_1000/00008.png', 'media/birds_1000/00009.png'\n$ bbox_x0  &lt;f64&gt; 15.0, 131.0, 40.0, 99.0, 104.0, 165.0, 147.0, 99.0, 86.0, 137.0\n$ bbox_y0  &lt;f64&gt; 44.0, 85.0, 5.0, 42.0, 32.0, 133.0, 89.0, 60.0, 148.0, 77.0\n$ bbox_x1  &lt;f64&gt; 480.0, 488.0, 345.0, 448.0, 451.0, 423.0, 360.0, 377.0, 410.0, 365.0\n$ bbox_y1  &lt;f64&gt; 331.0, 326.0, 239.0, 344.0, 284.0, 286.0, 235.0, 353.0, 300.0, 277.0\n$ index    &lt;str&gt; 'train', 'train', 'test', 'train', 'train', 'train', 'test', 'train', 'train', 'train'",
    "crumbs": [
      "Part V: Notes",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Datasets</span>"
    ]
  }
]