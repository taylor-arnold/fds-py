<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Taylor Arnold">

<title>22&nbsp; Datasets – Foundations of Data Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./21_notes.html" rel="prev">
<link href="./img/owl_explore.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-25064e4154141e7ea34817c6f1220590.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./21_notes.html">Part V: Notes</a></li><li class="breadcrumb-item"><a href="./22_datasets.html"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Datasets</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Foundations of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/taylor-arnold/fds-py" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Part I: Core</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_modify.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">EDA I: Organizing Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_graphics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">EDA II: Visualizing Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_combine.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">EDA III: Restructuring Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_collect.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">EDA IV: Collecting Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Part II: Programming</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_program.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Programming</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_strings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Strings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_dataformats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Data Formats</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_requests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Requests</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Part III: Models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_supervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Supervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_deeplearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14_cnnwordvec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">CNNs and Word2Vec</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15_transferlearn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Transfer Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Part IV: Applications</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16_spatial_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Spatial Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17_temporal_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Temporal Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18_network_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Network Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19_textual_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Textual Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20_image_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Image Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Part V: Notes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21_notes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Notes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22_datasets.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#setup" id="toc-setup" class="nav-link active" data-scroll-target="#setup"><span class="header-section-number">22.1</span> Setup</a></li>
  <li><a href="#countries" id="toc-countries" class="nav-link" data-scroll-target="#countries"><span class="header-section-number">22.2</span> Countries</a></li>
  <li><a href="#food-items" id="toc-food-items" class="nav-link" data-scroll-target="#food-items"><span class="header-section-number">22.3</span> Food Items</a></li>
  <li><a href="#majors-and-salary" id="toc-majors-and-salary" class="nav-link" data-scroll-target="#majors-and-salary"><span class="header-section-number">22.4</span> Majors and Salary</a></li>
  <li><a href="#criterion-films" id="toc-criterion-films" class="nav-link" data-scroll-target="#criterion-films"><span class="header-section-number">22.5</span> Criterion Films</a></li>
  <li><a href="#fifty-years-of-movies" id="toc-fifty-years-of-movies" class="nav-link" data-scroll-target="#fifty-years-of-movies"><span class="header-section-number">22.6</span> Fifty Years of Movies</a></li>
  <li><a href="#rva-flights" id="toc-rva-flights" class="nav-link" data-scroll-target="#rva-flights"><span class="header-section-number">22.7</span> RVA Flights</a></li>
  <li><a href="#what-we-eat-in-america" id="toc-what-we-eat-in-america" class="nav-link" data-scroll-target="#what-we-eat-in-america"><span class="header-section-number">22.8</span> What We Eat in America</a></li>
  <li><a href="#inference-data" id="toc-inference-data" class="nav-link" data-scroll-target="#inference-data"><span class="header-section-number">22.9</span> Inference Data</a></li>
  <li><a href="#keylogging" id="toc-keylogging" class="nav-link" data-scroll-target="#keylogging"><span class="header-section-number">22.10</span> Keylogging</a></li>
  <li><a href="#paris-metro" id="toc-paris-metro" class="nav-link" data-scroll-target="#paris-metro"><span class="header-section-number">22.11</span> Paris Metro <img style="height:0.8em; width: auto" src="img/spatial.png"></a></li>
  <li><a href="#us-city-population" id="toc-us-city-population" class="nav-link" data-scroll-target="#us-city-population"><span class="header-section-number">22.12</span> US City Population <img style="height:0.8em; width: auto" src="img/spatial.png"></a></li>
  <li><a href="#us-metropolitan-regions" id="toc-us-metropolitan-regions" class="nav-link" data-scroll-target="#us-metropolitan-regions"><span class="header-section-number">22.13</span> US Metropolitan Regions <img style="height:0.8em; width: auto" src="img/spatial.png"></a></li>
  <li><a href="#covid" id="toc-covid" class="nav-link" data-scroll-target="#covid"><span class="header-section-number">22.14</span> COVID <img style="height:0.8em; width: auto" src="img/spatial.png"></a></li>
  <li><a href="#u.s.-storms" id="toc-u.s.-storms" class="nav-link" data-scroll-target="#u.s.-storms"><span class="header-section-number">22.15</span> U.S. Storms <img style="height:0.8em; width: auto" src="img/spatial.png"></a></li>
  <li><a href="#shakespeare-plays" id="toc-shakespeare-plays" class="nav-link" data-scroll-target="#shakespeare-plays"><span class="header-section-number">22.16</span> Shakespeare Plays <img style="height:0.8em; width: auto" src="img/text.png"></a></li>
  <li><a href="#wikipedia-authors" id="toc-wikipedia-authors" class="nav-link" data-scroll-target="#wikipedia-authors"><span class="header-section-number">22.17</span> Wikipedia Authors <img style="height:0.8em; width: auto" src="img/text.png"></a></li>
  <li><a href="#imdb-reviews" id="toc-imdb-reviews" class="nav-link" data-scroll-target="#imdb-reviews"><span class="header-section-number">22.18</span> IMDb Reviews <img style="height:0.8em; width: auto" src="img/text.png"></a></li>
  <li><a href="#ag-news" id="toc-ag-news" class="nav-link" data-scroll-target="#ag-news"><span class="header-section-number">22.19</span> AG News <img style="height:0.8em; width: auto" src="img/text.png"></a></li>
  <li><a href="#amazon-reviews" id="toc-amazon-reviews" class="nav-link" data-scroll-target="#amazon-reviews"><span class="header-section-number">22.20</span> Amazon Reviews <img style="height:0.8em; width: auto" src="img/text.png"></a></li>
  <li><a href="#bbc-headlines" id="toc-bbc-headlines" class="nav-link" data-scroll-target="#bbc-headlines"><span class="header-section-number">22.21</span> BBC Headlines <img style="height:0.8em; width: auto" src="img/text.png"></a></li>
  <li><a href="#sentiment-treebank" id="toc-sentiment-treebank" class="nav-link" data-scroll-target="#sentiment-treebank"><span class="header-section-number">22.22</span> Sentiment Treebank <img style="height:0.8em; width: auto" src="img/text.png"></a></li>
  <li><a href="#goemotions" id="toc-goemotions" class="nav-link" data-scroll-target="#goemotions"><span class="header-section-number">22.23</span> GoEmotions <img style="height:0.8em; width: auto" src="img/text.png"></a></li>
  <li><a href="#fsa-owi-color-images" id="toc-fsa-owi-color-images" class="nav-link" data-scroll-target="#fsa-owi-color-images"><span class="header-section-number">22.24</span> FSA-OWI Color Images <img style="height:0.8em; width: auto" src="img/vision.png"></a></li>
  <li><a href="#mnist" id="toc-mnist" class="nav-link" data-scroll-target="#mnist"><span class="header-section-number">22.25</span> MNIST <img style="height:0.8em; width: auto" src="img/vision.png"></a></li>
  <li><a href="#imagenet" id="toc-imagenet" class="nav-link" data-scroll-target="#imagenet"><span class="header-section-number">22.26</span> ImageNet <img style="height:0.8em; width: auto" src="img/vision.png"></a></li>
  <li><a href="#oxford-flowers" id="toc-oxford-flowers" class="nav-link" data-scroll-target="#oxford-flowers"><span class="header-section-number">22.27</span> Oxford Flowers <img style="height:0.8em; width: auto" src="img/vision.png"></a></li>
  <li><a href="#caltech-ucsd-birds" id="toc-caltech-ucsd-birds" class="nav-link" data-scroll-target="#caltech-ucsd-birds"><span class="header-section-number">22.28</span> Caltech-UCSD Birds <img style="height:0.8em; width: auto" src="img/vision.png"></a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./21_notes.html">Part V: Notes</a></li><li class="breadcrumb-item"><a href="./22_datasets.html"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Datasets</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-datasets" class="quarto-section-identifier"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Datasets</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="setup" class="level2" data-number="22.1">
<h2 data-number="22.1" class="anchored" data-anchor-id="setup"><span class="header-section-number">22.1</span> Setup</h2>
<p>Load all of the modules and datasets needed for the chapter. In each of the sections below we briefly present the datasets used in this text and the supplemental materials. The <code>glimpse</code> method is used to show all of the column names, data types, and the first few rows of the dataset.</p>
<div id="040c14bc" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> polars <span class="im">as</span> pl</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> funs <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> plotnine <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> polars <span class="im">import</span> col <span class="im">as</span> c</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>theme_set(theme_minimal())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="countries" class="level2" data-number="22.2">
<h2 data-number="22.2" class="anchored" data-anchor-id="countries"><span class="header-section-number">22.2</span> Countries</h2>
<p>Sourced from GapMinder and WikiData, the countries dataset provides a snapshot of 135 nations, identifying each by its full standard name and three-letter ISO code. Geographically, entries are categorized into broad regions and specific subregions, accompanied by precise latitude and longitude coordinates. The data captures essential socioeconomic health through metrics such as total population (in millions), life expectancy, and the Human Development Index (HDI). Economic conditions are represented by GDP figures and the Gini coefficient, which measures income inequality, while broader well-being is gauged via a happiness index. Additionally, the dataset includes infrastructure and cultural details, specifically tracking cellphone adoption rates, the percentage of the population with access to improved water sources, and the primary languages spoken.</p>
<div id="1ac2517d" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>countries <span class="op">=</span> pl.read_csv(<span class="st">"data/countries.csv"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>countries.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 135
Columns: 15
$ iso          &lt;str&gt; 'SEN', 'VEN', 'FIN', 'USA', 'LKA', 'DOM', 'SGP', 'GAB', 'BGR', 'TZA'
$ full_name    &lt;str&gt; 'Senegal', 'Venezuela, Bolivarian Republic of', 'Finland', 'United States of America', 'Sri Lanka', 'Dominican Republic', 'Singapore', 'Gabon', 'Bulgaria', 'Tanzania, United Republic of'
$ region       &lt;str&gt; 'Africa', 'Americas', 'Europe', 'Americas', 'Asia', 'Americas', 'Asia', 'Africa', 'Europe', 'Africa'
$ subregion    &lt;str&gt; 'Western Africa', 'South America', 'Northern Europe', 'Northern America', 'Southern Asia', 'Caribbean', 'South-eastern Asia', 'Middle Africa', 'Eastern Europe', 'Eastern Africa'
$ pop          &lt;f64&gt; 18.932, 28.517, 5.623, 347.276, 23.229, 11.52, 5.871, 2.593, 6.715, 70.546
$ lexp         &lt;f64&gt; 70.43, 76.18, 82.84, 79.83, 78.51, 74.35, 85.63, 68.68, 74.33, 68.59
$ lat          &lt;f64&gt; 14.366667, 8.0, 65.0, 39.828175, 7.0, 18.8, 1.3, -0.683330555, 42.75, -6.306944444
$ lon          &lt;f64&gt; -14.283333, -67.0, 27.0, -98.5795, 81.0, -70.2, 103.8, 11.5, 25.5, 34.853888888
$ hdi          &lt;f64&gt; 0.53, 0.709, 0.948, 0.938, 0.776, 0.776, 0.946, 0.733, 0.845, 0.555
$ gdp          &lt;i64&gt; 4871, 8899, 57574, 78389, 14380, 25663, 137906, 19543, 36211, 3924
$ gini         &lt;f64&gt; 38.1, 44.8, 27.7, 47.7, 39.3, 39.6, null, 38.0, 40.3, 40.5
$ happy        &lt;f64&gt; 50.93, 57.65, 76.99, 65.21, 36.02, 59.21, 66.54, 51.04, 55.9, 40.42
$ cellphone    &lt;f64&gt; 66.0, 96.8, 156.4, 91.7, 83.1, 90.6, 145.5, 93.6, 137.1, 46.9
$ water_access &lt;f64&gt; 54.93987, 95.66913, 99.44798, 99.72235, 90.77437, 86.1939, 100.0, 49.20331, 86.00395, 26.78297
$ lang         &lt;str&gt; 'pbp|fra|wol', 'spa|vsl', 'fin|swe', 'eng', 'sin|sin|tam|tam', 'spa', 'eng|msa|cmn|tam', 'fra', 'bul', 'eng|swa'
</code></pre>
</div>
</div>
<p>Also sourced from GapMinder, the cellphone dataset is a longitudinal record containing 3,480 observations that track the adoption of mobile technology over time. Unlike the previous cross-sectional dataset, this table uses a time-series format, recording data for specific nations identified by their three-letter iso codes across multiple years. The primary metric, cell, quantifies mobile phone subscriptions (expressed per 100 people), allowing for the analysis of growth trends and technological saturation within different countries over the recorded period.</p>
<div id="734905d5" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>cellphone <span class="op">=</span> pl.read_csv(<span class="st">"data/countries_cellphone.csv"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>cellphone.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 3480
Columns: 3
$ iso  &lt;str&gt; 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AFG'
$ year &lt;i64&gt; 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012
$ cell &lt;f64&gt; 0.87978, 2.54662, 4.91711, 9.9133, 18.0167, 29.8268, 38.2289, 36.1187, 47.0152, 50.1967
</code></pre>
</div>
</div>
<p>Sourced from Wikidata, the borders dataset provides a relational map of international boundaries, containing 829 entries that define connections between nations. Each row represents a single land border, linking a primary country (iso) to one of its adjacent neighbors (iso_neighbor) using their three-letter ISO codes. Because a single country often shares borders with multiple neighbors, the iso column contains repeated values, effectively creating an adjacency list that allows for the analysis of geographic clustering, continent connectivity, and geopolitical relationships.</p>
<div id="4c73388d" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>borders <span class="op">=</span> pl.read_csv(<span class="st">"data/countries_borders.csv"</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>borders.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 829
Columns: 2
$ iso          &lt;str&gt; 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AFG', 'AGO', 'AGO', 'AGO', 'AGO'
$ iso_neighbor &lt;str&gt; 'IRN', 'PAK', 'CHN', 'TJK', 'TKM', 'UZB', 'COD', 'GAB', 'NAM', 'COG'
</code></pre>
</div>
</div>
</section>
<section id="food-items" class="level2" data-number="22.3">
<h2 data-number="22.3" class="anchored" data-anchor-id="food-items"><span class="header-section-number">22.3</span> Food Items</h2>
<p>The food dataset profiles 61 common culinary items, providing a comprehensive nutritional and descriptive breakdown for each. It categorizes items into broad food_group classifications (such as fruits, vegetables, grains, and meats) and details their dietary composition through macronutrients—including total and saturated fats, carbohydrates, sugar, fiber, and protein—as well as cholesterol and calorie counts. The dataset also tracks micronutrient content, specifically sodium, iron, and vitamins A and C. Beyond nutritional metrics, the table includes metadata sourced from WikiData, such as a URL slug (wiki), a textual description defining the item, and its primary visual color.</p>
<div id="99bf10fd" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>food <span class="op">=</span> pl.read_csv(<span class="st">"data/food.csv"</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>food.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 61
Columns: 17
$ item        &lt;str&gt; 'Apple', 'Asparagus', 'Avocado', 'Banana', 'Chickpea', 'String Bean', 'Beef', 'Bell Pepper', 'Crab', 'Broccoli'
$ food_group  &lt;str&gt; 'fruit', 'vegetable', 'fruit', 'fruit', 'grains', 'vegetable', 'meat', 'vegetable', 'fish', 'vegetable'
$ calories    &lt;i64&gt; 52, 20, 160, 89, 180, 31, 288, 26, 87, 34
$ total_fat   &lt;f64&gt; 0.1, 0.1, 14.6, 0.3, 2.9, 0.1, 19.5, 0.0, 1.0, 0.3
$ sat_fat     &lt;f64&gt; 0.028, 0.046, 2.126, 0.112, 0.309, 0.026, 7.731, 0.059, 0.222, 0.039
$ cholesterol &lt;i64&gt; 0, 0, 0, 0, 0, 0, 87, 0, 78, 0
$ sodium      &lt;i64&gt; 1, 2, 7, 1, 243, 6, 384, 2, 293, 33
$ carbs       &lt;f64&gt; 13.81, 3.88, 8.53, 22.84, 29.98, 7.13, 0.0, 6.03, 0.04, 6.64
$ fiber       &lt;f64&gt; 2.4, 2.1, 6.7, 2.6, 8.6, 3.4, 0.0, 2.0, 0.0, 2.6
$ sugar       &lt;f64&gt; 10.39, 1.88, 0.66, 12.23, 5.29, 1.4, 0.0, 4.2, 0.0, 1.7
$ protein     &lt;f64&gt; 0.26, 2.2, 2.0, 1.09, 9.54, 1.82, 26.33, 0.99, 18.06, 2.82
$ iron        &lt;i64&gt; 1, 12, 3, 1, 17, 6, 15, 2, 4, 4
$ vitamin_a   &lt;i64&gt; 1, 15, 3, 1, 0, 14, 0, 63, 0, 12
$ vitamin_c   &lt;i64&gt; 8, 9, 17, 15, 3, 27, 0, 317, 5, 149
$ wiki        &lt;str&gt; 'apple', 'asparagus', 'avocado', 'banana', 'chickpea', 'green_bean', 'beef', 'bell_pepper', 'callinectes_sapidus', 'broccoli'
$ description &lt;str&gt; 'A common, round fruit produced by the tree &lt;i&gt;Malus domestica&lt;/i&gt;, cultivated in temperate climates.', 'Any of various perennial plants of the genus &lt;i&gt;Asparagus&lt;/i&gt; having leaflike stems, scalelike leaves, and small flowers.', 'The large, usually yellowish-green or black, pulpy fruit of the avocado tree.', 'An elongated curved tropical fruit that grows in bunches and has a creamy flesh and a smooth skin.', 'An annual Asian plant (&lt;i&gt;Cicer arietinum&lt;/i&gt;) in the pea family, widely cultivated for the edible seeds in its short inflated pods.', 'A long, slender variety of green bean.', 'The meat from a cow, bull or other bovine.', '&lt;i&gt;Capsicum annuum&lt;/i&gt;, an edible spicy-sweet fruit, originating in the New World.', 'A crustacean of the infraorder &lt;i&gt;Brachyura&lt;/i&gt;, having five pairs of legs, the foremost of which are in the form of claws, and a carapace.', 'A plant, &lt;i&gt;Brassica oleracea var. italica&lt;/i&gt;, of the cabbage family, Brassicaceae; especially, the tree-shaped flower and stalk that are eaten as a vegetable.'
$ color       &lt;str&gt; 'red', 'green', 'green', 'yellow', 'brown', 'green', 'red', 'green', 'red', 'green'
</code></pre>
</div>
</div>
<p>The diet dataset is a small reference table containing 6 rows that define dietary compliance for major food groups. It links broad food_group categories (such as fruit, vegetable, grains, meat, fish, and dairy) to specific restrictive diets. Boolean-style columns (yes/no) indicate whether each group is permissible within vegan, vegetarian, and pescatarian lifestyles, effectively serving as a lookup table for filtering food items based on dietary restrictions.</p>
<div id="bb3efb92" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>diet <span class="op">=</span> pl.read_csv(<span class="st">"data/food_diet_restrictions.csv"</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>diet.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 6
Columns: 4
$ food_group  &lt;str&gt; 'fruit', 'vegetable', 'grains', 'meat', 'fish', 'dairy'
$ vegan       &lt;str&gt; 'yes', 'yes', 'yes', 'no', 'no', 'no'
$ vegetarian  &lt;str&gt; 'yes', 'yes', 'yes', 'no', 'no', 'yes'
$ pescatarian &lt;str&gt; 'yes', 'yes', 'yes', 'no', 'yes', 'yes'
</code></pre>
</div>
</div>
<p>The recipe dataset provides a structural breakdown of culinary dishes, listing the specific components required to prepare them. Organized in a “long” format, each row represents a single ingredient for a given recipe, rather than a single row per dish. This means complex recipes like “Pot Roast” or “Guacamole” appear across multiple lines, each detailing a constituent item and its corresponding amount (in grams). This granular structure facilitates the aggregation of nutritional data by allowing individual ingredients to be linked back to detailed food profiles.</p>
<div id="bc21705e" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>recipe <span class="op">=</span> pl.read_csv(<span class="st">"data/food_recipes.csv"</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>recipe.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 10
Columns: 3
$ recipe     &lt;str&gt; 'Pot Roast', 'Pot Roast', 'Pot Roast', 'Pot Roast', 'Pot Roast', 'Pot Roast', 'Guacamole', 'Guacamole', 'Guacamole', 'Guacamole'
$ ingredient &lt;str&gt; 'Beef', 'Carrot', 'Potato', 'Onion', 'Tomato', 'Bay Leaf', 'Avocado', 'Onion', 'Tomato', 'Lime'
$ amount     &lt;i64&gt; 1200, 400, 1000, 500, 200, 5, 1000, 500, 500, 150
</code></pre>
</div>
</div>
</section>
<section id="majors-and-salary" class="level2" data-number="22.4">
<h2 data-number="22.4" class="anchored" data-anchor-id="majors-and-salary"><span class="header-section-number">22.4</span> Majors and Salary</h2>
<p>Sourced from the U.S. Bureau of Labor Statistics, the major dataset offers a high-resolution profile of the earnings distribution for various undergraduate fields of study. Unlike summary tables that report only a median income, this dataset uses a long-format structure to trace the entire salary curve, containing 8,316 rows that correspond to 99 percentile ranks for roughly 84 distinct majors. For each major, the data lists the percentile (from 1 to 99) and the associated earnings value at that rank. This granular approach allows for a deeper analysis of financial outcomes, enabling comparisons of income inequality within fields and assessing the risk-reward profiles—such as the reliable “floor” versus the potential “ceiling” of wages—across different career paths.</p>
<div id="ef937b61" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>major <span class="op">=</span> pl.read_csv(<span class="st">"data/majors.csv"</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>major.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 8316
Columns: 3
$ major      &lt;str&gt; 'Accounting', 'Accounting', 'Accounting', 'Accounting', 'Accounting', 'Accounting', 'Accounting', 'Accounting', 'Accounting', 'Accounting'
$ percentile &lt;i64&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10
$ earnings   &lt;f64&gt; 1.538733, 1.673857, 1.760265, 1.816639, 1.872135, 1.933803, 1.980822, 2.019701, 2.059812, 2.090352
</code></pre>
</div>
</div>
</section>
<section id="criterion-films" class="level2" data-number="22.5">
<h2 data-number="22.5" class="anchored" data-anchor-id="criterion-films"><span class="header-section-number">22.5</span> Criterion Films</h2>
<p>The film dataset contains 1,479 entries from the Criterion Collection, a prestigious home-video distribution company dedicated to preserving and publishing “important classic and contemporary films” from around the world. Often regarded as a canon of cinema as an art form, the collection includes technically restored and historically significant works.</p>
<p>The dataset identifies each film by its standard title, release year, and unique imdb_id. It captures the creative backbone of each work through columns for directors, writers, and genre classifications, alongside production details like the country of origin, primary languages, and runtime. Critical reception is well-documented with aggregated scores from IMDb (including vote counts), Rotten Tomatoes, and Metacritic. Additionally, the table is enriched with encyclopedic context via Wikipedia extracts and descriptions, and occasionally includes financial metrics like production budgets and box office returns.</p>
<div id="3b27de32" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>film <span class="op">=</span> pl.read_csv(<span class="st">"data/criterion.csv"</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>film.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1479
Columns: 18
$ imdb_id               &lt;str&gt; 'tt0012349', 'tt0012364', 'tt0013257', 'tt0014429', 'tt0014624', 'tt0015634', 'tt0015768', 'tt0015841', 'tt0016142', 'tt0017075'
$ title                 &lt;str&gt; 'The Kid', 'The Phantom Carriage', 'Häxan', 'Safety Last!', 'A Woman of Paris: A Drama of Fate', 'Body and Soul', 'Master of the House', 'The Freshman', 'The Mystic', 'The Lodger: A Story of the London Fog'
$ year                  &lt;i64&gt; 1921, 1921, 1922, 1923, 1923, 1925, 1925, 1925, 1925, 1927
$ language              &lt;str&gt; 'None|English', 'None|Swedish', 'Swedish|Danish', 'English', 'English', 'English', 'Danish', 'None|English', 'None|English', 'None'
$ genre                 &lt;str&gt; 'Comedy|Drama|Family', 'Drama|Fantasy|Horror', 'Documentary|Fantasy|Horror', 'Action|Comedy|Thriller', 'Drama|Romance', 'Crime|Drama|Thriller', 'Comedy|Drama', 'Comedy|Family|Romance', 'Drama', 'Crime|Drama|Mystery'
$ director              &lt;str&gt; 'Charles Chaplin', 'Victor Sjöström', 'Benjamin Christensen', 'Fred C. Newmeyer|Sam Taylor', 'Charles Chaplin', 'Oscar Micheaux', 'Carl Theodor Dreyer', 'Fred C. Newmeyer|Sam Taylor', 'Tod Browning', 'Alfred Hitchcock'
$ writer                &lt;str&gt; 'Charles Chaplin', 'Selma Lagerlöf|Victor Sjöström', 'Benjamin Christensen', 'Hal Roach|Sam Taylor|Tim Whelan', 'Charles Chaplin', 'Oscar Micheaux', 'Carl Theodor Dreyer|Svend Rindom', 'Sam Taylor|Ted Wilde|John Grey', 'Tod Browning|Waldemar Young', 'Marie Belloc Lowndes|Eliot Stannard|Alfred Hitchcock'
$ country               &lt;str&gt; 'United States', 'Sweden', 'Sweden|Denmark', 'United States', 'United States', 'United States', 'Denmark', 'United States', 'United States', 'United Kingdom'
$ imdb_votes            &lt;i64&gt; 142797, 15311, 18391, 23503, 6548, 1221, 2506, 6373, 489, 14371
$ rating_imdb           &lt;f64&gt; 8.2, 8.0, 7.6, 8.1, 6.9, 6.2, 7.0, 7.5, 6.7, 7.3
$ rating_rt             &lt;i64&gt; 100, 100, 93, 97, 94, null, 100, 95, null, 96
$ rating_mc             &lt;i64&gt; null, null, null, null, 76, null, null, null, null, 82
$ runtime_raw           &lt;i64&gt; 68, 106, 107, 73, 84, 102, 107, 76, 70, 70
$ wikipedia_pageid      &lt;i64&gt; 1346905, 7329426, 3644898, 76313, 546663, 1506585, 11072916, 3831825, 17325678, 287408
$ wikipedia_description &lt;str&gt; '1921 silent film by Charlie Chaplin', '1921 film by Victor Sjöström', 'Swedish 1922 silent horror essay film', '1923 American silent romantic comedy film', '1923 drama film by Charlie Chaplin', '1925 film directed by Oscar Micheaux', '1925 film by Carl Theodor Dreyer', '1925 film', '1925 film', '1927 silent film by Alfred Hitchcock'
$ wikipedia_extract     &lt;str&gt; "The Kid is a 1921 American silent comedy-drama film written, produced, directed by and starring Charlie Chaplin, and features Jackie Coogan as his foundling baby, adopted son and sidekick. This was Chaplin's first full-length film as a director. It was a huge success and was the second-highest-grossing film in 1921. Now considered one of the greatest films of the silent era, it was selected for preservation in the United States National Film Registry by the Library of Congress in 2011.", "The Phantom Carriage is a 1921 Swedish silent film directed by and starring Victor Sjöström, based on the 1912 novel Thy Soul Shall Bear Witness! (Körkarlen) by Swedish author Selma Lagerlöf. In the film, Sjöström plays a drunkard named David Holm who, on the night of New Year's Eve, is compelled by the ghostly driver of Death's carriage to reflect on his past mistakes. Alongside Sjöström, the film's cast includes Hilda Borgström, Tore Svennberg, and Astrid Holm.", "Häxan is a 1922 Swedish-Danish silent horror essay film written and directed by Benjamin Christensen. Consisting partly of documentary-style storytelling as well as dramatized narrative sequences, the film purports to chart the historical roots and superstitions surrounding witchcraft, beginning in the Middle Ages through the 20th century. Based partly on Christensen's own study of the Malleus Maleficarum, a 15th-century German guide for inquisitors, Häxan proposes that such witch-hunts may have stemmed from misunderstandings of mental or neurological disorders, triggering mass hysteria.", "Safety Last! is a 1923 American silent romantic-comedy film starring Harold Lloyd. It includes one of the most famous images from the silent-film era: Lloyd clutching the hands of a large clock as he dangles from the outside of a skyscraper above moving traffic. The film was highly successful and critically hailed, and it cemented Lloyd's status as a major figure in early motion pictures. It is still popular at revivals, and it is viewed today as one of the great film comedies.", 'A Woman of Paris is a 1923 silent drama film written, produced, and directed by Charlie Chaplin. It stars Edna Purviance as the title character, along with Clarence Geldart, Carl Miller, Lydia Knott, Charles K. French and Adolphe Menjou. A United Artists production, the film was an atypical dramatic work for Chaplin.', 'Body and Soul is a 1925 race film produced, written, directed, and distributed by Oscar Micheaux and starring Paul Robeson in his motion picture debut. In 2019, the film was selected by the Library of Congress for inclusion in the National Film Registry for being "culturally, historically, or aesthetically significant".', 'Master of the House is a 1925 Danish silent drama film directed and written by acclaimed filmmaker Carl Theodor Dreyer. The film marked the debut of Karin Nellemose, and it is regarded by many as a classic of Danish cinema.', "The Freshman is a 1925 American silent comedy film that tells the story of a college freshman trying to become popular by joining the school football team. It stars Harold Lloyd, Jobyna Ralston, Brooks Benedict, and James Anderson. It remains one of Lloyd's most successful and enduring films. When the film opened on September 20 at the B.S. Moss Colony Theater on Broadway, Broderick &amp; Felsen's production of Campus Capers was the opening act which was engaged for the full ten weeks of the film's run.", "The Mystic is a 1925 American MGM silent drama film directed by Tod Browning, who also co-wrote it with Waldemar Young. It is the only one of nine silent MGM films directed by Browning from 1925 to 1929 that does not star Lon Chaney. The film costars Aileen Pringle and Conway Tearle. Aileen Pringle's gowns in the film were by already famous Romain de Tirtoff. A print of the film exists.", "The Lodger: A Story of the London Fog is a 1927 British silent thriller film directed by Alfred Hitchcock and starring Marie Ault, Arthur Chesney, June Tripp, Malcolm Keen and Ivor Novello. Hitchcock's third feature film, it was released on 14 February 1927 in London and on 10 June 1928 in New York City. The film is based on the 1913 novel The Lodger by Marie Belloc Lowndes and the play Who Is He? co-written by Belloc Lowndes. Its plot concerns the hunt for a Jack the Ripper-like serial killer in London."
$ budget_raw            &lt;i64&gt; 250000, null, 2000000, 121000, 351000, null, null, 301681, null, 12000
$ box_office_raw        &lt;i64&gt; null, null, null, null, 634000, null, null, null, null, null
</code></pre>
</div>
</div>
</section>
<section id="fifty-years-of-movies" class="level2" data-number="22.6">
<h2 data-number="22.6" class="anchored" data-anchor-id="fifty-years-of-movies"><span class="header-section-number">22.6</span> Fifty Years of Movies</h2>
<p>The movie dataset serves as the central hub, containing 5,000 observations that represent the top-100 grossing U.S. films for each year from 1970 to 2020. It captures essential metadata such as the film’s title, release year, MPA rating, and runtime, alongside measures of commercial and critical success like gross revenue, IMDb user ratings/vote counts, and Metacritic scores. Uniquely, it also includes computer vision metrics derived from the film’s promotional poster, quantifying visual attributes such as poster_brightness, saturation, and edgeness (a measure of visual complexity).</p>
<div id="4d7b6b24" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>movie <span class="op">=</span> pl.read_csv(<span class="st">"data/movies_50_years.csv"</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>movie.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 5000
Columns: 12
$ year              &lt;i64&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970
$ title             &lt;str&gt; 'Love Story', 'Airport', 'MASH', 'Patton', 'The AristoCats', 'Little Big Man', 'Tora! Tora! Tora!', 'Catch-22', 'The Owl and the Pussycat', 'Joe'
$ mpa               &lt;str&gt; 'PG', 'G', 'R', 'GP', 'G', 'PG-13', 'G', 'R', 'PG', 'R'
$ runtime           &lt;i64&gt; 100, 137, 116, 172, 78, 139, 144, 122, 95, 107
$ gross             &lt;f64&gt; 106.4, 100.49, 81.6, 61.7, 37.68, 31.56, 29.55, 24.91, 23.68, 19.32
$ rating_count      &lt;i64&gt; 28330, 16512, 64989, 90461, 87551, 31412, 30347, 20997, 3107, 2633
$ rating            &lt;f64&gt; 6.9, 6.6, 7.5, 7.9, 7.1, 7.6, 7.5, 7.2, 6.5, 6.8
$ metacritic        &lt;i64&gt; null, 42, null, null, null, null, 46, null, null, null
$ poster_brightness &lt;f64&gt; 79.039734052134, 70.73515993593905, 74.5400023238925, 83.12899118937443, 79.79474571281945, 67.96583791250038, 39.79528775599128, 62.28054483453459, 67.22113941912305, 31.82685812294916
$ poster_saturation &lt;f64&gt; 8.029792248510015, 29.28457189363516, 40.103629182765395, 17.433849565817365, 12.481991945072153, 9.016387405954426, 48.60645939534094, 35.96620937559402, 10.24255420931898, 27.578054400938452
$ poster_edgeness   &lt;f64&gt; 4.586166444178613, 4.954734636760736, 3.5102847848915624, 3.657573618987647, 4.400358220849864, 5.359519670438313, 2.1121361608919753, 3.6546805917391145, 4.894551155673808, 4.229543347907548
$ description       &lt;str&gt; 'A boy and a girl from different backgrounds fall in love regardless of their upbringing - and then tragedy strikes.', 'A bomber on board an airplane, an airport almost closed by snow, and various personal problems of the people involved.', 'The staff of a Korean War field hospital use humor and high jinks to keep their sanity in the face of the horror of war.', 'The World War II phase of the career of controversial American general George S. Patton.', 'With the help of a smooth talking tomcat, a family of Parisian felines set to inherit a fortune from their owner try to make it back home after a jealous butler kidnaps them and leaves them in the country.', 'Jack Crabb, looking back from extreme old age, tells of his life being raised by Native Americans and fighting with General Custer.', 'In 1941, following months of economic embargo, Japan prepares to open its war against the United States with a surprise attack on the US naval base at Pearl Harbor.', 'A man is trying desperately to be certified insane during World War II, so he can stop flying missions.', 'A stuffy author enters into an explosive relationship with his neighbor, a foul-mouthed, freewheeling prostitute.', "Two men, Bill, a wealthy conservative, and Joe, a far-right factory worker, form a dangerous bond after Bill confesses to murdering his daughter's drug dealer boyfriend to Joe."
</code></pre>
</div>
</div>
<p>The color dataset provides a detailed breakdown of the color palettes used in the film posters. Structured in a long format, it links each movie to multiple rows representing specific color categories—spanning hues like “red” or “blue” and greyscale tones like “black” or “white.” The percentage column quantifies the dominance of each color, enabling the analysis of visual trends in movie marketing over the last half-century (such as the rise of darker or more saturated poster designs).</p>
<div id="f7c97463" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>color <span class="op">=</span> pl.read_csv(<span class="st">"data/movies_50_years_color.csv"</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>color.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 46980
Columns: 5
$ year       &lt;i64&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970
$ title      &lt;str&gt; 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story'
$ color_type &lt;str&gt; 'hue', 'hue', 'hue', 'hue', 'hue', 'hue', 'greyscale', 'greyscale', 'greyscale', 'hue'
$ color      &lt;str&gt; 'red', 'orange', 'yellow', 'green', 'blue', 'violet', 'black', 'grey', 'white', 'other'
$ percentage &lt;f64&gt; 2.6356547746208077, 3.0933561204870754, 0.05420850245674002, 0.2360606707968383, 0.31937620166631064, 0.0005340739158299509, 10.963736381115147, 6.9082461012604135, 75.78882717368084, 0.0
</code></pre>
</div>
</div>
<p>The genre dataset acts as a mapping table to handle the one-to-many relationship between films and their narrative categories. Since a single movie often fits into multiple classifications (e.g., a film that is both “Action” and “Sci-Fi”), this table lists each genre tag on a separate row. This structure allows for precise filtering and aggregation, facilitating analysis of how genre popularity—like the decline of Westerns or the rise of Superhero films—has shifted over the 50-year period.</p>
<div id="9bcdc8ce" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>genre <span class="op">=</span> pl.read_csv(<span class="st">"data/movies_50_years_genre.csv"</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>genre.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 11887
Columns: 3
$ year  &lt;i64&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970
$ title &lt;str&gt; 'Love Story', 'Love Story', 'Airport', 'Airport', 'Airport', 'MASH', 'MASH', 'MASH', 'Patton', 'Patton'
$ genre &lt;str&gt; 'Drama', 'Romance', 'Action', 'Drama', 'Thriller', 'Comedy', 'Drama', 'War', 'Biography', 'Drama'
</code></pre>
</div>
</div>
<p>The people dataset details the key creative talent behind each film, listing the director and the top four billed actors (“starring”) ranked by prominence. Beyond simply naming the individuals, this table enriches the data with demographic inference: it includes predicted gender classifications and a confidence score (gender_conf) for each name. These predictions are derived from U.S. Social Security name data, allowing for longitudinal studies of gender representation in top-tier Hollywood productions.</p>
<div id="8e9edf7f" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>people <span class="op">=</span> pl.read_csv(<span class="st">"data/movies_50_years_people.csv"</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>people.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 24648
Columns: 7
$ year        &lt;i64&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970
$ title       &lt;str&gt; 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Love Story', 'Airport', 'Airport', 'Airport', 'Airport', 'Airport'
$ role        &lt;str&gt; 'director', 'starring', 'starring', 'starring', 'starring', 'director', 'director', 'starring', 'starring', 'starring'
$ rank        &lt;i64&gt; 1, 1, 2, 3, 4, 1, 2, 1, 2, 3
$ person      &lt;str&gt; 'Arthur Hiller', 'Ali MacGraw', "Ryan O'Neal", 'John Marley', 'Ray Milland', 'George Seaton', 'Henry Hathaway', 'Burt Lancaster', 'Dean Martin', 'George Kennedy'
$ gender      &lt;str&gt; 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male'
$ gender_conf &lt;f64&gt; 0.9937, 0.6877, 0.9768, 0.9961, 0.984, 0.9932, 0.9935, 1.0, 0.9875, 0.9932
</code></pre>
</div>
</div>
</section>
<section id="rva-flights" class="level2" data-number="22.7">
<h2 data-number="22.7" class="anchored" data-anchor-id="rva-flights"><span class="header-section-number">22.7</span> RVA Flights</h2>
<p>Sourced from the U.S. Bureau of Transportation, these five datasets provide a comprehensive record of commercial aviation activity departing from Richmond International Airport (RIC) during its record-breaking year of 2019.</p>
<p>The rva dataset is the central fact table, containing 24,808 rows that represent the complete set of commercial departures from Richmond for the year. It captures the pulse of daily operations, logging scheduling data (planned vs.&nbsp;actual departure/arrival times), delays, and routing information (origin to dest). It also serves as the connector for the other tables, linking to them via keys like carrier, tailnum, and time_hour.</p>
<div id="19444740" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>rva <span class="op">=</span> pl.read_csv(<span class="st">"data/flightsrva_flights.csv.gz"</span>, null_values<span class="op">=</span>[<span class="st">"NA"</span>])</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>rva.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 24808
Columns: 19
$ year           &lt;i64&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019
$ month          &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
$ day            &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
$ dep_time       &lt;i64&gt; 548, 552, 558, 630, 639, 641, 648, 655, 717, 734
$ sched_dep_time &lt;i64&gt; 550, 600, 600, 630, 645, 645, 654, 700, 725, 730
$ dep_delay      &lt;i64&gt; -2, -8, -2, 0, -6, -4, -6, -5, -8, 4
$ arr_time       &lt;i64&gt; 728, 814, 817, 713, 748, 827, 910, 933, 828, 903
$ sched_arr_time &lt;i64&gt; 740, 824, 810, 729, 824, 847, 924, 945, 855, 850
$ arr_delay      &lt;i64&gt; -12, -10, 7, -16, -36, -20, -14, -12, -27, 13
$ carrier        &lt;str&gt; 'WN', 'B6', 'YX', 'YV', 'AA', 'DL', 'YX', 'AA', '9E', 'UA'
$ flight         &lt;i64&gt; 25, 33, 135, 145, 58, 28, 117, 62, 79, 29
$ tailnum        &lt;str&gt; 'N485WN', 'N624JB', 'N818MD', 'N88327', 'N680AW', 'N960DN', 'N443YX', 'N930AU', 'N8943A', 'N29717'
$ origin         &lt;str&gt; 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC'
$ dest           &lt;str&gt; 'ATL', 'FLL', 'MSP', 'IAD', 'CLT', 'ATL', 'MIA', 'DFW', 'JFK', 'ORD'
$ air_time       &lt;i64&gt; 85, 115, 161, 26, 52, 85, 119, 194, 54, 121
$ distance       &lt;i64&gt; 481, 805, 970, 100, 257, 481, 825, 1158, 288, 642
$ hour           &lt;i64&gt; 5, 6, 6, 6, 6, 6, 6, 7, 7, 7
$ minute         &lt;i64&gt; 50, 0, 0, 30, 45, 45, 54, 0, 25, 30
$ time_hour      &lt;str&gt; '2019-01-01T05:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T07:00:00Z', '2019-01-01T07:00:00Z', '2019-01-01T07:00:00Z'
</code></pre>
</div>
</div>
<p>The weather dataset offers an hourly meteorological log for the airport, containing 8,735 observations. It tracks environmental conditions—such as wind speed, visibility, and humidity—that are critical for analyzing flight delays. The time_hour column allows this data to be precisely joined with flight departures to assess the impact of weather on airport performance.</p>
<div id="136572c7" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>weather <span class="op">=</span> pl.read_csv(<span class="st">"data/flightsrva_weather.csv.gz"</span>, null_values<span class="op">=</span>[<span class="st">"NA"</span>])</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>weather.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 8735
Columns: 15
$ origin     &lt;str&gt; 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC', 'RIC'
$ year       &lt;i64&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019
$ month      &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
$ day        &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
$ hour       &lt;i64&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9
$ temp       &lt;str&gt; null, null, null, null, null, null, null, null, null, null
$ dewp       &lt;str&gt; null, null, null, null, null, null, null, null, null, null
$ humid      &lt;str&gt; null, null, null, null, null, null, null, null, null, null
$ wind_dir   &lt;i64&gt; 180, 180, 180, 180, 190, 200, 200, 210, 220, 210
$ wind_speed &lt;f64&gt; 8.05546, 12.658579999999999, 13.809359999999998, 13.809359999999998, 12.658579999999999, 19.56326, 17.261699999999998, 18.41248, 16.11092, 16.11092
$ wind_gust  &lt;f64&gt; 9.2700622588, 14.567240692399997, 15.891535300799996, 15.891535300799996, 14.567240692399997, 22.5130083428, 19.864419125999994, 21.188713734399997, 18.5401245176, 18.5401245176
$ precip     &lt;f64&gt; null, null, null, null, null, null, null, null, null, null
$ pressure   &lt;str&gt; null, null, null, null, null, null, null, null, null, null
$ visib      &lt;f64&gt; 1.0, 1.5, 1.5, 4.0, 9.0, 10.0, 10.0, 10.0, 10.0, 10.0
$ time_hour  &lt;str&gt; '2019-01-01T05:00:00Z', '2019-01-01T06:00:00Z', '2019-01-01T07:00:00Z', '2019-01-01T08:00:00Z', '2019-01-01T09:00:00Z', '2019-01-01T10:00:00Z', '2019-01-01T11:00:00Z', '2019-01-01T12:00:00Z', '2019-01-01T13:00:00Z', '2019-01-01T14:00:00Z'
</code></pre>
</div>
</div>
<p>The airport dataset acts as a geospatial lookup table for the 1,251 US destinations accessible from Richmond. It maps three-letter FAA codes (like “ATL” or “ORD”) to their full names, time zones, and exact latitude/longitude coordinates, enabling the mapping of flight paths and the calculation of distance-based metrics.</p>
<div id="8f3b4be9" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>airport <span class="op">=</span> pl.read_csv(<span class="st">"data/flightsrva_airports.csv.gz"</span>, null_values<span class="op">=</span>[<span class="st">"NA"</span>])</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>airport.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1251
Columns: 8
$ faa   &lt;str&gt; 'AAF', 'AAP', 'ABE', 'ABI', 'ABL', 'ABQ', 'ABR', 'ABY', 'ACK', 'ACT'
$ name  &lt;str&gt; 'Apalachicola Regional Airport', 'Andrau Airpark', 'Lehigh Valley International Airport', 'Abilene Regional Airport', 'Ambler Airport', 'Albuquerque International Sunport', 'Aberdeen Regional Airport', 'Southwest Georgia Regional Airport', 'Nantucket Memorial Airport', 'Waco Regional Airport'
$ lat   &lt;f64&gt; 29.72750092, 29.7224998474, 40.652099609375, 32.4113006592, 67.1063, 35.040199, 45.449100494384766, 31.535499572753903, 41.25310135, 31.611299514770508
$ lon   &lt;f64&gt; -85.02749634, -95.5883026123, -75.44080352783203, -99.6819000244, -157.856989, -106.609001, -98.42179870605467, -84.19450378417969, -70.06020355, -97.23049926757812
$ alt   &lt;i64&gt; 20, 79, 393, 1791, 334, 5355, 1302, 197, 47, 516
$ tz    &lt;i64&gt; -5, -6, -5, -6, -9, -7, -6, -5, -5, -6
$ dst   &lt;str&gt; 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A'
$ tzone &lt;str&gt; 'America/New_York', 'America/Chicago', 'America/New_York', 'America/Chicago', 'America/Anchorage', 'America/Denver', 'America/Chicago', 'America/New_York', 'America/New_York', 'America/Chicago'
</code></pre>
</div>
</div>
<p>The airline dataset is a small reference table linking the 14 unique two-letter carrier codes found in the flight logs (e.g., “AA”, “DL”) to their full corporate names (e.g., “American Airlines Inc.”, “Delta Air Lines Inc.”).</p>
<div id="f8984816" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>airline <span class="op">=</span> pl.read_csv(<span class="st">"data/flightsrva_airlines.csv.gz"</span>, null_values<span class="op">=</span>[<span class="st">"NA"</span>])</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>airline.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 14
Columns: 2
$ carrier &lt;str&gt; '9E', 'AA', 'B6', 'DL', 'EV', 'G4', 'MQ', 'NK', 'OH', 'OO'
$ name    &lt;str&gt; 'Endeavor Air Inc.', 'American Airlines Inc.', 'JetBlue Airways', 'Delta Air Lines Inc.', 'ExpressJet Airlines LLC d/b/a aha!', 'Allegiant Air', 'Envoy Air', 'Spirit Air Lines', 'PSA Airlines Inc.', 'SkyWest Airlines Inc.'
</code></pre>
</div>
</div>
<p>The plane dataset provides a technical registry for the aircraft used in these flights. Indexed by unique tail numbers, it details the hardware specifications for 3,120 individual planes, including their manufacturer, model year, engine type, and seating capacity, allowing for analysis of fleet modernization and equipment usage.</p>
<div id="335dcf11" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>plane <span class="op">=</span> pl.read_csv(<span class="st">"data/flightsrva_planes.csv.gz"</span>, null_values<span class="op">=</span>[<span class="st">"NA"</span>])</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>plane.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 3120
Columns: 9
$ tailnum      &lt;str&gt; 'N101HQ', 'N102HQ', 'N102UW', 'N103HQ', 'N103SY', 'N103US', 'N104HQ', 'N104UW', 'N10575', 'N105HQ'
$ year         &lt;i64&gt; 2007, 2007, 1998, 2007, 2014, 1999, 2007, 1999, 2002, 2007
$ type         &lt;str&gt; 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine', 'Fixed wing multi engine'
$ manufacturer &lt;str&gt; 'EMBRAER-EMPRESA BRASILEIRA DE', 'EMBRAER-EMPRESA BRASILEIRA DE', 'AIRBUS INDUSTRIE', 'EMBRAER-EMPRESA BRASILEIRA DE', 'EMBRAER S A', 'AIRBUS INDUSTRIE', 'EMBRAER-EMPRESA BRASILEIRA DE', 'AIRBUS INDUSTRIE', 'EMBRAER', 'EMBRAER'
$ model        &lt;str&gt; 'ERJ 170-200 LR', 'ERJ 170-200 LR', 'A320-214', 'ERJ 170-200 LR', 'ERJ 170-200 LR', 'A320-214', 'ERJ 170-200 LR', 'A320-214', 'EMB-145LR', 'ERJ 170-200 LR'
$ engines      &lt;i64&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2
$ seats        &lt;i64&gt; 80, 80, 182, 80, 88, 182, 80, 182, 55, 88
$ speed        &lt;i64&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
$ engine       &lt;str&gt; 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan', 'Turbo-fan'
</code></pre>
</div>
</div>
</section>
<section id="what-we-eat-in-america" class="level2" data-number="22.8">
<h2 data-number="22.8" class="anchored" data-anchor-id="what-we-eat-in-america"><span class="header-section-number">22.8</span> What We Eat in America</h2>
<p>The <code>wweia</code> dataset serves as a granular log of dietary intake events, containing over 173,000 observations where each row represents a specific food item consumed by a participant. It captures the “what, when, and where” of eating habits: identifying the item via a standard food_code, pinpointing the occasion with temporal markers (time, day_of_week, meal_name), and noting the origin (food_source) and location (at_home). Crucially, this transactional table details the nutritional impact of each specific portion, recording the mass in grams and providing a breakdown of energy (kcal), macronutrients (protein, carbs, fat, sugar), and other constituents like caffeine and alcohol.</p>
<div id="703f7955" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>wweia <span class="op">=</span> pl.read_csv(<span class="st">"data/wweia_food.csv"</span>, ignore_errors<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>wweia.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 173174
Columns: 15
$ id          &lt;i64&gt; 109263, 109263, 109263, 109263, 109263, 109263, 109263, 109263, 109263, 109263
$ food_code   &lt;i64&gt; 28320300, 91746110, 58106210, 64104010, 11710801, 54304020, 57124200, 94000100, 11710801, 94000100
$ day_of_week &lt;i64&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5
$ time        &lt;i64&gt; 19, 18, 12, 16, 19, 14, 16, 8, 8, 14
$ meal_name   &lt;str&gt; 'Dinner', 'Snack', 'Lunch', 'Snack', 'Dinner', 'Snack', 'Snack', 'Extended consumption', 'Breakfast', 'Snack'
$ food_source &lt;str&gt; 'Store - grocery/supermarket', 'Child/Adult care center', 'Child/Adult care center', 'Store - grocery/supermarket', 'Store - grocery/supermarket', 'Child/Adult care center', 'Store - grocery/supermarket', 'NA', 'Store - grocery/supermarket', 'NA'
$ at_home     &lt;str&gt; 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No'
$ grams       &lt;f64&gt; 199.5, 20.0, 238.0, 209.0, 124.0, 10.0, 11.67, 105.0, 130.2, 105.0
$ kcal        &lt;i64&gt; 114, 101, 633, 99, 123, 49, 45, 0, 129, 0
$ protein     &lt;f64&gt; 12.11, 2.2, 27.11, 0.19, 3.55, 1.09, 0.63, 0.0, 3.72, 0.0
$ carbs       &lt;f64&gt; 5.07, 12.03, 79.33, 23.71, 13.83, 5.94, 9.45, 0.0, 14.52, 0.0
$ sugar       &lt;f64&gt; 2.13, 10.4, 8.52, 21.17, 13.02, 0.45, 4.02, 0.0, 13.67, 0.0
$ fat         &lt;f64&gt; 4.95, 5.09, 23.06, 0.52, 5.92, 2.27, 0.58, 0.0, 6.21, 0.0
$ caffeine    &lt;i64&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 0
$ alcohol     &lt;i64&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
</code></pre>
</div>
</div>
<p>The <code>demo</code> dataset provides the socioeconomic and demographic context for the 13,724 survey participants, linked to the food log by a unique id. It constructs a profile for each individual, tracking fundamental attributes such as age, gender, and race, alongside indicators of social status like education level (edu_level) and family structure. Economic wellbeing is quantified by the ratio_to_poverty (the ratio of family income to the federal poverty threshold), allowing researchers to analyze how diet quality varies across different income brackets and population segments.</p>
<div id="c073f49c" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>demo <span class="op">=</span> pl.read_csv(<span class="st">"data/wweia_demo.csv"</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>demo.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 13724
Columns: 8
$ id               &lt;i64&gt; 109263, 109264, 109265, 109266, 109269, 109270, 109271, 109272, 109273, 109274
$ age              &lt;i64&gt; 2, 13, 2, 29, 2, 11, 49, 0, 36, 68
$ gender           &lt;str&gt; 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male'
$ edu_level        &lt;str&gt; 'NA', 'NA', 'NA', '5', 'NA', 'NA', '2', 'NA', '4', '4'
$ race             &lt;str&gt; 'Other', 'Mexican American', 'White', 'Other', 'Other Hispanic', 'Black', 'White', 'Mexican American', 'White', 'Missing'
$ family_status    &lt;str&gt; 'NA', 'NA', 'NA', 'Other', 'NA', 'NA', 'Other', 'NA', 'Other', 'Other'
$ ratio_to_poverty &lt;str&gt; '4.66', '0.83', '3.06', '5', '0.96', '1.88', 'NA', '0.73', '0.83', '1.2'
$ lang_interview   &lt;str&gt; 'English', 'English', 'English', 'English', 'English', 'English', 'English', 'English', 'English', 'English'
</code></pre>
</div>
</div>
<p>The <code>meta</code> dataset acts as the definitive taxonomy for the survey, containing 7,444 entries that map the numeric food_code found in the consumption logs to human-readable definitions. It organizes the vast array of food items into a hierarchical structure, linking specific descriptions (e.g., “Milk, low sodium, whole”) to broader category_descriptions (e.g., “Milk, whole”) and high-level food_group classifications (e.g., “Milk and Dairy”). This reference table is essential for aggregating granular food data into meaningful dietary patterns consistent with nutritional guidelines.</p>
<div id="42af9c02" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>meta <span class="op">=</span> pl.read_csv(<span class="st">"data/wweia_meta.csv"</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>meta.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 7444
Columns: 7
$ food_code             &lt;i64&gt; 11000000, 11100000, 11111000, 11111100, 11111150, 11111160, 11111170, 11112110, 11112120, 11112130
$ food_code_description &lt;str&gt; 'Milk, human', 'Milk, NFS', 'Milk, whole', 'Milk, low sodium, whole', 'Milk, calcium fortified, whole', 'Milk, calcium fortified, low fat (1%)', 'Milk, calcium fortified, fat free (skim)', 'Milk, reduced fat (2%)', 'Milk, acidophilus, low fat (1%)', 'Milk, acidophilus, reduced fat (2%)'
$ category_number       &lt;i64&gt; 9602, 1004, 1002, 1002, 1002, 1006, 1008, 1004, 1006, 1004
$ category_description  &lt;str&gt; 'Human milk', 'Milk, reduced fat', 'Milk, whole', 'Milk, whole', 'Milk, whole', 'Milk, lowfat', 'Milk, nonfat', 'Milk, reduced fat', 'Milk, lowfat', 'Milk, reduced fat'
$ meta_number           &lt;i64&gt; 96, 10, 10, 10, 10, 10, 10, 10, 10, 10
$ meta_name             &lt;str&gt; 'Human Milk', 'Milk', 'Milk', 'Milk', 'Milk', 'Milk', 'Milk', 'Milk', 'Milk', 'Milk'
$ food_group            &lt;str&gt; 'Baby Foods', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy', 'Milk and Dairy'
</code></pre>
</div>
</div>
</section>
<section id="inference-data" class="level2" data-number="22.9">
<h2 data-number="22.9" class="anchored" data-anchor-id="inference-data"><span class="header-section-number">22.9</span> Inference Data</h2>
<p>Derived from the CDC’s 2010 National Survey of Family Growth (NSFG), the <code>marriage</code> dataset is a focused univariate collection regarding family formation trends. It consists of a single column, age, which records the age in years at which 5,534 U.S. women entered into their first marriage. This simple numeric vector serves as a foundational sample for estimating population parameters—such as the median age of first marriage—and analyzing shifts in nuptiality over time.</p>
<div id="0e59e416" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>marriage <span class="op">=</span> pl.read_csv(<span class="st">"data/inference_age_at_mar.csv"</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>marriage.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 5534
Columns: 1
$ age &lt;i64&gt; 32, 25, 24, 26, 32, 29, 23, 23, 29, 27
</code></pre>
</div>
</div>
<p>Originating from a study in rural New South Wales, Australia, the <code>absent</code> dataset investigates the factors influencing school attendance among 146 primary school students. The target variable, days, counts the total days a student was absent during the school year. These figures are contextualized by categorical demographic and academic indicators, including ethnicity (eth), gender (sex), age group (age), and lrn, a classification of the student’s learning status.</p>
<div id="d0a1bc1e" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>absent <span class="op">=</span> pl.read_csv(<span class="st">"data/inference_absenteeism.csv"</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>absent.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 146
Columns: 5
$ eth  &lt;str&gt; 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A'
$ sex  &lt;str&gt; 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M'
$ age  &lt;str&gt; 'F0', 'F0', 'F0', 'F0', 'F0', 'F0', 'F0', 'F0', 'F1', 'F1'
$ lrn  &lt;str&gt; 'SL', 'SL', 'SL', 'AL', 'AL', 'AL', 'AL', 'AL', 'SL', 'SL'
$ days &lt;i64&gt; 2, 11, 14, 5, 5, 13, 20, 22, 6, 6
</code></pre>
</div>
</div>
<p>The <code>sulph</code> dataset captures the results of a clinical control trial testing the efficacy of the drug sulphinpyrazone in post-heart attack care. It tracks 1,475 patients, dividing them into experimental and placebo arms via the group column. The primary endpoint is recorded in the binary outcome column (“lived” or “died”), creating a classic contingency structure used to calculate odds ratios and determine if the drug provides a statistically significant survival benefit compared to the control.</p>
<div id="332735b2" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>sulph <span class="op">=</span> pl.read_csv(<span class="st">"data/inference_sulphinpyrazone.csv"</span>)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>sulph.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1475
Columns: 2
$ group   &lt;str&gt; 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control'
$ outcome &lt;str&gt; 'died', 'died', 'died', 'died', 'died', 'died', 'died', 'died', 'died', 'died'
</code></pre>
</div>
</div>
<p>Sourced from a survey of 1,325 UCLA students, the <code>speed</code> dataset combines physiological metrics with self-reported risk behavior. It logs the student’s sex and height (in inches), alongside a behavioral metric: speed, representing the fastest speed the student has ever driven a vehicle (presumably in mph). This combination allows for inference tasks such as testing for gender-based differences in driving habits or exploring correlations between physical stature and risk-taking.</p>
<div id="341626c1" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>speed <span class="op">=</span> pl.read_csv(<span class="st">"data/inference_speed_sex_height.csv"</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>speed.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1279
Columns: 3
$ speed  &lt;i64&gt; 85, 40, 87, 110, 110, 120, 90, 90, 80, 95
$ sex    &lt;str&gt; 'female', 'male', 'female', 'female', 'male', 'female', 'female', 'female', 'female', 'male'
$ height &lt;f64&gt; 69.0, 71.0, 64.0, 60.0, 70.0, 61.0, 65.0, 65.0, 61.0, 69.0
</code></pre>
</div>
</div>
<p>The <code>possum</code> dataset provides a morphometric profile of 104 brushtail possums captured across Australia and New Guinea. Aside from sex and age estimates, the data tracks geographic provenance through site codes and population regions (pop, e.g., “Vic” for Victoria). The dataset is defined by its precise biological measurements in millimeters—specifically head_l (head length), skull_w (skull width), total_l (total body length), and tail_l (tail length)—which are often used to classify subspecies or study regional physical variations.</p>
<div id="3dd1e17d" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>possum <span class="op">=</span> pl.read_csv(<span class="st">"data/inference_possum.csv"</span>)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>possum.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 104
Columns: 8
$ site    &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
$ pop     &lt;str&gt; 'Vic', 'Vic', 'Vic', 'Vic', 'Vic', 'Vic', 'Vic', 'Vic', 'Vic', 'Vic'
$ sex     &lt;str&gt; 'm', 'f', 'f', 'f', 'f', 'f', 'm', 'f', 'f', 'f'
$ age     &lt;str&gt; '8', '6', '6', '6', '2', '1', '2', '6', '9', '6'
$ head_l  &lt;f64&gt; 94.1, 92.5, 94.0, 93.2, 91.5, 93.1, 95.3, 94.8, 93.4, 91.8
$ skull_w &lt;f64&gt; 60.4, 57.6, 60.0, 57.1, 56.3, 54.8, 58.2, 57.6, 56.3, 58.0
$ total_l &lt;f64&gt; 89.0, 91.5, 95.5, 92.0, 85.5, 90.5, 89.5, 91.0, 91.5, 89.5
$ tail_l  &lt;f64&gt; 36.0, 36.5, 39.0, 38.0, 36.0, 35.5, 36.0, 37.0, 37.0, 37.5
</code></pre>
</div>
</div>
</section>
<section id="keylogging" class="level2" data-number="22.10">
<h2 data-number="22.10" class="anchored" data-anchor-id="keylogging"><span class="header-section-number">22.10</span> Keylogging</h2>
<p>The klog dataset is a high-resolution behavioral log capturing the precise keystroke dynamics of students writing in English. With over 1.1 million observations, each row represents a single key press or input event. The data records the temporal flow of writing through timestamps (t0 for press, t1 for release) and calculated durations (dur), offering insight into motor processing and cognitive hesitation. The input and code columns differentiate between the resulting character (e.g., “I”) and the physical key actuated (e.g., “KeyI” or “Space”), allowing for the reconstruction of the text and the analysis of editing behaviors like backspacing or pausing.</p>
<div id="42c25637" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>klog <span class="op">=</span> pl.read_csv(<span class="st">"data/keylog.csv.gz"</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>klog.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1145051
Columns: 7
$ id        &lt;str&gt; 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP', 'R_00RbUqO7jXLDItP'
$ t0        &lt;f64&gt; 20914.10000000009, 21146.20000000018, 21234.30000000028, 22074.10000000009, 22306.20000000018, 23674.30000000028, 23818.39999999991, 24043.70000000018, 25066.30000000028, 25170.20000000018
$ t1        &lt;f64&gt; 20978.5, 21226.30000000028, 21290.10000000009, 22154.30000000028, 22394.39999999991, 23738.60000000009, 23874.5, 24090.30000000028, 25130.30000000028, 25250.0
$ dur       &lt;f64&gt; 64.3999999999069, 80.1000000000968, 55.7999999998137, 80.2000000001863, 88.1999999997242, 64.2999999998137, 56.1000000000931, 46.6000000000968, 64.0, 79.7999999998174
$ dur_after &lt;f64&gt; 80.1000000000968, 55.7999999998137, 80.2000000001863, 88.1999999997242, 64.2999999998137, 56.1000000000931, 46.6000000000968, 64.0, 79.7999999998174, 72.2999999998174
$ input     &lt;str&gt; 'I', 'f', null, 'I', null, 'c', 'o', 'u', 'l', 'd'
$ code      &lt;str&gt; 'KeyI', 'KeyF', 'Space', 'KeyI', 'Space', 'KeyC', 'KeyO', 'KeyU', 'KeyL', 'KeyD'
</code></pre>
</div>
</div>
<p>The meta dataset provides the demographic and linguistic context for the 823 participants tracked in the keylogs. It links each unique session id to the writer’s age and, crucially, their native language background (lang). The dataset also includes a cefr rating (Common European Framework of Reference for Languages) , which categorizes their English proficiency into standard levels such as “B1/B2” (independent user) or “C1/C2” (proficient user). This metadata enables comparative analysis of how L1 background and L2 proficiency manifest in low-level typing patterns.</p>
<div id="c6944112" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>meta <span class="op">=</span> pl.read_csv(<span class="st">"data/keylog-meta.csv.gz"</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>meta.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 823
Columns: 4
$ id   &lt;str&gt; 'R_2EGIsZARLydD3Uc', 'R_1obCaysaZCWZXoG', 'R_3fqTek829k38iCk', 'R_brxD7Q5ZnPW8Gn7', 'R_1k1RE78cBbZyZMA', 'R_1NwuZMzRkVIR0WT', 'R_2t8LOS9nQDBQPA8', 'R_239Q0X5YLwB7U6Z', 'R_10xbkjEmnsusfb1', 'R_10CbLBzAnYKgWxB'
$ age  &lt;i64&gt; 25, 22, 22, 43, 23, 32, 24, 28, 32, 21
$ lang &lt;str&gt; 'Italian', 'Spanish', 'Polish', 'English', 'Polish', 'English', 'Spanish', 'English', 'Polish', 'Polish'
$ cefr &lt;str&gt; 'C1/C2', 'B1/B2', 'B1/B2', 'C1/C2', 'B1/B2', 'C1/C2', 'C1/C2', 'C1/C2', 'B1/B2', 'B1/B2'
</code></pre>
</div>
</div>
</section>
<section id="paris-metro" class="level2" data-number="22.11">
<h2 data-number="22.11" class="anchored" data-anchor-id="paris-metro"><span class="header-section-number">22.11</span> Paris Metro <img style="height:0.8em; width: auto" src="img/spatial.png"></h2>
<p>The pmetro dataset captures the geospatial layout of the Paris Métro system, containing 371 entries that represent individual station stops or track segments. Each row identifies a station by name and links it to its specific line number and official branding line_color (provided as a hex code). Uniquely, the dataset is structured to facilitate network visualization rather than just point plotting: in addition to the station’s own coordinates (lat, lon), it includes lat_end and lon_end columns. This “start-to-end” structure effectively defines the edges between stations, allowing for the reconstruction of the connected path of each subway line.</p>
<div id="c8327658" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>pmetro <span class="op">=</span> pl.read_csv(<span class="st">"data/paris_metro_stops.csv"</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>pmetro.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 371
Columns: 7
$ name       &lt;str&gt; 'La Defense - Grande Arche', 'Esplanade de la Defense', 'Pont de Neuilly (Avenue de Madrid)', "Les Sablons (Jardin d'acclimatation)", 'Argentine', 'Charles De Gaulle-Etoile', 'George-V', 'Franklin D.Roosevelt', 'Champs-Elysees-Clemenceau', 'Concorde'
$ line       &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
$ line_color &lt;str&gt; '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00', '#ffbe00'
$ lon        &lt;f64&gt; 2.237018056395013, 2.247932435324861, 2.260515077888117, 2.271686721050983, 2.289322589613773, 2.295904906076514, 2.300560451248796, 2.30747079344783, 2.313545549946741, 2.322943412243541
$ lat        &lt;f64&gt; 48.892187076449495, 48.88863121777117, 48.884708201322525, 48.88119152058607, 48.87559404986666, 48.87514981973562, 48.872023809500426, 48.86980822019895, 48.86790534489708, 48.86628580458387
$ lon_end    &lt;str&gt; '2.247932435324861', '2.260515077888117', '2.271686721050983', '2.289322589613773', '2.295904906076514', '2.300560451248796', '2.30747079344783', '2.313545549946741', '2.322943412243541', '2.330129877112861'
$ lat_end    &lt;str&gt; '48.88863121777117', '48.884708201322525', '48.88119152058607', '48.87559404986666', '48.87514981973562', '48.872023809500426', '48.86980822019895', '48.86790534489708', '48.86628580458387', '48.864343778733904'
</code></pre>
</div>
</div>
</section>
<section id="us-city-population" class="level2" data-number="22.12">
<h2 data-number="22.12" class="anchored" data-anchor-id="us-city-population"><span class="header-section-number">22.12</span> US City Population <img style="height:0.8em; width: auto" src="img/spatial.png"></h2>
<p>The us_pop dataset traces the demographic evolution of the United States through a historical record of urban growth. Spanning from the first census in 1790 through 2010, it logs the population (measured in thousands) for distinct cities identified by name and state. The data appears to track modern cities backward in time, showing values of 0.0 for years prior to a city’s founding or incorporation (e.g., Anchorage in 1790). Enriched with geospatial coordinates (lat and lon), this longitudinal collection facilitates the analysis of urbanization patterns, capturing the country’s westward expansion and the explosive growth of metropolitan hubs over two centuries.</p>
<div id="aa5fd48d" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>us_pop <span class="op">=</span> pl.read_csv(<span class="st">"data/us_city_population.csv"</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>us_pop.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 6900
Columns: 6
$ city       &lt;str&gt; 'Anchorage, AK', 'Birmingham, AL', 'Huntsville, AL', 'Mobile, AL', 'Montgomery, AL', 'Little Rock, AR', 'Chandler, AZ', 'Gilbert, AZ', 'Glendale, AZ', 'Mesa, AZ'
$ state      &lt;str&gt; 'AK', 'AL', 'AL', 'AL', 'AL', 'AR', 'AZ', 'AZ', 'AZ', 'AZ'
$ year       &lt;i64&gt; 1790, 1790, 1790, 1790, 1790, 1790, 1790, 1790, 1790, 1790
$ population &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
$ lon        &lt;f64&gt; -149.2743541, -86.799047, -86.5389964, -88.1002261, -86.2685927, -92.358556, -111.8549429, -111.7421907, -112.1899006, -111.7173787
$ lat        &lt;f64&gt; 61.177549, 33.5274441, 34.7842707, 30.668426, 32.3462512, 34.7254318, 33.2828736, 33.3102088, 33.5331113, 33.4019259
</code></pre>
</div>
</div>
</section>
<section id="us-metropolitan-regions" class="level2" data-number="22.13">
<h2 data-number="22.13" class="anchored" data-anchor-id="us-metropolitan-regions"><span class="header-section-number">22.13</span> US Metropolitan Regions <img style="height:0.8em; width: auto" src="img/spatial.png"></h2>
<p>Sourced from the Census Bureau’s American Community Survey, this collection of datasets provides a multi-dimensional view of U.S. demographics and economics, centered on Metropolitan Statistical Areas (CBSAs).</p>
<p>The metro dataset is the primary analytic table, profiling 934 metropolitan areas identified by a unique geoid. It aggregates key socioeconomic indicators, including population size (pop), population density, and the median age of residents. Economic health is captured through median household income and housing metrics like home ownership rates (percent_own) and the median cost of a one-bedroom rental. Geographically, each metro is assigned to a broad census region (quad) and division, and is precisely located via latitude/longitude coordinates. The accompanying metro_geo dataset provides the corresponding polygon geometries for these areas, enabling choropleth mapping and spatial analysis.</p>
<div id="42af2789" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>metro <span class="op">=</span> pl.read_csv(<span class="st">"data/acs_cbsa.csv"</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>metro.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 934
Columns: 13
$ name             &lt;str&gt; 'New York', 'Los Angeles', 'Chicago', 'Dallas', 'Houston', 'Washington', 'Philadelphia', 'Miami', 'Atlanta', 'Boston'
$ geoid            &lt;i64&gt; 35620, 31080, 16980, 19100, 26420, 47900, 37980, 33100, 12060, 14460
$ quad             &lt;str&gt; 'NE', 'W', 'NC', 'S', 'S', 'S', 'NE', 'S', 'S', 'NE'
$ lon              &lt;f64&gt; -74.10105570561859, -118.1487215689162, -87.95881973164443, -96.97050780978928, -95.40157389770467, -77.51307477160977, -75.30263491667849, -80.50630736521515, -84.39956676469873, -71.0999121719376
$ lat              &lt;f64&gt; 40.768770318020096, 34.219405716738684, 41.70060516046628, 32.84947968570761, 29.78708316635632, 38.812483836818316, 39.90521296481641, 26.15536900531196, 33.691787081105744, 42.555193833166065
$ pop              &lt;f64&gt; 20.011812, 13.202558, 9.607711, 7.54334, 7.048954, 6.332069, 6.215222, 6.105897, 6.026734, 4.91203
$ density          &lt;f64&gt; 1051.3064674555676, 1040.6472811000378, 508.62940568377377, 323.1814035995303, 316.5435135006062, 363.73268864646764, 506.06813036177755, 430.1031618191652, 263.27582111238434, 517.8277024367719
$ age_median       &lt;f64&gt; 42.9, 41.6, 41.9, 41.3, 41.0, 42.4, 42.6, 43.9, 41.9, 42.2
$ hh_income_median &lt;i64&gt; 86445, 81652, 78790, 76916, 72551, 111252, 79070, 62870, 75267, 99039
$ percent_own      &lt;f64&gt; 55.3, 51.3, 68.9, 64.1, 65.1, 67.4, 71.1, 60.8, 67.3, 66.4
$ rent_1br_median  &lt;i64&gt; 1430, 1468, 1060, 1106, 997, 1601, 1083, 1230, 1181, 1390
$ rent_perc_income &lt;f64&gt; 31.0, 33.6, 29.0, 29.1, 30.0, 28.8, 30.0, 36.8, 30.3, 29.5
$ division         &lt;str&gt; 'Middle Atlantic', 'Pacific', 'East North Central', 'West South Central', 'West South Central', 'South Atlantic', 'Middle Atlantic', 'South Atlantic', 'South Atlantic', 'New England'
</code></pre>
</div>
</div>
<p>The next several datasets handle the higher-level state geography. The state table serves as a reference for the 50 U.S. states, providing names, abbreviations, and total populations, while state_geo contains their boundaries.</p>
<div id="8213768e" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>state <span class="op">=</span> pl.read_csv(<span class="st">"data/acs_state.csv"</span>)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>state.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 50
Columns: 3
$ state &lt;str&gt; 'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia'
$ abb   &lt;str&gt; 'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA'
$ pop   &lt;f64&gt; 4.997675, 0.735951, 7.079203, 3.006309, 39.455353, 5.723176, 3.60533, 0.981892, 21.339762, 10.625615
</code></pre>
</div>
</div>
<p>The metro_cw (crosswalk) table bridges the two geographic levels, handling the complexity of metropolitan areas that span multiple state lines (e.g., the New York metro area covering parts of NY, NJ, and PA). It uses the prop column to indicate what fraction of a metro area’s footprint or population falls within a specific state.</p>
<div id="d999b6a4" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>metro_cw <span class="op">=</span> pl.read_csv(<span class="st">"data/acs_cbsa_to_state.csv"</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>metro_cw.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 996
Columns: 3
$ geoid &lt;i64&gt; 35620, 35620, 35620, 31080, 16980, 16980, 16980, 19100, 26420, 47900
$ state &lt;str&gt; 'NY', 'NJ', 'PA', 'CA', 'IL', 'IN', 'WI', 'TX', 'TX', 'VA'
$ prop  &lt;f64&gt; 0.6483270833289084, 0.3486404584192905, 0.0030324582518010926, 1.0, 0.9065905299185738, 0.07571270790662321, 0.017696762174803024, 1.0, 1.0, 0.48420377548328675
</code></pre>
</div>
</div>
<p>The transit dataset provides a breakdown of transportation modes, listing the percentage of the population that commutes via car, public transportation, bicycle, or other means, as well as those who work from home.</p>
<div id="567d1942" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>transit <span class="op">=</span> pl.read_csv(<span class="st">"data/acs_cbsa_commute_type.csv"</span>)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>transit.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 939
Columns: 11
$ name                  &lt;str&gt; 'New York', 'Los Angeles', 'Chicago', 'Dallas', 'Houston', 'Washington', 'Philadelphia', 'Miami', 'Atlanta', 'Boston'
$ geoid                 &lt;i64&gt; 35620, 31080, 16980, 19100, 26420, 47900, 37980, 33100, 12060, 14460
$ pop                   &lt;f64&gt; 20.011812, 13.202558, 9.607711, 7.54334, 7.048954, 6.332069, 6.215222, 6.105897, 6.026734, 4.91203
$ car                   &lt;f64&gt; 53.51, 80.13, 74.23, 85.27, 86.49, 69.21, 75.33, 84.17, 81.38, 68.81
$ public_transportation &lt;f64&gt; 27.77, 4.07, 10.03, 1.04, 1.85, 10.12, 7.87, 2.62, 2.39, 10.75
$ taxicab               &lt;f64&gt; 0.78, 0.27, 0.38, 0.11, 0.14, 0.45, 0.25, 0.47, 0.45, 0.32
$ motorcycle            &lt;f64&gt; 0.05, 0.21, 0.05, 0.1, 0.09, 0.11, 0.06, 0.16, 0.09, 0.05
$ bicycle               &lt;f64&gt; 0.71, 0.61, 0.61, 0.14, 0.26, 0.74, 0.6, 0.49, 0.15, 0.94
$ walked                &lt;f64&gt; 5.53, 2.28, 2.78, 1.22, 1.18, 2.88, 3.22, 1.46, 1.17, 5.05
$ other_means           &lt;f64&gt; 1.06, 1.18, 1.01, 0.97, 1.3, 1.09, 0.96, 1.43, 1.14, 1.07
$ worked_from_home      &lt;f64&gt; 10.59, 11.25, 10.91, 11.15, 8.69, 15.41, 11.71, 9.21, 13.22, 13.02
</code></pre>
</div>
</div>
<p>Complementing this, the commute dataset uses a “long” format to capture the distribution of travel times. Instead of a single average, it breaks commute durations into specific time bins (defined by time_min and time_max), with the per column indicating the percentage of commuters falling into each interval.</p>
<div id="929b3fb7" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>commute <span class="op">=</span> pl.read_csv(<span class="st">"data/acs_cbsa_commute_time.csv"</span>)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>commute.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 13146
Columns: 6
$ name     &lt;str&gt; 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York'
$ geoid    &lt;i64&gt; 35620, 35620, 35620, 35620, 35620, 35620, 35620, 35620, 35620, 35620
$ pop      &lt;f64&gt; 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812
$ per      &lt;f64&gt; 3.43, 2.97, 3.32, 7.66, 7.72, 14.4, 10.74, 15.69, 7.97, 9.56
$ time_min &lt;f64&gt; 0.0, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0
$ time_max &lt;f64&gt; 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 10.0
</code></pre>
</div>
</div>
<p>The hh dataset offers a granular look at economic disparity by mapping the full distribution of household income for each metro area. Like the commute data, it is structured in a long format, where each row represents a specific income bracket (bounded by band_min and band_max). The per column quantifies the share of households within that bracket, allowing for a more nuanced analysis of wealth distribution—such as identifying the “middle class” squeeze or poverty rates—than a simple median value could provide.</p>
<div id="a9ac63f8" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>hh <span class="op">=</span> pl.read_csv(<span class="st">"data/acs_cbsa_hh_income.csv"</span>)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>hh.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 15024
Columns: 6
$ name     &lt;str&gt; 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York', 'New York'
$ geoid    &lt;i64&gt; 35620, 35620, 35620, 35620, 35620, 35620, 35620, 35620, 35620, 35620
$ pop      &lt;f64&gt; 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812, 20.011812
$ per      &lt;f64&gt; 5.66, 3.97, 3.27, 3.31, 3.2, 3.12, 2.92, 3.06, 2.66, 5.63
$ band_min &lt;i64&gt; 0, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000
$ band_max &lt;str&gt; '10000', '14999', '19999', '24999', '29999', '34999', '39999', '44999', '49999', '59999'
</code></pre>
</div>
</div>
<p>A geographic file that contains the polygons for each of the US states. There are keys to join to the other structured datasets above.</p>
<div id="57ad0716" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>state_geo <span class="op">=</span> DSGeo.read_file(<span class="st">"data/acs_state.geojson"</span>)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>state_geo.drop(c.geometry).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 50
Columns: 3
$ name &lt;str&gt; 'Maine', 'New Hampshire', 'Delaware', 'South Carolina', 'Nebraska', 'Washington', 'New Mexico', 'South Dakota', 'Texas', 'California'
$ abb  &lt;str&gt; 'ME', 'NH', 'DE', 'SC', 'NE', 'WA', 'NM', 'SD', 'TX', 'CA'
$ fips &lt;str&gt; '23', '33', '10', '45', '31', '53', '35', '46', '48', '06'
</code></pre>
</div>
</div>
<p>And another geographic file that contains the polygons for each of the metro regions. There are keys to join to the other structured datasets above.</p>
<div id="2c6e30cd" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>metro_geo <span class="op">=</span> DSGeo.read_file(<span class="st">"data/acs_cbsa_geo.geojson"</span>)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>metro_geo.drop(c.geometry).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 939
Columns: 3
$ geoid &lt;f64&gt; 35620.0, 31080.0, 16980.0, 19100.0, 26420.0, 47900.0, 37980.0, 33100.0, 12060.0, 14460.0
$ quad  &lt;str&gt; 'NE', 'W', 'NC', 'S', 'S', 'S', 'NE', 'S', 'S', 'NE'
$ pop   &lt;f64&gt; 20.011812, 13.202558, 9.607711, 7.54334, 7.048954, 6.332069, 6.215222, 6.105897, 6.026734, 4.91203
</code></pre>
</div>
</div>
</section>
<section id="covid" class="level2" data-number="22.14">
<h2 data-number="22.14" class="anchored" data-anchor-id="covid"><span class="header-section-number">22.14</span> COVID <img style="height:0.8em; width: auto" src="img/spatial.png"></h2>
<p>The covid dataset is a longitudinal record tracking the daily impact of the COVID-19 pandemic across France’s administrative departments. It uses a time-series structure where each row represents the status of a specific department (departement) on a given date. The metrics capture the strain on the healthcare system and the severity of the outbreak, recording cumulative statistics for deceased patients and recovered cases, as well as real-time snapshots of patients currently hospitalised or in intensive care (reanimation). Additionally, some columns track daily flows, such as new hospital admissions (hospitalised_new), allowing for analysis of infection waves and healthcare capacity over time.</p>
<div id="5a0019e0" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>covid <span class="op">=</span> pl.read_csv(<span class="st">"data/france_departement_covid.csv"</span>)</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>covid.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 19998
Columns: 9
$ date             &lt;str&gt; '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18', '2020-03-18'
$ departement      &lt;str&gt; '01', '02', '03', '04', '05', '06', '07', '08', '09', '10'
$ departement_name &lt;str&gt; 'Ain', 'Aisne', 'Allier', 'Alpes-de-Haute-Provence', 'Hautes-Alpes', 'Alpes-Maritimes', 'Ardèche', 'Ardennes', 'Ariège', 'Aube'
$ deceased         &lt;i64&gt; 0, 9, 0, 0, 0, 2, 0, 0, 0, 0
$ hospitalised     &lt;i64&gt; 2, 0, 0, 3, 8, 25, 0, 0, 1, 5
$ reanimation      &lt;i64&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 0
$ recovered        &lt;i64&gt; 1, 0, 0, 2, 9, 47, 0, 1, 2, 0
$ hospitalised_new &lt;str&gt; 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA'
$ reanimation_new  &lt;str&gt; 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA'
</code></pre>
</div>
</div>
<p>The pop dataset is a concise demographic reference table listing the total resident population for each of the 101 French departments. Indexed by the standard two-character departement code (e.g., “01” for Ain, “75” for Paris), this table serves as a critical normalization tool. It allows researchers to convert raw counts from the COVID-19 or economic datasets into standardized rates (such as cases per 100,000 inhabitants), enabling fair comparisons between densely populated urban areas and rural regions.</p>
<div id="9f4fd674" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>pop <span class="op">=</span> pl.read_csv(<span class="st">"data/france_departement_population.csv"</span>)</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>pop.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 101
Columns: 2
$ departement &lt;str&gt; '01', '02', '03', '04', '05', '06', '07', '08', '09', '10'
$ population  &lt;i64&gt; 643350, 534490, 337988, 163915, 141284, 1083310, 325712, 273579, 153153, 310020
</code></pre>
</div>
</div>
<p>The fr_city dataset focuses on the country’s major urban centers, providing geospatial and demographic details for 58 significant cities. It identifies each city by name and links it to its broader administrative region (admin_name), such as “Ile-de-France” or “Nouvelle-Aquitaine.” The data includes precise lat and lon coordinates for mapping and a population figure that represents the broader urban or metropolitan area (agglomeration) rather than just the municipal limits. This dataset allows for spatial analysis of city-level hubs distinct from the broader departmental data.</p>
<div id="e93b0d60" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>fr_city <span class="op">=</span> pl.read_csv(<span class="st">"data/france_cities.csv"</span>)</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>fr_city.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 58
Columns: 5
$ city       &lt;str&gt; 'Paris', 'Lyon', 'Marseille', 'Lille', 'Nice', 'Toulouse', 'Bordeaux', 'Rouen', 'Strasbourg', 'Nantes'
$ lat        &lt;f64&gt; 48.8667, 45.77, 43.29, 50.65, 43.715, 43.62, 44.85, 49.4304, 48.58, 47.2104
$ lon        &lt;f64&gt; 2.3333, 4.83, 5.375, 3.08, 7.265, 1.4499, -0.595, 1.08, 7.75, -1.59
$ population &lt;i64&gt; 9904000, 1423000, 1400000, 1044000, 927000, 847000, 803000, 532559, 439972, 438537
$ admin_name &lt;str&gt; 'Ile-de-France', 'Auvergne-Rhone-Alpes', "Provence-Alpes-Cote d'Azur", 'Hauts-de-France', "Provence-Alpes-Cote d'Azur", 'Occitanie', 'Nouvelle-Aquitaine', 'Normandie', 'Grand Est', 'Pays de la Loire'
</code></pre>
</div>
</div>
<p>The gdp dataset provides an economic profile of the country at the departmental level. It maps each departement (identified by both code and name) to its Gross Domestic Product (GDP) per capita in Euros. This metric serves as a proxy for regional standard of living and economic productivity, allowing for correlations with health outcomes or infrastructure availability.</p>
<div id="fc3e3d7a" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>gdp <span class="op">=</span> pl.read_csv(<span class="st">"data/france_departement_gdp.csv"</span>)</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>gdp.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 96
Columns: 3
$ departement      &lt;str&gt; '01', '02', '03', '04', '05', '06', '07', '08', '09', '10'
$ departement_name &lt;str&gt; 'Ain', 'Aisne', 'Allier', 'Alpes-de-Haute-Provence', 'Hautes-Alpes', 'Alpes-Maritimes', 'Ardèche', 'Ardennes', 'Ariège', 'Aube'
$ gdp_eur          &lt;i64&gt; 28296, 25556, 27657, 28149, 28672, 38488, 24720, 26816, 23534, 31098
</code></pre>
</div>
</div>
<p>The dep dataset serves as the geospatial backbone for mapping French administrative divisions. It contains the boundaries for the 101 departments, linking the standard departement codes and names to a hidden geometry column (polygons). By joining this spatial file with the covid, pop, or gdp tables, users can visualize data through choropleth maps, revealing geographic patterns such as regional economic clusters or the spatial spread of the pandemic.</p>
<div id="ef53a4a0" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>dep <span class="op">=</span> DSGeo.read_file(<span class="st">"data/france_departement_sml.geojson"</span>)</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>dep.drop(c.geometry).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 101
Columns: 2
$ departement      &lt;str&gt; '01', '02', '03', '04', '05', '06', '07', '08', '09', '10'
$ departement_name &lt;str&gt; 'Ain', 'Aisne', 'Allier', 'Alpes-de-Haute-Provence', 'Hautes-Alpes', 'Alpes-Maritimes', 'Ardèche', 'Ardennes', 'Ariège', 'Aube'
</code></pre>
</div>
</div>
<p>We have a second covid dataset consisting of a granular time-series record tracking the spread of the virus across Italy’s administrative landscape. Unlike national or regional summaries, this table drills down to the provincial level (roughly equivalent to U.S. counties), providing a daily count of total cases for each province and its parent region. With over 68,000 observations, it allows for the analysis of local outbreaks and the specific trajectory of the pandemic within distinct geographic pockets from February 2020 onwards.</p>
<div id="7c0e9a32" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>covid <span class="op">=</span> pl.read_csv(<span class="st">"data/it_province_covid.csv"</span>)</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>covid.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 68694
Columns: 4
$ date     &lt;str&gt; '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24', '2020-02-24'
$ region   &lt;str&gt; 'Abruzzo', 'Abruzzo', 'Abruzzo', 'Abruzzo', 'Basilicata', 'Basilicata', 'Calabria', 'Calabria', 'Calabria', 'Calabria'
$ province &lt;str&gt; "L'Aquila", 'Teramo', 'Pescara', 'Chieti', 'Potenza', 'Matera', 'Cosenza', 'Catanzaro', 'Reggio di Calabria', 'Crotone'
$ cases    &lt;i64&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
</code></pre>
</div>
</div>
<p>The it_city dataset serves as a geospatial reference for 388 distinct Italian urban centers. It identifies cities by name—ranging from major metropolises like Rome and Milan to smaller regional hubs—and provides their precise latitude (lat) and longitude (lon) coordinates. Additionally, the pop column lists the resident population for each city, enabling analysis that correlates population density or urban size with other socioeconomic or health indicators.</p>
<div id="788db09b" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>it_city <span class="op">=</span> pl.read_csv(<span class="st">"data/it_cities.csv"</span>)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>it_city.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 388
Columns: 4
$ city_name &lt;str&gt; 'Rome', 'Milan', 'Naples', 'Turin', 'Palermo', 'Genoa', 'Bologna', 'Florence', 'Catania', 'Bari'
$ lon       &lt;f64&gt; 12.51133, 9.18951, 14.26811, 7.68682, 13.33561, 8.94439, 11.33875, 11.24626, 15.07041, 16.8554
$ lat       &lt;f64&gt; 41.89193, 45.46427, 40.85216, 45.07049, 38.13205, 44.40478, 44.49381, 43.77925, 37.49223, 41.11148
$ pop       &lt;i64&gt; 2318895, 1236837, 959470, 870456, 648260, 580223, 366133, 349296, 290927, 277387
</code></pre>
</div>
</div>
<p>The prov dataset provides the essential spatial geometry required to map the provincial data. It contains the administrative boundaries for Italy’s 107 provinces, identifying each by its standard name (e.g., “Torino”, “Firenze”). By linking this geospatial file with the covid dataset via the province column, users can construct choropleth maps to visualize the spatial distribution and evolution of case counts across the peninsula.</p>
<div id="4aed3d45" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>prov <span class="op">=</span> DSGeo.read_file(<span class="st">"data/it_province.geojson"</span>)</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>prov.drop(c.geometry).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 107
Columns: 1
$ province &lt;str&gt; 'Torino', 'Vercelli', 'Novara', 'Cuneo', 'Asti', 'Alessandria', 'Biella', 'Verbano-Cusio-Ossola', "Valle d'Aosta/Vallée d'Aoste", 'Varese'
</code></pre>
</div>
</div>
</section>
<section id="u.s.-storms" class="level2" data-number="22.15">
<h2 data-number="22.15" class="anchored" data-anchor-id="u.s.-storms"><span class="header-section-number">22.15</span> U.S. Storms <img style="height:0.8em; width: auto" src="img/spatial.png"></h2>
<p>Sourced from NOAA’s National Hurricane Center, these three datasets provide a historical record of North Atlantic tropical cyclones. The storm dataset records the comprehensive trajectory and intensity history of tropical cyclones since 1950. It contains over 25,000 timestamped observations, logging the status of a storm every six hours. The data captures the storm’s precise geographic position (lat, lon), its maximum sustained wind speed (wind) in knots, and its Saffir-Simpson category (ranging from 0 for tropical storms/depressions to 5 for catastrophic hurricanes).</p>
<div id="36f8064c" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>storm <span class="op">=</span> pl.read_csv(<span class="st">"data/storms.csv"</span>)</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>storm.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 25112
Columns: 12
$ year     &lt;i64&gt; 1950, 1950, 1950, 1950, 1950, 1950, 1950, 1950, 1950, 1950
$ month    &lt;i64&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8
$ day      &lt;i64&gt; 12, 12, 12, 12, 13, 13, 13, 13, 14, 14
$ hour     &lt;i64&gt; 0, 6, 12, 18, 0, 6, 12, 18, 0, 6
$ name     &lt;str&gt; 'Able', 'Able', 'Able', 'Able', 'Able', 'Able', 'Able', 'Able', 'Able', 'Able'
$ letter   &lt;str&gt; 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A'
$ doy      &lt;i64&gt; 224, 224, 224, 224, 225, 225, 225, 225, 226, 226
$ lon      &lt;f64&gt; -55.5, -56.3, -57.4, -58.6, -60.0, -61.1, -62.2, -63.2, -63.8, -64.6
$ lat      &lt;f64&gt; 17.1, 17.7, 18.2, 19.0, 20.0, 20.7, 21.3, 22.0, 22.7, 23.1
$ status   &lt;str&gt; 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS'
$ category &lt;i64&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
$ wind     &lt;i64&gt; 35, 40, 45, 50, 50, 50, 55, 55, 60, 60
</code></pre>
</div>
</div>
<p>The gender dataset classifies 262 distinct storm names by gender. It provides the name, the assigned gender (male or female), and a prob score reflecting the confidence of that assignment based on U.S. naming conventions. This table supports behavioral research, such as investigations into whether the gender of a storm’s name psychologically impacts public risk perception and preparedness levels.</p>
<div id="3bcd7b0b" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>gender <span class="op">=</span> pl.read_csv(<span class="st">"data/storm_gender.csv"</span>)</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>gender.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 262
Columns: 3
$ name   &lt;str&gt; 'Abby', 'Able', 'Agnes', 'Alberto', 'Alex', 'Alice', 'Alicia', 'Allen', 'Allison', 'Alma'
$ gender &lt;str&gt; 'female', 'male', 'female', 'male', 'male', 'female', 'female', 'male', 'female', 'female'
$ prob   &lt;f64&gt; 1.0, 1.0, 1.0, 1.0, 0.9634, 1.0, 1.0, 0.9936, 0.9987, 0.9867
</code></pre>
</div>
</div>
<p>The storm_type dataset acts as a lookup table for the meteorological classifications found in the main tracking data. It maps the two-letter status codes (like ‘HU’ or ‘TD’) to their full descriptions (e.g., ‘hurricane’ or ‘tropical depression’), clarifying the specific developmental stage or physical nature of the cyclone at each observation point.</p>
<div id="d32edfd0" class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>storm_type <span class="op">=</span> pl.read_csv(<span class="st">"data/storm_codes.csv"</span>)</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>storm_type.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 9
Columns: 2
$ status      &lt;str&gt; 'TD', 'TS', 'HU', 'EX', 'SD', 'SS', 'LO', 'WV', 'DB'
$ status_name &lt;str&gt; 'tropical depression', 'tropical storm', 'hurricane', 'extratropical cyclone', 'subtropical depression', 'subtropical storm', 'low', 'tropical wave', 'disturbance'
</code></pre>
</div>
</div>
</section>
<section id="shakespeare-plays" class="level2" data-number="22.16">
<h2 data-number="22.16" class="anchored" data-anchor-id="shakespeare-plays"><span class="header-section-number">22.16</span> Shakespeare Plays <img style="height:0.8em; width: auto" src="img/text.png"></h2>
<p>A catalog of Shakespeare’s 37 plays, capturing each work’s identity and publication metadata. This serves as the metadata reference table, with each play assigned a short identifier (like “ham” for Hamlet) that links to all other tables. The data comes from the Folger Shakespeare Library’s scholarly editions.</p>
<div id="d3f13008" class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>play <span class="op">=</span> pl.read_csv(<span class="st">"data/shakespeare_plays.csv"</span>)</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>play.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 37
Columns: 4
$ play_id     &lt;str&gt; 'mnd', 'aww', 'ant', 'ayl', 'cor', 'cym', 'ham', '1h4', '2h4', 'h5'
$ title       &lt;str&gt; 'A Midsummer Night’s Dream', 'All’s Well That Ends Well', 'Antony and Cleopatra', 'As You Like It', 'Coriolanus', 'Cymbeline', 'Hamlet', 'Henry IV, Part I', 'Henry IV, Part II', 'Henry V'
$ author      &lt;str&gt; 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare', 'William Shakespeare'
$ short_title &lt;str&gt; 'MND', 'AWW', 'Ant', 'AYL', 'Cor', 'Cym', 'Ham', '1H4', '2H4', 'H5'
</code></pre>
</div>
</div>
<p>The dramatis personae across all plays—1,126 characters from Hamlet to Puck to Lady Macbeth. Each character is tied to their play and includes role descriptions where available (e.g., “duke of Athens” for Theseus, “father to Hermia” for Egeus). This table enables analysis of character networks, naming patterns, and the social structures Shakespeare constructed.</p>
<div id="33444582" class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>char <span class="op">=</span> pl.read_csv(<span class="st">"data/shakespeare_characters.csv"</span>)</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>char.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1126
Columns: 4
$ character_id     &lt;str&gt; 'Hermia_MND', 'Lysander_MND', 'Helena_MND', 'Demetrius_MND', 'Theseus_MND', 'Hippolyta_MND', 'Egeus_MND', 'Philostrate_MND', 'Bottom_MND', 'Quince_MND'
$ play_id          &lt;str&gt; 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd'
$ name             &lt;str&gt; 'Hermia', 'Lysander', 'Helena', 'Demetrius', 'Theseus', 'Hippolyta', 'Egeus', 'Philostrate', 'Nick Bottom', 'Peter Quince'
$ role_description &lt;str&gt; null, null, null, null, 'duke of Athens', 'queen of the Amazons', 'father to Hermia', 'master of the revels to Theseus', 'weaver', 'carpenter'
</code></pre>
</div>
</div>
<p>The spoken text of the plays, segmented into 80,592 individual lines of verse or prose. Each line is attributed to a specific character and located within the play’s structure (act, scene, line number). This is the core table for literary analysis—studying who speaks, how much, in what style (verse vs.&nbsp;prose), and how dialogue flows through each scene.</p>
<div id="1246178d" class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>line <span class="op">=</span> pl.read_csv(<span class="st">"data/shakespeare_lines.csv.gz"</span>)</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>line.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 80592
Columns: 8
$ line_id      &lt;str&gt; 'mnd_line_000001', 'mnd_line_000002', 'mnd_line_000003', 'mnd_line_000004', 'mnd_line_000005', 'mnd_line_000006', 'mnd_line_000007', 'mnd_line_000008', 'mnd_line_000009', 'mnd_line_000010'
$ play_id      &lt;str&gt; 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd'
$ character_id &lt;str&gt; 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Hippolyta_MND', 'Hippolyta_MND', 'Hippolyta_MND', 'Hippolyta_MND'
$ act          &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
$ scene        &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
$ line_number  &lt;str&gt; '1.1.1', '1.1.2', '1.1.3', '1.1.4', '1.1.5', '1.1.6', '1.1.7', '1.1.8', '1.1.9', '1.1.10'
$ line_type    &lt;str&gt; 'verse', 'verse', 'verse', 'verse', 'verse', 'verse', 'verse', 'verse', 'verse', 'verse'
$ text         &lt;str&gt; 'Now, fair Hippolyta, our nuptial hour', 'Draws on apace. Four happy days bring in', 'Another moon. But, O, methinks how slow', 'This old moon wanes! She lingers my desires', 'Like to a stepdame or a dowager', 'Long withering out a young man’s revenue.', 'Four days will quickly steep themselves in night;', 'Four nights will quickly dream away the time;', 'And then the moon, like to a silver bow', 'New-bent in heaven, shall behold the night'
</code></pre>
</div>
</div>
<p>A granular, linguistically-annotated word table with nearly 600,000 tokens. Each word is linked back to its line, character, and play, and includes its lemma (base form) and part-of-speech tag. This enables computational stylistics: vocabulary richness, word frequency analysis, grammatical patterns, and comparative studies of how different characters or plays use language.</p>
<div id="ce8a958b" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>word <span class="op">=</span> pl.read_csv(<span class="st">"data/shakespeare_words.csv.gz"</span>)</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>word.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 599864
Columns: 8
$ word_id      &lt;str&gt; 'mnd_word_00000001', 'mnd_word_00000002', 'mnd_word_00000003', 'mnd_word_00000004', 'mnd_word_00000005', 'mnd_word_00000006', 'mnd_word_00000007', 'mnd_word_00000008', 'mnd_word_00000009', 'mnd_word_00000010'
$ play_id      &lt;str&gt; 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd', 'mnd'
$ character_id &lt;str&gt; 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND', 'Theseus_MND'
$ line_id      &lt;str&gt; 'mnd_line_000001', 'mnd_line_000001', 'mnd_line_000001', 'mnd_line_000001', 'mnd_line_000001', 'mnd_line_000001', 'mnd_line_000002', 'mnd_line_000002', 'mnd_line_000002', 'mnd_line_000002'
$ word         &lt;str&gt; 'Now', 'fair', 'Hippolyta', 'our', 'nuptial', 'hour', 'Draws', 'on', 'apace', 'Four'
$ lemma        &lt;str&gt; 'now', 'fair', 'Hippolyta', 'our', 'nuptial', 'hour', 'draw', 'on', 'apace', 'four'
$ pos          &lt;str&gt; 'av', 'j', 'n1-nn', 'po', 'j', 'n1', 'vvz', 'acp-p', 'av', 'crd'
$ location     &lt;str&gt; '1.1.1', '1.1.1', '1.1.1', '1.1.1', '1.1.1', '1.1.1', '1.1.2', '1.1.2', '1.1.2', '1.1.2'
</code></pre>
</div>
</div>
<p>There are also versions with other early-modern British playwrights: <code>emed_plays.csv</code>, <code>emed_characters.csv</code>, <code>emed_lines.csv</code> and <code>emed_words.csv</code>.</p>
</section>
<section id="wikipedia-authors" class="level2" data-number="22.17">
<h2 data-number="22.17" class="anchored" data-anchor-id="wikipedia-authors"><span class="header-section-number">22.17</span> Wikipedia Authors <img style="height:0.8em; width: auto" src="img/text.png"></h2>
<p>Sourced from Wikipedia, these six datasets provide a multi-faceted view of 75 prominent British authors, ranging from medieval poets like Chaucer to modern literary figures.</p>
<p>The wiki dataset serves as the biographical master table for the collection. It anchors each author (doc_id) with key vital statistics, including their years of birth and death, assigned literary era (e.g., “Early”, “Sixteenth C”), and gender. To facilitate linking with the other tables, it includes the URL slug for their Wikipedia page (link) and a shortened name (short) for cleaner visualization.</p>
<div id="204fc745" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>wiki <span class="op">=</span> pl.read_csv(<span class="st">"data/wiki_uk_meta.csv.gz"</span>, ignore_errors<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>wiki.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 75
Columns: 7
$ doc_id &lt;str&gt; 'Marie de France', 'Geoffrey Chaucer', 'John Gower', 'William Langland', 'Margery Kempe', 'Thomas Malory', 'Thomas More', 'Edmund Spenser', 'Walter Raleigh', 'Philip Sidney'
$ born   &lt;i64&gt; 1160, 1343, 1330, 1332, 1373, 1405, 1478, 1552, 1552, 1554
$ died   &lt;i64&gt; 1215, 1400, 1408, 1386, 1438, 1471, 1535, 1599, 1618, 1586
$ era    &lt;str&gt; 'Early', 'Early', 'Early', 'Early', 'Early', 'Early', 'Sixteenth C', 'Sixteenth C', 'Sixteenth C', 'Sixteenth C'
$ gender &lt;str&gt; 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male'
$ link   &lt;str&gt; 'Marie_de_France', 'Geoffrey_Chaucer', 'John_Gower', 'William_Langland', 'Margery_Kempe', 'Thomas_Malory', 'Thomas_More', 'Edmund_Spenser', 'Walter_Raleigh', 'Philip_Sidney'
$ short  &lt;str&gt; 'Marie d. F.', 'Chaucer', 'Gower', 'Langland', 'Kempe', 'Malory', 'More', 'Spenser', 'Raleigh', 'Sidney'
</code></pre>
</div>
</div>
<p>The ptext dataset contains the full text of each of the Wikipedia pages for each of the authors.</p>
<div id="88803fed" class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>ptext <span class="op">=</span> pl.read_csv(<span class="st">"data/wiki_uk_authors_text.csv"</span>, ignore_errors<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>ptext.drop(c.text).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 75
Columns: 1
$ doc_id &lt;str&gt; 'Marie de France', 'Geoffrey Chaucer', 'John Gower', 'William Langland', 'Margery Kempe', 'Thomas Malory', 'Thomas More', 'Edmund Spenser', 'Walter Raleigh', 'Philip Sidney'
</code></pre>
</div>
</div>
<p>The anno dataset is a large, token-level linguistic corpus derived from the text of the authors’ Wikipedia biographies. With over 400,000 rows, it breaks down every sentence into individual words (token), providing deep grammatical analysis including the lemma, part-of-speech tags (upos, xpos), and morphological features (feats). It also captures dependency parsing relationships (relation, tid_source), enabling syntactic analysis of how these figures are described in encyclopedic text.</p>
<div id="cd03e15a" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>anno <span class="op">=</span> pl.read_csv(<span class="st">"data/wiki_uk_authors_anno.csv.gz"</span>, ignore_errors<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a>anno.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 407698
Columns: 11
$ doc_id        &lt;str&gt; 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France'
$ sid           &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
$ tid           &lt;i64&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10
$ token         &lt;str&gt; 'Marie', 'de', 'France', 'was', 'a', 'poet', 'possibly', 'born', 'in', 'what'
$ token_with_ws &lt;str&gt; 'Marie ', 'de ', 'France ', 'was ', 'a ', 'poet ', 'possibly ', 'born ', 'in ', 'what '
$ lemma         &lt;str&gt; 'Marie', 'de', 'France', 'be', 'a', 'poet', 'possibly', 'bear', 'in', 'what'
$ upos          &lt;str&gt; 'PROPN', 'PROPN', 'PROPN', 'AUX', 'DET', 'NOUN', 'ADV', 'VERB', 'SCONJ', 'PRON'
$ xpos          &lt;str&gt; 'NNP', 'NNP', 'NNP', 'VBD', 'DT', 'NN', 'RB', 'VBN', 'IN', 'WP'
$ feats         &lt;str&gt; 'Number=Sing', 'Number=Sing', 'Number=Sing', 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin', 'Definite=Ind|PronType=Art', 'Number=Sing', 'NA', 'Tense=Past|VerbForm=Part|Voice=Pass', 'NA', 'PronType=Int'
$ tid_source    &lt;i64&gt; 6, 1, 1, 6, 6, 0, 8, 6, 13, 13
$ relation      &lt;str&gt; 'nsubj', 'flat', 'flat', 'cop', 'det', 'root', 'advmod', 'acl', 'mark', 'nsubj'
</code></pre>
</div>
</div>
<p>The cite dataset maps the direct “knowledge graph” between these authors within Wikipedia. It functions as a directed adjacency list, where each row represents a hyperlink connecting one author’s page (doc_id) to another’s (doc_id2). This structure allows researchers to construct a citation network, revealing which authors are explicitly referenced in the biographies of their peers—for example, showing how Geoffrey Chaucer is linked to later writers like Charles Dickens or William Shakespeare.</p>
<div id="3cd2b86e" class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>cite <span class="op">=</span> pl.read_csv(<span class="st">"data/wiki_uk_citations.csv"</span>, ignore_errors<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>cite.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 377
Columns: 2
$ doc_id  &lt;str&gt; 'Marie de France', 'Geoffrey Chaucer', 'Geoffrey Chaucer', 'Geoffrey Chaucer', 'Geoffrey Chaucer', 'Geoffrey Chaucer', 'Geoffrey Chaucer', 'John Gower', 'John Gower', 'John Gower'
$ doc_id2 &lt;str&gt; 'Geoffrey Chaucer', 'William Langland', 'John Gower', 'John Dryden', 'Philip Sidney', 'Charles Dickens', 'William Shakespeare', 'William Langland', 'Geoffrey Chaucer', 'C. S. Lewis'
</code></pre>
</div>
</div>
<p>The cocite dataset captures latent connections between authors by tracking how often they appear together in other Wikipedia articles. Rather than direct links, it counts the co-occurrences (count) between a primary author (doc_id) and another figure (doc_id_out).</p>
<div id="59fe61ed" class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>cocite <span class="op">=</span> pl.read_csv(<span class="st">"data/wiki_uk_cocitations.csv"</span>, ignore_errors<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>cocite.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 2114
Columns: 3
$ doc_id     &lt;str&gt; 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France'
$ doc_id_out &lt;str&gt; 'Matthew Arnold', 'Oscar Wilde', 'Samuel Pepys', 'T. S. Eliot', 'Thomas Malory', 'W. H. Auden', 'Walter Raleigh', 'William Blake', 'William Langland', 'William Shakespeare'
$ count      &lt;i64&gt; 1, 2, 1, 1, 1, 1, 1, 1, 1, 2
</code></pre>
</div>
</div>
<p>The rev dataset provides a longitudinal history of the Wikipedia articles themselves, logging over 35,000 individual edits. Each row records a specific revision, detailing the user who made the change, the timestamp (datetime), and the resulting file size. It also captures the editor’s comment, offering a window into the community collaboration and content disputes that shape the public narrative around these literary figures.</p>
<div id="baf26d4d" class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>rev <span class="op">=</span> pl.read_csv(<span class="st">"data/wiki_uk_page_revisions.csv"</span>, ignore_errors<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>rev.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 35470
Columns: 5
$ doc_id   &lt;str&gt; 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France'
$ user     &lt;str&gt; 'YurikBot', 'YurikBot', 'Ccarroll', '63.231.20.88', 'ExplicitImplicity', 'ExplicitImplicity', 'Adam keller', 'Adam keller', '209.174.134.1', '74.71.36.72'
$ datetime &lt;str&gt; '2006-07-06T23:24:54Z', '2006-07-24T18:39:08Z', '2006-09-10T13:36:44Z', '2006-09-18T04:59:53Z', '2006-09-23T22:27:20Z', '2006-09-23T22:31:03Z', '2006-10-03T18:18:09Z', '2006-10-03T18:19:32Z', '2006-10-10T18:48:26Z', '2006-10-11T05:29:32Z'
$ size     &lt;i64&gt; 3061, 3085, 3085, 3105, 3104, 3132, 3140, 3132, 3160, 3132
$ comment  &lt;str&gt; 'robot  Adding: [[it:Maria di Francia]]', 'robot  Adding: [[pt:Maria de França]]', null, null, 'i believe it is stupid to translate one word, but not the other. revert if you think otherwise.', null, null, null, null, null
</code></pre>
</div>
</div>
<p>The views dataset tracks the public interest in these authors through daily page view statistics. It records the number of visits (views) to each author’s Wikipedia page for every day in August 2023. This time-series data allows for the analysis of current popularity trends, revealing how historical figures maintain relevance or experience spikes in attention due to news, anniversaries, or cultural events.</p>
<div id="bc21a075" class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>views <span class="op">=</span> pl.read_csv(<span class="st">"data/wiki_uk_page_views.csv"</span>, ignore_errors<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>views.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 4490
Columns: 5
$ doc_id &lt;str&gt; 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France'
$ year   &lt;i64&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023
$ month  &lt;i64&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8
$ day    &lt;i64&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10
$ views  &lt;i64&gt; 121, 138, 138, 129, 104, 139, 107, 109, 127, 117
</code></pre>
</div>
</div>
<p>We also have a dataset ptext_fr, which is a version of the French Wikipedia pages for the same authors. Here are the texts.</p>
<div id="e10672ab" class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>ptext_fr <span class="op">=</span> pl.read_csv(<span class="st">"data/wiki_uk_authors_text_fr.csv"</span>, ignore_errors<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>ptext_fr.drop(c.text).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 73
Columns: 1
$ doc_id &lt;str&gt; 'Marie de France', 'Geoffrey Chaucer', 'John Gower', 'William Langland', 'Margery Kempe', 'Thomas Malory', 'Thomas More', 'Edmund Spenser', 'Walter Raleigh', 'Philip Sidney'
</code></pre>
</div>
</div>
<p>And likewise, anno_fr captures the French-language specific annotations.</p>
<div id="4d89415c" class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>anno_fr <span class="op">=</span> pl.read_csv(<span class="st">"data/wiki_uk_authors_anno_fr.csv.gz"</span>, ignore_errors<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>anno_fr.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 215735
Columns: 11
$ doc_id        &lt;str&gt; 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France', 'Marie de France'
$ sid           &lt;i64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
$ tid           &lt;str&gt; '1', '2', '3', '4', '5', '6', '7', '8', '9', '10-11'
$ token         &lt;str&gt; 'Marie', 'de', 'France', 'est', 'une', 'poétesse', 'de', 'la', 'Renaissance', 'du'
$ token_with_ws &lt;str&gt; 'Marie ', 'de ', 'France ', 'est ', 'une ', 'poétesse ', 'de ', 'la ', 'Renaissance ', 'du '
$ lemma         &lt;str&gt; 'Marie', 'de', 'France', 'être', 'un', 'poétesse', 'de', 'le', 'Renaissance', 'NA'
$ upos          &lt;str&gt; 'NOUN', 'ADP', 'PROPN', 'AUX', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'NA'
$ xpos          &lt;str&gt; 'S', 'E', 'SP', 'V', 'RI', 'S', 'E', 'RD', 'S', 'NA'
$ feats         &lt;str&gt; 'Gender=Fem|Number=Sing', 'NA', 'NA', 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin', 'Definite=Ind|Gender=Fem|Number=Sing|PronType=Art', 'Gender=Fem|Number=Sing', 'NA', 'Definite=Def|Gender=Fem|Number=Sing|PronType=Art', 'Gender=Fem|Number=Sing', 'NA'
$ tid_source    &lt;str&gt; '6', '3', '1', '6', '6', '0', '9', '9', '6', 'NA'
$ relation      &lt;str&gt; 'nsubj', 'case', 'nmod', 'cop', 'det', 'root', 'case', 'det', 'nmod', 'NA'
</code></pre>
</div>
</div>
</section>
<section id="imdb-reviews" class="level2" data-number="22.18">
<h2 data-number="22.18" class="anchored" data-anchor-id="imdb-reviews"><span class="header-section-number">22.18</span> IMDb Reviews <img style="height:0.8em; width: auto" src="img/text.png"></h2>
<p>The imdb5k dataset is a curated, 5,000-observation subset of the classic IMDb movie review corpus, designed for rapid prototyping and lightweight sentiment analysis experiments. Each row represents a single review, categorized by a binary sentiment label (“positive” or “negative”) and assigned to a specific data partition (index, e.g., “train” or “test”). Although hidden from the preview for brevity, the dataset is fully equipped for modern NLP tasks: it includes the raw text of the critique and e5, a pre-computed 1024-dimensional embedding vector.</p>
<div id="38c4890b" class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>imdb5k <span class="op">=</span> pl.read_parquet(<span class="st">"data/imdb5k_pca.parquet"</span>)</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>imdb5k.drop(c.e5, c.text).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 5000
Columns: 3
$ id    &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'
$ label &lt;str&gt; 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive'
$ index &lt;str&gt; 'test', 'test', 'test', 'train', 'test', 'train', 'train', 'test', 'train', 'train'
</code></pre>
</div>
</div>
<p>The imdb dataset represents the complete, 50,000-row standard benchmark for binary sentiment classification. Balanced between positive and negative polarities, it provides the robust volume of data necessary for training deep learning models. Like the smaller version, it contains metadata for experimental reproducibility (id and index). Crucially, it pairs the unstructured content of the reviews (text) with high-fidelity, 1024-dimensional vector representations (e5), enabling advanced machine learning applications—such as semantic search or clustering—straight out of the box.</p>
<div id="2d1908a4" class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>imdb <span class="op">=</span> pl.read_parquet(<span class="st">"data/imdb_pca.parquet"</span>)</span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a>imdb.drop(c.e5, c.text).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 50000
Columns: 3
$ id    &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'
$ label &lt;str&gt; 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative'
$ index &lt;str&gt; 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'
</code></pre>
</div>
</div>
</section>
<section id="ag-news" class="level2" data-number="22.19">
<h2 data-number="22.19" class="anchored" data-anchor-id="ag-news"><span class="header-section-number">22.19</span> AG News <img style="height:0.8em; width: auto" src="img/text.png"></h2>
<p>The agnews dataset is a widely used benchmark for multiclass topic classification, comprising 127,600 news articles harvested from more than 2,000 sources. Unlike the binary sentiment datasets, this corpus challenges models to distinguish between four distinct thematic categories: World, Sports, Business, and Sci/Tech. Each entry is assigned a specific label and partitioned into training or testing sets via the index column. While the preview displays only the metadata, the dataset is fully enriched with the raw article text and pre-computed, 1024-dimensional e5 embeddings, streamlining the workflow for developing and evaluating advanced NLP models.</p>
<div id="71238e1a" class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>agnews <span class="op">=</span> pl.read_parquet(<span class="st">"data/agnews_pca.parquet"</span>)</span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>agnews.drop(c.e5, c.text).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 127600
Columns: 3
$ id    &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'
$ label &lt;str&gt; 'Business', 'Business', 'Business', 'Business', 'Business', 'Business', 'Business', 'Business', 'Business', 'Business'
$ index &lt;str&gt; 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'
</code></pre>
</div>
</div>
</section>
<section id="amazon-reviews" class="level2" data-number="22.20">
<h2 data-number="22.20" class="anchored" data-anchor-id="amazon-reviews"><span class="header-section-number">22.20</span> Amazon Reviews <img style="height:0.8em; width: auto" src="img/text.png"></h2>
<p>The amazon dataset is a specialized subset of the massive Amazon Product Reviews corpus, containing 10,000 observations selected for the task of product category classification. Unlike standard sentiment analysis datasets, the challenge here is to predict the correct product department (e.g., “All Beauty”) based solely on the content of the review. The dataset includes standard metadata columns—a unique id, the target label, and an index assigning rows to training or testing partitions. Like the previous NLP collections, it comes fully prepared with both the raw text and pre-computed, 1024-dimensional e5 embeddings, enabling immediate experimentation with dense vector-based classification models.</p>
<div id="5d29e09a" class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>amazon <span class="op">=</span> pl.read_parquet(<span class="st">"data/amazon_pca.parquet"</span>)</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>amazon.drop(c.e5, c.text).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 10000
Columns: 3
$ id    &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'
$ label &lt;str&gt; 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty', 'All Beauty'
$ index &lt;str&gt; 'train', 'test', 'train', 'train', 'test', 'train', 'train', 'train', 'train', 'train'
</code></pre>
</div>
</div>
</section>
<section id="bbc-headlines" class="level2" data-number="22.21">
<h2 data-number="22.21" class="anchored" data-anchor-id="bbc-headlines"><span class="header-section-number">22.21</span> BBC Headlines <img style="height:0.8em; width: auto" src="img/text.png"></h2>
<p>The bbc dataset is a concise benchmark for multiclass document classification, derived from a collection of 2,225 articles published by BBC News. It organizes content into five distinct thematic categories: business, entertainment, politics, sport, and tech. Each row represents a single article, tagged with its ground-truth label and assigned to a specific partition (index) for training or evaluation. Designed for modern NLP workflows, the dataset comes enriched with both the raw text and pre-computed, 1024-dimensional e5 embeddings, allowing researchers to immediately apply vector-based machine learning techniques like clustering or topic modeling.</p>
<div id="0edf7dc5" class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>bbc <span class="op">=</span> pl.read_parquet(<span class="st">"data/bbc_pca.parquet"</span>)</span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a>bbc.drop(c.e5, c.text).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 2225
Columns: 3
$ id    &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'
$ label &lt;str&gt; 'politics', 'entertainment', 'sport', 'entertainment', 'business', 'tech', 'sport', 'sport', 'tech', 'entertainment'
$ index &lt;str&gt; 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'
</code></pre>
</div>
</div>
</section>
<section id="sentiment-treebank" class="level2" data-number="22.22">
<h2 data-number="22.22" class="anchored" data-anchor-id="sentiment-treebank"><span class="header-section-number">22.22</span> Sentiment Treebank <img style="height:0.8em; width: auto" src="img/text.png"></h2>
<p>The sst dataset contains the Stanford Sentiment Treebank (SST-5), a premier benchmark for fine-grained sentiment analysis. Unlike binary classification tasks, this corpus of 11,855 movie review excerpts challenges models to discern subtle emotional gradations across a five-point scale: “very negative”, “negative”, “neutral”, “positive”, and “very positive”. The dataset includes standard metadata columns for row identification (id) and data partitioning (index). Consistent with the other NLP collections in this series, it is provided with both the raw text and pre-computed, 1024-dimensional e5 embeddings, enabling nuanced research into how vector models capture intensity and neutrality in language.</p>
<div id="001704e8" class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>sst <span class="op">=</span> pl.read_parquet(<span class="st">"data/sst5_pca.parquet"</span>)</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>sst.drop(c.e5, c.text).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 11855
Columns: 3
$ id    &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'
$ label &lt;str&gt; 'very positive', 'negative', 'negative', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'negative', 'very positive'
$ index &lt;str&gt; 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'
</code></pre>
</div>
</div>
</section>
<section id="goemotions" class="level2" data-number="22.23">
<h2 data-number="22.23" class="anchored" data-anchor-id="goemotions"><span class="header-section-number">22.23</span> GoEmotions <img style="height:0.8em; width: auto" src="img/text.png"></h2>
<p>The goemo dataset represents GoEmotions, a large-scale, fine-grained corpus designed to capture the complexity of human emotional expression. Sourced from Reddit comments, this collection of over 54,000 observations moves far beyond simple positive/negative sentiment, categorizing text into a rich taxonomy of 27 distinct emotions—such as “admiration,” “remorse,” “curiosity,” and “confusion”—plus a “neutral” state. The data is structured for multi-label classification, with separate columns indicating the presence or absence of each specific emotion. As with the other NLP datasets in this series, it comes enriched with both the raw text and pre-computed, 1024-dimensional e5 embeddings, facilitating advanced research into the vector-space relationships between subtle emotional concepts.</p>
<div id="904472fd" class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>goemo <span class="op">=</span> pl.read_parquet(<span class="st">"data/goemotions_pca.parquet"</span>)</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>goemo.drop(c.e5, c.text).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 54263
Columns: 30
$ id             &lt;str&gt; 'doc0001', 'doc0002', 'doc0003', 'doc0004', 'doc0005', 'doc0006', 'doc0007', 'doc0008', 'doc0009', 'doc0010'
$ index          &lt;str&gt; 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'
$ admiration     &lt;str&gt; 'no admiration', 'no admiration', 'no admiration', 'no admiration', 'no admiration', 'no admiration', 'no admiration', 'no admiration', 'admiration', 'no admiration'
$ amusement      &lt;str&gt; 'no amusement', 'no amusement', 'no amusement', 'no amusement', 'no amusement', 'no amusement', 'no amusement', 'no amusement', 'no amusement', 'no amusement'
$ anger          &lt;str&gt; 'no anger', 'no anger', 'anger', 'no anger', 'no anger', 'no anger', 'no anger', 'no anger', 'no anger', 'no anger'
$ annoyance      &lt;str&gt; 'no annoyance', 'no annoyance', 'no annoyance', 'no annoyance', 'annoyance', 'no annoyance', 'no annoyance', 'no annoyance', 'no annoyance', 'no annoyance'
$ approval       &lt;str&gt; 'no approval', 'no approval', 'no approval', 'no approval', 'no approval', 'no approval', 'no approval', 'no approval', 'no approval', 'no approval'
$ caring         &lt;str&gt; 'no caring', 'no caring', 'no caring', 'no caring', 'no caring', 'no caring', 'no caring', 'no caring', 'no caring', 'no caring'
$ confusion      &lt;str&gt; 'no confusion', 'no confusion', 'no confusion', 'no confusion', 'no confusion', 'no confusion', 'no confusion', 'no confusion', 'no confusion', 'no confusion'
$ curiosity      &lt;str&gt; 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity', 'no curiosity'
$ desire         &lt;str&gt; 'no desire', 'no desire', 'no desire', 'no desire', 'no desire', 'no desire', 'no desire', 'desire', 'no desire', 'no desire'
$ disappointment &lt;str&gt; 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment', 'no disappointment'
$ disapproval    &lt;str&gt; 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval', 'no disapproval'
$ disgust        &lt;str&gt; 'no disgust', 'no disgust', 'no disgust', 'no disgust', 'no disgust', 'no disgust', 'no disgust', 'no disgust', 'no disgust', 'no disgust'
$ embarrassment  &lt;str&gt; 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment', 'no embarrassment'
$ excitement     &lt;str&gt; 'no excitement', 'no excitement', 'no excitement', 'no excitement', 'no excitement', 'no excitement', 'no excitement', 'no excitement', 'no excitement', 'no excitement'
$ fear           &lt;str&gt; 'no fear', 'no fear', 'no fear', 'fear', 'no fear', 'no fear', 'no fear', 'no fear', 'no fear', 'no fear'
$ gratitude      &lt;str&gt; 'no gratitude', 'no gratitude', 'no gratitude', 'no gratitude', 'no gratitude', 'no gratitude', 'gratitude', 'no gratitude', 'no gratitude', 'no gratitude'
$ grief          &lt;str&gt; 'no grief', 'no grief', 'no grief', 'no grief', 'no grief', 'no grief', 'no grief', 'no grief', 'no grief', 'no grief'
$ joy            &lt;str&gt; 'no joy', 'no joy', 'no joy', 'no joy', 'no joy', 'no joy', 'no joy', 'no joy', 'no joy', 'no joy'
$ love           &lt;str&gt; 'no love', 'no love', 'no love', 'no love', 'no love', 'no love', 'no love', 'no love', 'no love', 'no love'
$ nervousness    &lt;str&gt; 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness', 'no nervousness'
$ optimism       &lt;str&gt; 'no optimism', 'no optimism', 'no optimism', 'no optimism', 'no optimism', 'no optimism', 'no optimism', 'optimism', 'no optimism', 'no optimism'
$ pride          &lt;str&gt; 'no pride', 'no pride', 'no pride', 'no pride', 'no pride', 'no pride', 'no pride', 'no pride', 'no pride', 'no pride'
$ realization    &lt;str&gt; 'no realization', 'no realization', 'no realization', 'no realization', 'no realization', 'no realization', 'no realization', 'no realization', 'no realization', 'no realization'
$ relief         &lt;str&gt; 'no relief', 'no relief', 'no relief', 'no relief', 'no relief', 'no relief', 'no relief', 'no relief', 'no relief', 'no relief'
$ remorse        &lt;str&gt; 'no remorse', 'no remorse', 'no remorse', 'no remorse', 'no remorse', 'no remorse', 'no remorse', 'no remorse', 'no remorse', 'no remorse'
$ sadness        &lt;str&gt; 'no sadness', 'no sadness', 'no sadness', 'no sadness', 'no sadness', 'no sadness', 'no sadness', 'no sadness', 'no sadness', 'no sadness'
$ surprise       &lt;str&gt; 'no surprise', 'no surprise', 'no surprise', 'no surprise', 'no surprise', 'surprise', 'no surprise', 'no surprise', 'no surprise', 'no surprise'
$ neutral        &lt;str&gt; 'neutral', 'neutral', 'no neutral', 'no neutral', 'no neutral', 'no neutral', 'no neutral', 'no neutral', 'no neutral', 'neutral'
</code></pre>
</div>
</div>
</section>
<section id="fsa-owi-color-images" class="level2" data-number="22.24">
<h2 data-number="22.24" class="anchored" data-anchor-id="fsa-owi-color-images"><span class="header-section-number">22.24</span> FSA-OWI Color Images <img style="height:0.8em; width: auto" src="img/vision.png"></h2>
<p>Sourced from the Library of Congress, the fsac dataset contains a sample of 500 color photographs from the Farm Security Administration - Office of War Information (FSA-OWI) collection. This historic government project was originally established to document rural poverty during the Great Depression and later expanded to capture American mobilization for World War II. While the collection is famous for its iconic black-and-white imagery, this dataset highlights the less common, vibrant color work shot on early Kodachrome film. Each row represents a single photograph, accessible via the filepath, and includes rich provenance details such as the photographer (e.g., Russell Lee, Jack Delano) and a descriptive caption. The data is geocoded with city, state, and lat/lon coordinates.</p>
<div id="7f106c66" class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>fsac <span class="op">=</span> pl.read_csv(<span class="st">"data/fsac.csv"</span>)</span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>fsac.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 500
Columns: 11
$ filepath     &lt;str&gt; 'media/fsac/1a35266v.jpg', 'media/fsac/1a34940v.jpg', 'media/fsac/1a34143v.jpg', 'media/fsac/1a35375v.jpg', 'media/fsac/1a34758v.jpg', 'media/fsac/1a34718v.jpg', 'media/fsac/1a34400v.jpg', 'media/fsac/1a34371v.jpg', 'media/fsac/1a34230v.jpg', 'media/fsac/1a34505v.jpg'
$ call_number  &lt;str&gt; 'LC-DIG-fsac-1a35266', 'LC-DIG-fsac-1a34940', 'LC-DIG-fsac-1a34143', 'LC-DIG-fsac-1a35375', 'LC-DIG-fsac-1a34758', 'LC-DIG-fsac-1a34718', 'LC-DIG-fsac-1a34400', 'LC-DIG-fsac-1a34371', 'LC-DIG-fsac-1a34230', 'LC-DIG-fsac-1a34505'
$ photographer &lt;str&gt; 'Alfred T. Palmer', 'Howard R. Hollem', 'Russell Lee', 'Alfred T. Palmer', 'Jack Delano', 'Jack Delano', 'Marion Post Wolcott', 'Marion Post Wolcott', 'Russell Lee', 'John Collier'
$ caption      &lt;str&gt; 'Eight generator units in the generator room of a new addition to TVA&amp;#39;s hydroelectric plant at Wilson Dam, Sheffield vicinity, Ala. Located 260 miles above the mouth of the Tennessee River, the dam has an authorized power installation of 288,000 kw., which can be increased to a possible ultimate of 444,000 kw. The reservoir at the dam adds 377,000 acre-feet of water to controlled storage on the Tennessee River system', 'Metal tubing at the Consolidated Aircraft Corp. plant, Fort Worth, Texas', 'The school at Pie Town, New Mexico is held in the Farm Bureau building, which was constructed by cooperative effort', 'Drilling horizontal stabilizers: operating a hand drill, this woman worker at Vultee-Nashville is shown working on the horizontal stabilizer for a Vultee &amp;quot;Vengeance&amp;quot; dive bomber, Tennessee. The &amp;quot;Vengeance&amp;quot; (A-31) was originally designed for the French. It was later adopted by the R.A.F. and still later by the U.S. Army Air Forces. It is a single-engine, low-wing plane, carrying a crew of two men and having six machine guns of varying calibers', 'Santa Fe R.R. trains going through Cajon Pass in the San Bernardino Mountains, Cajon, Calif. On the right, streamliner &amp;quot;Chief&amp;quot; going west; in the background, on the left, a freight train with a helper engine, going east. Santa Fe trip', 'General view of the city and the Atchison, Topeka, and Santa Fe Railroad, Amarillo, Texas. Santa Fe R.R. trip', 'Houses which have been condemned by the Board of Health but are still occupied by Negro migratory workers, Belle Glade, Fla.', 'Burley tobacco is placed on sticks to wilt after cutting, before it is taken into the barn for drying and curing on the Russell Spears&amp;#39; farm, vicinity of Lexington, Ky.', 'Shasta dam under construction, California', 'Trampas, New Mexico'
$ year         &lt;i64&gt; 1942, 1942, 1940, 1943, 1943, 1943, 1941, 1940, 1942, 1943
$ month        &lt;i64&gt; 6, 10, 10, 2, 3, 3, 1, 9, 6, 1
$ state        &lt;str&gt; 'Alabama', 'Texas', 'New Mexico', 'Tennessee', 'California', 'Texas', 'Florida', 'Kentucky', 'California', 'New Mexico'
$ city         &lt;str&gt; 'Sheffield', 'Fort Worth', 'Pie Town', 'Nashville', 'Cajon', 'Amarillo', 'Belle Glade', 'Lexington', 'Redding', 'Trampas'
$ county       &lt;str&gt; 'Colbert County', 'Tarrant County', 'Catron County', 'Davidson County', 'San County', 'Potter County', 'Palm Beach', 'Fayette County', 'Shasta County', 'Taos County'
$ longitude    &lt;f64&gt; -87.6986407, -97.3208496, -108.1347836, -86.7844432, -116.9625269, -101.8312969, -80.6675577, -84.4777153, -122.3916754, -105.7589053
$ latitude     &lt;f64&gt; 34.7650887, 32.725409, 34.2983884, 36.1658899, 32.7947731, 35.2219971, 26.6845104, 37.9886892, 40.5865396, 36.1311359
</code></pre>
</div>
</div>
</section>
<section id="mnist" class="level2" data-number="22.25">
<h2 data-number="22.25" class="anchored" data-anchor-id="mnist"><span class="header-section-number">22.25</span> MNIST <img style="height:0.8em; width: auto" src="img/vision.png"></h2>
<p>The mnist dataset is a 1,000-sample subset of the classic MNIST database, widely considered the “Hello World” of computer vision. It consists of grayscale images of handwritten digits ranging from 0 to 9. The table links the ground-truth numeric label to the corresponding image file via the filepath column and assigns each observation to a specific data partition (index) for training or testing purposes.</p>
<div id="9c3c93bc" class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>mnist <span class="op">=</span> pl.read_csv(<span class="st">"data/mnist_1000.csv"</span>)</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>mnist.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1000
Columns: 3
$ label    &lt;i64&gt; 3, 9, 9, 8, 8, 5, 9, 7, 3, 2
$ filepath &lt;str&gt; 'media/mnist_1000/00000.png', 'media/mnist_1000/00001.png', 'media/mnist_1000/00002.png', 'media/mnist_1000/00003.png', 'media/mnist_1000/00004.png', 'media/mnist_1000/00005.png', 'media/mnist_1000/00006.png', 'media/mnist_1000/00007.png', 'media/mnist_1000/00008.png', 'media/mnist_1000/00009.png'
$ index    &lt;str&gt; 'test', 'test', 'train', 'test', 'train', 'train', 'train', 'train', 'train', 'train'
</code></pre>
</div>
</div>
<p>The emnist dataset is a 10,000-sample subset of EMNIST (Extended MNIST), a more challenging dataset that expands the original digit recognition task to include handwritten letters. Unlike the standard MNIST, the label column here is alphanumeric, containing a mix of digits and both uppercase and lowercase characters (e.g., ‘g’, ‘P’, ‘4’), reflecting the greater complexity of the classification problem.</p>
<div id="d64e3534" class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb142"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a>emnist <span class="op">=</span> pl.read_csv(<span class="st">"data/emnist_10000.csv"</span>)</span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a>emnist.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 10000
Columns: 3
$ label    &lt;str&gt; 'g', 'b', 'P', '9', 't', '4', 'M', 'b', 'e', 'A'
$ filepath &lt;str&gt; 'media/emnist_10000/00000.png', 'media/emnist_10000/00001.png', 'media/emnist_10000/00002.png', 'media/emnist_10000/00003.png', 'media/emnist_10000/00004.png', 'media/emnist_10000/00005.png', 'media/emnist_10000/00006.png', 'media/emnist_10000/00007.png', 'media/emnist_10000/00008.png', 'media/emnist_10000/00009.png'
$ index    &lt;str&gt; 'train', 'train', 'train', 'train', 'test', 'train', 'train', 'train', 'train', 'train'
</code></pre>
</div>
</div>
<p>The fmnist dataset represents a 10,000-sample subset of Fashion-MNIST, designed by Zalando Research as a modern, more difficult drop-in replacement for the original digit dataset. Instead of abstract numbers, the images depict distinct articles of clothing and accessories. The label column provides the text description of the class—such as “dress,” “pullover,” or “ankle boot”—making it intuitive to interpret model errors (e.g., confusing a “shirt” with a “t-shirt”).</p>
<div id="af21bc94" class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb144"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>fmnist <span class="op">=</span> pl.read_csv(<span class="st">"data/fashionmnist_10000.csv"</span>)</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>fmnist.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 10000
Columns: 3
$ label    &lt;str&gt; 'dress', 'shirt', 'pullover', 'pullover', 'ankle boot', 'pullover', 'trouser', 'ankle boot', 'ankle boot', 'pullover'
$ filepath &lt;str&gt; 'media/fashionmnist_10000/00000.png', 'media/fashionmnist_10000/00001.png', 'media/fashionmnist_10000/00002.png', 'media/fashionmnist_10000/00003.png', 'media/fashionmnist_10000/00004.png', 'media/fashionmnist_10000/00005.png', 'media/fashionmnist_10000/00006.png', 'media/fashionmnist_10000/00007.png', 'media/fashionmnist_10000/00008.png', 'media/fashionmnist_10000/00009.png'
$ index    &lt;str&gt; 'test', 'test', 'train', 'test', 'train', 'train', 'train', 'train', 'train', 'test'
</code></pre>
</div>
</div>
</section>
<section id="imagenet" class="level2" data-number="22.26">
<h2 data-number="22.26" class="anchored" data-anchor-id="imagenet"><span class="header-section-number">22.26</span> ImageNet <img style="height:0.8em; width: auto" src="img/vision.png"></h2>
<p>The inet dataset is a 1,000-sample subset of Imagenette, a corpus derived from the massive ImageNet database but restricted to ten distinct, easily distinguishable classes (e.g., “church,” “gas_pump,” “garbage_truck”). Designed as a lightweight benchmark for rapid prototyping, it links each ground-truth label to its source image via filepath. Crucially, this version comes pre-packaged with two state-of-the-art embedding vectors (vit and siglip), allowing researchers to immediately compare how different vision transformer architectures represent these visually distinct objects without needing to run heavy inference tasks.</p>
<div id="e31fb05d" class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb146"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a>inet <span class="op">=</span> pl.read_parquet(<span class="st">"data/imagenette_1000.parquet"</span>)</span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a>inet.drop(c.vit, c.siglip).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1000
Columns: 3
$ label    &lt;str&gt; 'church', 'tench', 'church', 'gas_pump', 'tench', 'garbage_truck', 'tench', 'gas_pump', 'gas_pump', 'garbage_truck'
$ filepath &lt;str&gt; 'media/imagenette_1000/00000.png', 'media/imagenette_1000/00001.png', 'media/imagenette_1000/00002.png', 'media/imagenette_1000/00003.png', 'media/imagenette_1000/00004.png', 'media/imagenette_1000/00005.png', 'media/imagenette_1000/00006.png', 'media/imagenette_1000/00007.png', 'media/imagenette_1000/00008.png', 'media/imagenette_1000/00009.png'
$ index    &lt;str&gt; 'train', 'train', 'test', 'train', 'train', 'train', 'test', 'test', 'train', 'train'
</code></pre>
</div>
</div>
<p>The woof dataset is a 1,000-sample subset of Imagewoof, a significantly harder classification challenge also drawn from ImageNet. Unlike the broad categories in Imagenette, this dataset focuses exclusively on ten specific dog breeds (e.g., “shih_tzu,” “border_terrier,” “dingo”), forcing models to discern fine-grained features rather than gross structural differences. Like its sibling dataset, it includes standard metadata (label, filepath, index) and is enriched with pre-computed vit and siglip embeddings, making it an ideal testbed for evaluating the discriminative power of modern vision models on subtle, fine-grained tasks.</p>
<div id="7e7f9b59" class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb148"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a>woof <span class="op">=</span> pl.read_parquet(<span class="st">"data/imagewoof_1000.parquet"</span>)</span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a>woof.drop(c.vit, c.siglip).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1000
Columns: 3
$ label    &lt;str&gt; 'shih_tzu', 'border_terrier', 'australian_terrier', 'golden_retriever', 'dingo', 'english_foxhound', 'border_terrier', 'old_english_sheepdog', 'rhodesian_ridgeback', 'rhodesian_ridgeback'
$ filepath &lt;str&gt; 'media/imagewoof_1000/00000.png', 'media/imagewoof_1000/00001.png', 'media/imagewoof_1000/00002.png', 'media/imagewoof_1000/00003.png', 'media/imagewoof_1000/00004.png', 'media/imagewoof_1000/00005.png', 'media/imagewoof_1000/00006.png', 'media/imagewoof_1000/00007.png', 'media/imagewoof_1000/00008.png', 'media/imagewoof_1000/00009.png'
$ index    &lt;str&gt; 'test', 'train', 'train', 'test', 'test', 'test', 'train', 'train', 'train', 'train'
</code></pre>
</div>
</div>
</section>
<section id="oxford-flowers" class="level2" data-number="22.27">
<h2 data-number="22.27" class="anchored" data-anchor-id="oxford-flowers"><span class="header-section-number">22.27</span> Oxford Flowers <img style="height:0.8em; width: auto" src="img/vision.png"></h2>
<p>The flowers dataset is a 1,000-sample subset of the Oxford Flowers dataset, a standard benchmark for fine-grained image classification. Unlike general object recognition, this dataset focuses on distinguishing between closely related floral species, with label entries such as “foxglove,” “clematis,” and “bishop of llandaff.” Each row links the specific flower class to its image via filepath and assigns it to a training or testing partition (index). Consistent with the previous vision datasets, it is enriched with pre-computed vit and siglip embedding vectors, enabling researchers to immediately apply clustering or similarity search to explore how different models encode botanical features.</p>
<div id="c2ba84ec" class="cell" data-execution_count="103">
<div class="sourceCode cell-code" id="cb150"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a>flowers <span class="op">=</span> pl.read_parquet(<span class="st">"data/flowers_1000.parquet"</span>)</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a>flowers.drop(c.vit, c.siglip).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1000
Columns: 3
$ label    &lt;str&gt; 'foxglove', 'clematis', 'tree mallow', 'bishop of llandaff', 'daffodil', 'hippeastrum', 'bougainvillea', 'rose', 'bougainvillea', 'azalea'
$ filepath &lt;str&gt; 'media/flowers_1000/00000.png', 'media/flowers_1000/00001.png', 'media/flowers_1000/00002.png', 'media/flowers_1000/00003.png', 'media/flowers_1000/00004.png', 'media/flowers_1000/00005.png', 'media/flowers_1000/00006.png', 'media/flowers_1000/00007.png', 'media/flowers_1000/00008.png', 'media/flowers_1000/00009.png'
$ index    &lt;str&gt; 'test', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'
</code></pre>
</div>
</div>
</section>
<section id="caltech-ucsd-birds" class="level2" data-number="22.28">
<h2 data-number="22.28" class="anchored" data-anchor-id="caltech-ucsd-birds"><span class="header-section-number">22.28</span> Caltech-UCSD Birds <img style="height:0.8em; width: auto" src="img/vision.png"></h2>
<p>The birds dataset is a curated subset of the Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset, a renowned benchmark for fine-grained visual categorization. While the full collection covers 200 species, this version (birds10) likely restricts the task to 10 distinct classes (e.g., “canary”), making it more accessible for rapid model testing. Each entry links the species label to its image filepath and assigns it to a data partition (index). Like the other classification datasets in this series, it comes ready-to-use with pre-computed vit and siglip embeddings, allowing for immediate exploration of how transformer models distinguish between subtle avian features.</p>
<div id="c10d7d98" class="cell" data-execution_count="105">
<div class="sourceCode cell-code" id="cb152"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a>birds <span class="op">=</span> pl.read_parquet(<span class="st">"data/birds10.parquet"</span>)</span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>birds.drop(c.vit, c.siglip).glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1555
Columns: 3
$ label    &lt;str&gt; 'canary', 'canary', 'canary', 'canary', 'canary', 'canary', 'canary', 'canary', 'canary', 'canary'
$ filepath &lt;str&gt; 'media/birds10/00000.png', 'media/birds10/00001.png', 'media/birds10/00002.png', 'media/birds10/00003.png', 'media/birds10/00004.png', 'media/birds10/00005.png', 'media/birds10/00006.png', 'media/birds10/00007.png', 'media/birds10/00008.png', 'media/birds10/00009.png'
$ index    &lt;str&gt; 'test', 'train', 'train', 'train', 'train', 'test', 'train', 'train', 'train', 'test'
</code></pre>
</div>
</div>
<p>The birds_bbox dataset complements the classification data by focusing on object detection and localization. Also derived from the CUB-200-2011 collection, it contains 1,000 observations where the primary task is not just naming the bird, but locating it within the frame. In addition to the label and filepath, this dataset provides precise coordinates for a bounding box: bbox_x0 and bbox_y0 define the top-left corner, while bbox_x1 and bbox_y1 mark the bottom-right. This granular spatial data is essential for training models to separate the subject from complex natural backgrounds.</p>
<div id="87519a0b" class="cell" data-execution_count="106">
<div class="sourceCode cell-code" id="cb154"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a>birds_bbox <span class="op">=</span> pl.read_csv(<span class="st">"data/birds_1000.csv"</span>)</span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a>birds_bbox.glimpse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1000
Columns: 7
$ label    &lt;str&gt; 'Gray_Catbird', 'Sayornis', 'Tennessee_Warbler', 'White_throated_Sparrow', 'Ring_billed_Gull', 'Tree_Swallow', 'Florida_Jay', 'Yellow_breasted_Chat', 'Rusty_Blackbird', 'House_Sparrow'
$ filepath &lt;str&gt; 'media/birds_1000/00000.png', 'media/birds_1000/00001.png', 'media/birds_1000/00002.png', 'media/birds_1000/00003.png', 'media/birds_1000/00004.png', 'media/birds_1000/00005.png', 'media/birds_1000/00006.png', 'media/birds_1000/00007.png', 'media/birds_1000/00008.png', 'media/birds_1000/00009.png'
$ bbox_x0  &lt;f64&gt; 15.0, 131.0, 40.0, 99.0, 104.0, 165.0, 147.0, 99.0, 86.0, 137.0
$ bbox_y0  &lt;f64&gt; 44.0, 85.0, 5.0, 42.0, 32.0, 133.0, 89.0, 60.0, 148.0, 77.0
$ bbox_x1  &lt;f64&gt; 480.0, 488.0, 345.0, 448.0, 451.0, 423.0, 360.0, 377.0, 410.0, 365.0
$ bbox_y1  &lt;f64&gt; 331.0, 326.0, 239.0, 344.0, 284.0, 286.0, 235.0, 353.0, 300.0, 277.0
$ index    &lt;str&gt; 'train', 'train', 'test', 'train', 'train', 'train', 'test', 'train', 'train', 'train'
</code></pre>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/taylor-arnold\.github\.io\/fds-py");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./21_notes.html" class="pagination-link" aria-label="Notes">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Notes</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Taylor Arnold</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>A <a href="https://distantviewing.org/">Distant Viewing Lab</a> project</p>
</div>
  </div>
</footer>




</body></html>