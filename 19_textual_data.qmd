# Textual Data {#sec-text}

```{python}
#| include: false
import warnings
warnings.filterwarnings('ignore')
```

::: {.callout-tip collapse="true"}
## Practice Notebooks

- Notebook19a [[Colab↗](https://colab.research.google.com/github/taylor-arnold/fds-py-nb/blob/main/nb/notebook19a.ipynb?hl=en)]
- Notebook19b [[Colab↗](https://colab.research.google.com/github/taylor-arnold/fds-py-nb/blob/main/nb/notebook19b.ipynb?hl=en)]

:::

## Setup

Load all of the modules and datasets needed for the chapter. We also load the spacy module designed specifically for processing large collections of text.

```{python}
#| output: false
import numpy as np
import polars as pl

from funs import *
from plotnine import *
from polars import col as c
theme_set(theme_minimal())

import spacy

meta = pl.read_csv("data/wiki_uk_meta.csv.gz")
docs = pl.read_csv("data/wiki_uk_authors_text.csv")
docs_fr = pl.read_csv("data/wiki_uk_authors_text_fr.csv")
```

## Introduction

Every dataset we have encountered so far in this book has consisted of structured observations: rows with well-defined columns containing numbers, categories, or dates. But an enormous amount of the world's information exists as unstructured text. Medical records contain free-text physician notes alongside coded diagnoses. Customer feedback arrives as open-ended survey responses. Historical archives preserve centuries of human thought in letters, newspapers, and books. Social scientists study political speeches, journalists analyze leaked documents, and humanists trace the evolution of literary style across generations.

Working with textual data requires a fundamentally different approach than working with numbers. A sentence is not just a sequence of characters but a structured object with grammar, meaning, and context. The word "bank" means something different in "river bank" than in "bank account." The phrase "not bad" typically means something positive despite containing a negation. These subtleties make text both rich and challenging to analyze computationally.

In this chapter, we introduce tools for transforming unstructured text into structured data that can be analyzed using the techniques developed throughout this book. The key insight is that once we have converted text into tables of tokens, counts, and annotations, we can apply familiar operations like filtering, grouping, and joining to answer questions about language. What words distinguish one author from another? Which terms best summarize a document? How does writing style vary across languages or time periods?

We will use spaCy, a modern natural language processing library, to handle the linguistic heavy lifting. SpaCy provides pre-trained models that can tokenize text, identify parts of speech, recognize named entities, and parse grammatical structure. Our job is to take the output of these models and reshape it into forms suitable for analysis.

## NLP Pipeline

We load spaCy's English language model, which contains statistical models trained on large corpora of English text. The "sm" suffix indicates the small model, which balances accuracy with speed and memory usage. Larger models are available for applications requiring higher precision.

```{python}
nlp = spacy.load("en_core_web_sm")
```

Our primary dataset consists of Wikipedia pages for authors from the United Kingdom. We have a metadata table containing information about each author.

```{python}
meta
```

As well as a seperate file giving each of the texts from the documents.

```{python}
docs
```

The metadata table contains structured information extracted from Wikipedia's infoboxes: birth and death dates, occupations, and other biographical details. The text table contains the prose content of each page, which we will process using natural language techniques.

Natural language processing transforms raw text into structured annotations. The `DSText.process` method sends each document through spaCy's processing pipeline, which performs several analyses in sequence: tokenization (splitting text into words and punctuation), part-of-speech tagging (identifying nouns, verbs, adjectives), lemmatization (reducing words to their base forms), named entity recognition (identifying people, places, organizations), and dependency parsing (analyzing grammatical relationships).

```{python}
anno = DSText.process(docs, nlp)
anno
```

There is a lot of information that has been automatically added to this table, thanks to the collective results of decades of research in computational linguistics and natural language processing. Each row corresponds to a word or a punctuation mark (created by the process of tokenization), along with metadata describing the token. Notice that reading down the column `token` reproduces the original text. The columns available are:

- **doc_id**: A key that allows us to group tokens into documents and to link back into the original input table.
- **sid**: Numeric identifier of the sentence number.
- **tid**: Numeric identifier of the token within a sentence. The first three columns form a primary key for the table.
- **token**: A character variable containing the detected token, which is either a word or a punctuation mark.
- **token_with_ws**: The token with white space (spaces and new-line characters) added. This is useful if we wanted to re-create the original text from the token table.
- **lemma**: A normalized version of the token. For example, it removes start-of-sentence capitalization, turns all nouns into their singular form, and converts verbs into their infinitive form.
- **upos**: The universal part of speech code, which are parts of speech that can be defined in most spoken languages. These tend to correspond to the parts of speech taught in primary schools, such as "NOUN", "ADJ" (adjective), and "ADV" (adverb).
- **tag**: A fine-grained part of speech code that depends on the specific language (here, English) and models being used.
- **is_alpha**, **is_stop**, **is_punct**: Boolean flags for alphabetic characters, stop words, and punctuation.
- **dep**: The dependency relation label describing how this token relates grammatically to another token.
- **head_idx**: The token index of the word in the sentence that this token is grammatically related to.
- **ent_type**: The named entity type, if this token is part of a recognized entity.

There are many analyses that can be performed on the extracted features that are present in the `anno` table. Fortunately, many of these can be performed by directly using Polars operations covered in the first five chapters of this text, without the need for any new text-specific functions. For example, we can find the most common nouns in the dataset by filtering on the universal part of speech and grouping by lemma with the code below.

```{python}
(
    anno
    .filter(c.upos == "NOUN")
    .group_by(c.lemma)
    .agg(count = pl.len())
    .sort(c.count, descending=True)
    .head(10)
)
```

The most frequent nouns across the set of documents roughly fall into one of two categories. Those such as "year", "life", "death", and "family" are nouns that we would frequently associate with biographical entries for nearly any group of people. Others, such as "poem", "book", "poet", and the somewhat more generic "work", capture the specific objects that authors would produce and therefore would be prominent elements of their respective Wikipedia pages. The fact that these two types of nouns show up at the top of the list helps to verify that both the dataset and the NLP pipeline are working as expected.

We can use a similar technique to learn about the contents of each of the individual documents. Suppose we wanted to know which adjectives are most used on each page. This can be done by a sequence of Polars operations. First, we filter the data by the part of speech and group the rows of the dataset by the document id and lemma. Then, we count the number of rows for each unique combination of document and lemma and arrange the dataset in descending order of count. We can use the `head()` method on grouped data to take the most frequent adjectives within each document:

```{python}
(
    anno
    .filter(c.upos == "ADJ")
    .group_by([c.doc_id, c.lemma])
    .agg(count = pl.len())
    .sort([c.doc_id, c.count], descending=[False, True])
    .group_by(c.doc_id)
    .head(8)
    .group_by(c.doc_id)
    .agg(top_adj = c.lemma.sort().str.join("; "))
)
```

The output shows many connections between adjectives and the authors. Here, the connections again fall roughly into two groups. Some of the adjectives are fairly generic—such as "more", "other", and "many"—and probably say more about the people writing the pages than the subjects of the pages themselves. Other adjectives provide more contextual information about each of the authors. For example, several selected adjectives are key descriptions of an author's work, such as "Victorian" associated with certain authors and "Gothic" with others. While it is good to see expected relationships to demonstrate the data and techniques are functioning properly, it is also valuable when computational techniques highlight the unexpected.

## N-grams and Collocations

So far we have analyzed individual words in isolation. But meaning often emerges from combinations of words. The phrase "New York" refers to a specific city, not something novel and a name. "Machine learning" describes a field of study, not appliances that acquire knowledge. "Poet laureate" is a title, not just any poet who happens to be a laureate. To capture these multi-word expressions, we turn to *n-grams*: contiguous sequences of n tokens.

A unigram is a single token (what we have been working with). A bigram is a pair of adjacent tokens. A trigram is a sequence of three tokens. In general, an n-gram captures local word order, which is lost when we treat documents as unordered collections of words.

Constructing n-grams requires us to look at tokens in context. Polars window functions allow us to access neighboring rows, which we can use to pair each token with the tokens that follow it. The key is to shift the token column to align adjacent words on the same row.

```{python}
bigrams = (
    anno
    .filter(c.is_alpha)
    .with_columns(
        next_token = c.lemma.shift(-1).over([c.doc_id, c.sid]),
        next_is_alpha = c.is_alpha.shift(-1).over([c.doc_id, c.sid])
    )
    .filter(c.next_is_alpha == True)
    .with_columns(
        bigram = c.lemma + " " + c.next_token
    )
)
bigrams.select(c.doc_id, c.sid, c.tid, c.lemma, c.next_token, c.bigram)
```

The `shift(-1)` operation moves each column up by one position within each document and sentence, so that each row contains both the current token and the following token. We filter to keep only cases where both tokens are alphabetic, excluding bigrams that span punctuation or sentence boundaries.

Now we can count bigram frequencies just as we counted unigram frequencies:

```{python}
(
    bigrams
    .group_by(c.bigram)
    .agg(count = pl.len())
    .sort(c.count, descending=True)
    .head(15)
)
```

The most frequent bigrams by raw counts largely consist of functional phrases that appear in many types of text. The phrase "of the" appears often simply because both words are common, not because they form a meaningful unit. To identify true collocations—word pairs that occur together more often than chance would predict—we use *pointwise mutual information* (PMI). PMI compares the observed frequency of a bigram to the frequency we would expect if the two words were independent:

$$ \text{PMI}(w_1, w_2) = \log_2 \frac{P(w_1, w_2)}{P(w_1) \cdot P(w_2)} $$

A high PMI indicates that the words co-occur much more frequently than their individual frequencies would suggest. A PMI of zero means the words are independent. Negative PMI (rare in practice for bigrams that actually occur) would indicate the words avoid each other.

To compute this we need a few intermediate steps. First of all, we can count the number of times each word appears in all of the texts.

```{python}
word_counts = (
    anno
    .filter(c.is_alpha)
    .group_by(c.lemma)
    .agg(word_count = pl.len())
)

total_words = anno.filter(c.is_alpha).height
```

Then, we could the number of teachings each bigram occurs.

```{python}
bigram_counts = (
    bigrams
    .group_by(c.bigram, c.lemma, c.next_token)
    .agg(bigram_count = pl.len())
)

total_bigrams = bigrams.height
```

And, finally, we can combine the data together to get the PMI scores for each bigram and sort to find those with the highest scores.

```{python}
(
    bigram_counts
    .join(
        word_counts.rename({"lemma": "w1", "word_count": "w1_count"}),
        left_on="lemma",
        right_on="w1"
    )
    .join(
        word_counts.rename({"lemma": "w2", "word_count": "w2_count"}),
        left_on="next_token",
        right_on="w2"
    )
    .with_columns(
        p_bigram = c.bigram_count / total_bigrams,
        p_w1 = c.w1_count / total_words,
        p_w2 = c.w2_count / total_words
    )
    .with_columns(
        pmi = (c.p_bigram / (c.p_w1 * c.p_w2)).log() / np.log(2)
    )
    .filter(c.bigram_count >= 5)
    .sort(c.pmi, descending=True)
    .select(c.bigram, c.bigram_count, c.pmi)
    .head(15)
)
```

The high-PMI bigrams tell a different story than the high-frequency bigrams. These are phrases where the component words strongly predict each other: proper names, technical terms, and domain-specific expressions. Many of these would be good candidates for treating as single units in downstream analysis.

We can extend the same logic to trigrams by shifting twice:

```{python}
(
    anno
    .filter(c.is_alpha)
    .with_columns(
        next_token = c.lemma.shift(-1).over([c.doc_id, c.sid]),
        next_next_token = c.lemma.shift(-2).over([c.doc_id, c.sid]),
        next_is_alpha = c.is_alpha.shift(-1).over([c.doc_id, c.sid]),
        next_next_is_alpha = c.is_alpha.shift(-2).over([c.doc_id, c.sid])
    )
    .filter((c.next_is_alpha == True) & (c.next_next_is_alpha == True))
    .with_columns(
        trigram = c.lemma + " " + c.next_token + " " + c.next_next_token
    )
    .group_by(c.trigram)
    .agg(count = pl.len())
    .sort(c.count, descending=True)
    .head(15)
)
```

Trigrams capture even longer expressions, but again we see the raw scores simply find combinations of frequent words. Extending the code for PMI would be required to get more useful information from this table.

## Named Entity Recognition

Named entity recognition (NER) identifies and classifies proper nouns and other specific references in text. SpaCy's NER model recognizes several entity types: people (PERSON), organizations (ORG), geopolitical entities like countries and cities (GPE), dates (DATE), works of art (WORK_OF_ART), and others. These annotations are stored in the `ent_type` column of our token table.

```{python}
(
    anno
    .filter(c.ent_type != "")
    .select(c.doc_id, c.sid, c.tid, c.token, c.ent_type)
)
```

Each token that is part of a named entity receives a type label. Multi-word entities like "United Kingdom" have the same label on each constituent token. To work with complete entities rather than individual tokens, we need to group consecutive tokens with the same entity type.

We can identify entity boundaries by detecting where the entity type changes:

```{python}
entities = (
    anno
    .filter(c.ent_type != "")
    .with_columns(
        new_entity = c.ent_iob == "B"
    )
    .with_columns(
        entity_id = c.new_entity.cum_sum().over([c.doc_id])
    )
    .group_by([c.doc_id, c.entity_id, c.ent_type])
    .agg(
        entity_text = c.token.str.join(" "),
    )
)
entities.select(c.doc_id, c.ent_type, c.entity_text)
```

Now we can analyze entities at the appropriate level of granularity. For example, we can find the most frequently mentioned people across all documents:

```{python}
(
    entities
    .filter(c.ent_type == "PERSON")
    .group_by(c.entity_text)
    .agg(count = pl.len())
    .sort(c.count, descending=True)
    .head(15)
)
```

The most frequently mentioned people likely include both the subjects of the Wikipedia pages and other figures who appear across multiple biographies—editors, patrons, family members, or influential contemporaries.

We can examine which entity types are most common overall:

```{python}
(
    entities
    .group_by(c.ent_type)
    .agg(count = pl.len())
    .sort(c.count, descending=True)
)
```

Biographical articles naturally contain many dates (birth, death, publication) and references to people and places. The distribution of entity types provides a high-level characterization of the content.

Entity co-occurrence within documents can reveal relationships. Which people are mentioned together? Which places are associated with which organizations?

```{python}
# Find all pairs of people mentioned in the same document
people = (
    entities
    .filter(c.ent_type == "PERSON")
    .select(c.doc_id, person = c.entity_text)
)

person_pairs = (
    people
    .join(people, on="doc_id", suffix="_2")
    .filter(c.person < c.person_2)  # Avoid duplicates and self-pairs
    .group_by([c.person, c.person_2])
    .agg(co_occurrences = pl.len())
    .sort(c.co_occurrences, descending=True)
    .head(10)
)
person_pairs
```

Pairs of people who frequently appear together across documents may have historical connections: collaborators, rivals, members of the same literary movement, or subjects of comparative study.

## Dependency Parsing

While named entities tell us *what* is mentioned, dependency parsing tells us *how* words relate grammatically. Each token in a sentence has a *head*—another token that it modifies or depends on—and a *dependency relation* describing the nature of that relationship. The root of the sentence is typically the main verb, and all other tokens connect to it through a tree structure.

The dependency annotations are stored in the `dep` and `head_idx` columns. Common dependency relations include:

- **nsubj**: Nominal subject (the doer of an action)
- **dobj** / **obj**: Direct object (the receiver of an action)
- **amod**: Adjectival modifier
- **prep** / **pobj**: Prepositional phrases
- **compound**: Compound words or phrases
- **ROOT**: The root of the sentence

Let's look at a particular example from the Seamus Heaney article.

```{python}
(
    anno
    .filter(c.doc_id == "Seamus Heaney")
    .filter(c.sid == 1)
    .select(c.tid, c.token, c.upos, c.dep, c.head_idx)
)
```

We can use dependency relations to extract specific grammatical patterns. For example, to find what subjects do what actions, we can look for subject-verb pairs.

```{python}
verbs = (
    anno
    .filter(c.upos == "VERB")
    .select(
        c.doc_id, c.sid,
        verb_idx = c.tid,
        verb = c.lemma
    )
)

(
    anno
    .filter(c.dep == "nsubj")
    .select(
        c.doc_id, c.sid, 
        subject = c.lemma,
        verb_idx = c.head_idx
    )
    .join(verbs, on=[c.doc_id, c.sid, c.verb_idx])
    .group_by([c.subject, c.verb])
    .agg(count = pl.len())
    .sort(c.count, descending=True)
    .head(10)
)
```

This reveals the typical actions associated with different subjects in our corpus. We might see that authors "write", "publish", and "die", while works "appear", "receive", and "influence".

We can also extract adjective-noun pairs to see how different concepts are described:

```{python}
(
    anno
    .filter(c.dep == "amod", c.is_alpha)
    .select(
        c.doc_id, c.sid,
        adjective = c.lemma,
        noun_idx = c.head_idx
    )
    .join(
        anno.filter(c.upos == "NOUN").select(
            c.doc_id, c.sid, noun_idx = c.tid, noun = c.lemma,
        ),
        on=[c.doc_id, c.sid, c.noun_idx]
    )
    .group_by([c.adjective, c.noun])
    .agg(count = pl.len())
    .sort(c.count, descending=True)
    .head(10)
)
```

The adjective-noun combinations reveal the conceptual vocabulary of the corpus: what kinds of things exist in this textual world, and how are they characterized?

## Keyword in Context (KWIC)

Sometimes we want to see how a specific word is used across the corpus. A *concordance* or *keyword in context* (KWIC) display shows each occurrence of a target word along with the words that surround it. This technique, which predates computers, remains invaluable for understanding how language is actually used.

To build a KWIC display, we need to extract a window of tokens around each occurrence of our keyword. This logic is implemented in the `DSText.kwic` method.

```{python}
kwic_results = DSText.kwic(anno, "poetry", max_num=15, window=5)
kwic_results
```

The KWIC display reveals patterns that aggregate statistics miss. We can see the actual phrases in which a word appears, the grammatical constructions it participates in, and the semantic contexts that surround it. Is "poetry" typically the subject of a sentence or the object? What verbs and adjectives accompany it?

## Complexity Metrics

Beyond analyzing content, we can characterize the *style* of texts using quantitative measures of readability and complexity. These metrics, originally developed to assess the difficulty of reading materials for educational purposes, provide useful descriptive statistics for comparing texts.

For example, sentence length is one of the simplest style measures. Longer sentences tend to be more complex and harder to read.

```{python}
(
    anno
    .group_by([c.doc_id, c.sid])
    .agg(
        n_tokens = pl.len(),
        n_words = c.is_alpha.sum()
    )
    .group_by(c.doc_id)
    .agg(
        mean_sentence_length = c.n_words.mean(),
        max_sentence_length = c.n_words.max(),
        n_sentences = pl.len()
    )
    .sort(c.mean_sentence_length, descending=True)
    .head(10)
)
```

Type-token ratio (TTR) measures vocabulary richness: the number of unique words (types) divided by the total number of words (tokens). A higher TTR indicates more varied vocabulary.

```{python}
(
    anno
    .filter(c.is_alpha)
    .group_by(c.doc_id)
    .agg(
        n_tokens = pl.len(),
        n_types = c.lemma.n_unique()
    )
    .with_columns(
        ttr = c.n_types / c.n_tokens
    )
    .sort(c.ttr, descending=True)
    .head(10)
)
```

TTR is sensitive to document length: longer documents naturally have lower TTR because common words get repeated. For fair comparison across documents of different lengths, we can compute TTR on a fixed-size sample of tokens.

Average word length correlates with vocabulary sophistication. Documents using more polysyllabic, Latinate vocabulary will have higher average word lengths.

```{python}
(
    anno
    .filter(c.is_alpha)
    .with_columns(
        word_length = c.token.str.len_chars()
    )
    .group_by(c.doc_id)
    .agg(
        mean_word_length = c.word_length.mean()
    )
    .sort(c.mean_word_length, descending=True)
    .head(10)
)
```

Finally, function word ratio measures the proportion of grammatical words (articles, prepositions, pronouns) versus content words (nouns, verbs, adjectives). Different genres and styles have characteristic function word profiles.

```{python}
content_pos = ["NOUN", "VERB", "ADJ", "ADV"]

(
    anno
    .filter(c.is_alpha)
    .with_columns(
        is_content = c.upos.is_in(content_pos)
    )
    .group_by(c.doc_id)
    .agg(
        n_words = pl.len(),
        n_content = c.is_content.sum()
    )
    .with_columns(
        content_ratio = c.n_content / c.n_words
    )
    .sort(c.content_ratio, descending=True)
    .head(10)
)
```

## TF-IDF

In the previous sections, we saw that counting the number of times each token or lemma occurs in a document is a useful way of quickly summarizing the content of a document. This approach can be improved by using a scaled version of the count metric. The issue with raw counts is that they tend to highlight very common words such as "the", "have", and "her". These can be somewhat avoided by removing a pre-compiled set of known common words—often called *stopwords*—or by doing part of speech filtering. These coarse approaches, however, mostly just move the issue down to a slightly less common set of words that also do not necessarily summarize the contents of each document very well. For example, "publisher" is a frequently used term in many of the documents in this collection due to the subject matter, but that does not mean that it is particularly informative since it occurs in almost every page.

A common alternative technique is to combine information about the frequency of a word within a document with the frequency of the term across the entire collection. We return here to the importance of how we define a document, which will shape our analysis. Metrics of this form are known as *term frequency–inverse document frequency scores* (TF-IDF). A common version of TF-IDF computes a score for every combination of term and document by multiplying the term frequency by the logarithm of the inverse document frequency. The logarithm is a function that is used to make sure that counts do not grow too fast. For example, a count of about 1000 is only approximately twice as big on the logarithmic scale as a count of 25, in comparison to being 40 times larger on a linear scale. Mathematically, we define this TF-IDF function using the following formula, where *tf* gives the term frequency and *df* gives the document frequency:

$$ \text{tfidf} = \text{tf} \times \log\left(\frac{N + 1}{\text{df} + 1}\right) + 1 $$

Here, *N* is the total number of documents. This score gives a measurement of how important a term is in describing a document in the context of the other documents. If we select words with the highest TF-IDF score for each document, these should give a good measurement of what terms best describe each document uniquely from the rest of the collection. Note that while the scaling functions given above are popular choices, they are not universal. Other papers and software may make different choices with moderate effects on the output results.

```{python}
tfidf = DSText.compute_tfidf(anno, min_df=0.1, max_df=0.5)
tfidf
```

Then we can combine the top terms from 

```{python}
(
    tfidf
    .sort([c.doc_id, c.tfidf], descending=[False, True])
    .group_by(c.doc_id)
    .head(10)
    .group_by(c.doc_id)
    .agg(top_lemmas = c.lemma.str.join("; "))
)
```

The top TF-IDF terms for each document tend to be proper nouns and domain-specific vocabulary that distinguish one article from another. Compare these to the simple frequency counts from earlier: TF-IDF successfully downweights generic terms like "year" and "work" that appear across all biographies.

## Sparse Encoding

The analyses so far have kept our data in *long* format: one row per token or one row per document-term combination. For many applications, it is useful to reshape the data into a *document-term matrix* (DTM), where each row represents a document, each column represents a term, and each cell contains a count or weight. This transformation converts each document into a numerical vector, enabling us to apply the full toolkit of machine learning and dimensionality reduction techniques.

When we construct a DTM using TF-IDF weights, each document becomes a vector in a very high-dimensional space. If our vocabulary contains 10,000 unique terms, then each document is represented as a point in 10,000-dimensional space. The coordinate along each dimension indicates the TF-IDF weight for that term in that document. Crucially, most of these coordinates are zero: any given document uses only a small fraction of the total vocabulary. A vector where most entries are zero is called a *sparse* vector, and this representation is therefore known as a *sparse embedding*.

Mathematically, we can represent document $d$ as a vector $\mathbf{x}_d \in \mathbb{R}^V$ where $V$ is the vocabulary size:

$$
\mathbf{x}_d = \begin{bmatrix} \text{tfidf}(t_1, d) \\ \text{tfidf}(t_2, d) \\ \vdots \\ \text{tfidf}(t_V, d) \end{bmatrix}
$$

For a vocabulary of 10,000 terms, this vector has 10,000 entries, but a typical document might have non-zero values for only a few hundred of them.

This sparse approach contrasts sharply with the *dense embeddings* introduced in @sec-deep. Dense embeddings, produced by neural network models, represent each document as a vector in a much lower-dimensional space—typically 384 to 1024 dimensions—where every coordinate carries meaningful information. Rather than having thousands of interpretable dimensions (one per word), dense embeddings compress semantic meaning into a few hundred opaque dimensions learned during training.

There are several key differences between these approaches. A sparse TF-IDF vector might have $V = 10{,}000$ dimensions with only 200 non-zero entries. A dense embedding has perhaps $d = 384$ dimensions, all non-zero. We can express the sparsity ratio as $\frac{\text{non-zero entries}}{V}$, which is typically below 5% for text data.

Also, each dimension of a sparse vector corresponds to a specific word, so we can inspect a document's representation directly: a high value in the "algorithm" dimension means the document discusses algorithms. Dense embeddings offer no such transparency. Dimension 147 of a dense vector has no human-interpretable meaning; the representation is distributed across all dimensions in ways learned by the neural network. Furthermore, sparse vectors treat each word independently. The terms "car" and "automobile" occupy different dimensions with no built-in notion that they are synonyms. Dense embeddings, trained on massive text corpora, learn that semantically similar words should produce similar vectors. Two documents discussing the same topic in different vocabulary will have similar dense embeddings but potentially very different sparse vectors.

Finally, sparse vectors are cheap to compute—just count words and apply the TF-IDF formula—but expensive to store naively and limited in semantic power. Dense embeddings require a neural network forward pass, which is more computationally intensive, but the resulting vectors are compact and capture deeper meaning.

Despite these limitations, sparse embeddings remain valuable for many tasks. They require no pre-trained models, work immediately on any language or domain, and produce interpretable results. For exploratory analysis and visualization, sparse methods often provide sufficient structure to reveal patterns in the data.

The code below constructs a sparse document-term matrix from our annotated text, then applies UMAP to project these high-dimensional sparse vectors down to two dimensions for visualization. Documents that use similar vocabulary will cluster together in the resulting plot.

```{python}
(
    anno
    .pipe(
        DSSklearnText.umap,
        doc_id=c.doc_id,
        term_id=c.lemma,
        n_components=2,
        min_df=0.01,
        max_df=0.5
    )
    .predict(full=True)
    .pipe(ggplot, aes("dr0", "dr1"))
    + geom_text(aes(label = "doc_id"), size=8)
)
```

The `min_df` and `max_df` parameters filter the vocabulary: terms appearing in fewer than 1% of documents or more than 50% of documents are excluded. This removes both rare terms (which add noise) and ubiquitous terms (which add no discriminative power), focusing the representation on the most informative middle-frequency vocabulary.

## Across Languages

One of the reasons that we enjoy using the content of Wikipedia pages as example datasets for textual analysis is that it is possible to get the page text in a large number of different languages. One of the most interesting aspects of textual analysis is that we can apply our techniques to study how differences across languages and cultures affect the way that knowledge is created and distributed.

Let's see how our text analysis pipeline can be modified to work with Wikipedia pages from the French version of the site. SpaCy provides models for many different languages:

```{python}
nlp_fr = spacy.load("fr_core_news_sm")
```

And now, we can annotate the text as follows

```{python}
anno_fr = DSText.process(docs_fr, nlp_fr)
anno_fr
```

French and English have different grammatical structures that will be reflected in part-of-speech frequencies. French uses more determiners (articles), while English may use more auxiliary verbs. These differences are linguistic rather than content-based, but they affect how we should interpret comparative analyses.

Cross-linguistic analysis opens up fascinating questions about how knowledge is organized and transmitted across cultures. The same historical figure may be framed differently depending on the national perspective of the Wikipedia editors. Events that are central to one country's narrative may be peripheral to another's. Textual analysis provides the tools to investigate these questions systematically.

## References {-}
